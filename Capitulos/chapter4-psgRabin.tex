\chapter{Respondiendo a las preguntas}
~\label{cap:results}

\section{Juegos justos: la rta a la pregunta cualitativa}

\subsection{Draft}

Agg cambiando lo de las acciones

\section{Transformar Rabin a alcanzabilidad}

\subsection{Draft}

La idea es intentar hacer algo al estilo

\begin{theorem}[Reducción de Rabin]
	Sea $\Gk$ un juego estocástico politópico y sea $\Hk$ su interpretación extrema tal como se presenta en \cite{Polytopal}. Si tenemos una propiedad de Rabin $R$, y su respectivo conjunto de estados \textit{almost sure winning} $W_R$ entonces valen las siguientes ecuaciones:

	\begin{align*}
		&\inf_{\piGdiam} \sup_{\piGcuad} \ProbG(R) = \\
		= &\inf_{\piGdiam} \sup_{\piGcuad} \ProbG(\alc W_R) = \\
		= &\inf_{\piHdiamMD} \sup_{\piHcuadMD} \ProbH(\alc W_R) = \\
		= &\sup_{\piHcuadMD} \inf_{\piHdiamMD} \ProbH(\alc W_R) = \\
		= &\sup_{\piGcuad} \inf_{\piGdiam} \ProbG(\alc W_R) = \\
		= &\sup_{\piGcuad} \inf_{\piGdiam} \ProbG(R)
	\end{align*}

\end{theorem}

\begin{proof}
	\begin{align*}
		\inf_{\piGdiam} &\sup_{\piGcuad} \ProbG(R) \\
		&\leq \inf_{\piGdiam} \sup_{\piGcuad} \ProbG(\alc W_R)
		& (\text{por lema } \ref{*1})
		\\
		&\leq \inf_{\piHdiamMD} \sup_{\piHcuadMD} \ProbH(\alc W_R)
		& \text{(por Teorema 1)}
		\\
		&\leq \sup_{\piHcuadMD} \inf_{\piHdiamMD} \ProbH(\alc W_R)
		& \text{(por Teorema 1)}
		\\
		&\leq \sup_{\piGcuad} \inf_{\piGdiam} \ProbG(\alc W_R)
		& \text{(por Teorema 1)}
		\\
		&\leq \sup_{\piGcuad} \inf_{\piGdiam} \ProbG(R)
		& (\text{por lema } \ref{**2})
		\\
		&\leq \inf_{\piGdiam} \sup_{\piGcuad} \ProbG(R)
		& \text{(por prop de sup e inf)}
	\end{align*}
\end{proof}

\begin{lemma}
	\label{*1}
	Sea $\Gk$ la interpretación (?) de un juego estocástico politópico y sea $R$ una propiedad límite y $W_R$ su correspondiente conjunto casi seguramente ganador. Entonces vale que
	$$
		\inf_{\pidiamset} \sup_{\picuadset} \ProbG (R) \leq \inf_{\pidiamset} \sup_{\picuadset} \ProbG (\alc W_R)
	$$
\end{lemma}

\begin{proof}
	Supongamos una estrategia $\pidiam$ arbitraria de memoria finita. Sabemos que $\Gk$ con una estrategia de memoria finita fijada es un PMDP. Entonces, por lema \ref{PMDP-supWR} vale que para cada estrategia de memoria finita $\pidiam$
	$$
		\sup_{\picuadset} \ProbG (R) = \sup_{\picuadset} \ProbG (\alc W_R)
	$$
	lo que implica que (\hl{ahora no sé si me convence, tengo que redactar mejor esta justificación})
	capaz tengo que agg lo de contencion de caminos en P y WP
	$$
		\inf_{\pidiamset} \sup_{\picuadset} \ProbG (R) \leq \inf_{\pidiamset} \sup_{\picuadset} \ProbG (\alc W_R)
	$$
\end{proof}

\begin{lemma}
	\label{PMDP-supWR}
	Sea $\M$ un PMDP, $\pi$ una estrategia en $\M$, $s$ un estado en $\M$, $R$ una propiedad de Rabin y $W_R$ el conjunto casi seguramente ganador de $R$ (en el juego estocástico debería ser, no?), entonces vale que:

	% CAMBIAR WR por conjunto desde el que existe una estr que da prob 1 de ganar

	$$
		\sup_{\pi \in \Pi} \ProbPMDP (R) = \sup_{\pi \in \Pi} \ProbPMDP (\alc W_R)
	$$
\end{lemma}

\begin{proof}
	Por definición de conjunto casi seguramente ganador
	$$
		\Prob_{\M,s}^\pi (R) = \Prob_{\M,s}(\{ \omega \in \paths(s) \mid \inft(\omega) \models R\})
	$$

	Claramente los caminos que hacen que valga la propiedad también son caminos
	desde donde llego a algún estado desde donde existen estrategias para ganar con
	probabilidad 1. Por lo tanto, $\ProbPMDP (R) \leq \ProbPMDP (\alc W_R)$.

	Para ver que vale la igualdad, veremos que existe una estrategia de memoria
	finita $\pi$ que hace que $\ProbPMDP (R) = \sup_{\pi \in \Pi} \ProbPMDP (\alc
		W_R) $. Para ello, consideremos la estrategia sin memoria $\pi_0$ que maximiza
	las probabilidades de alcanzar $W_R$ desde todos los estados $s \in \M$
	(sabemos que esta estrategia existe por \cite{Polytopal, CONDON1992}). A su
	vez, sabemos que para cada estado $s \in W_R$ existe una estrategia $\pi_s$ que
	asegura ganar con probabilidad 1 frente al objetivo $R$.

	Sea entonces $\pi$ la estrategia que primero se comporta como $\pi_0$, hasta
	llegar a un estado $t$ en $W_R$, y a partir de allí $\pi$ se comporta como
	$\pi_t$. Con eso tenemos que:

	\begin{align*}
		\ProbPMDP (R) &= \sum_{t \in W_R} \Prob_{\M,s}^{\pi_0} ((\neg W_R \until t)) \cdot \underbrace{\Prob_{\M,t}^{\pi_t}(R)}_{=1} \\
		&= \sup_{\pi \in \Pi} \ProbPMDP (W_R)
	\end{align*}

	Como $\sup_{\pi \in \Pi} \ProbPMDP (W_R)$ es una cota superior las
	probabilidades para $R$ bajo todas las estrategias, con esto podemos concluir
	la igualdad $\sup_{\pi \in \Pi} \ProbPMDP (R) = \sup_{\pi \in \Pi} \ProbPMDP
		(\alc W_R)$.
\end{proof}

\hl{Esto es lo importante que habría que probar y no está probado.}

\begin{lemma}
	\label{**2}
	Sea $\Gk$ la interpretación (?) de un juego estocástico politópico y sea $R$ una propiedad límite y $W_R$ su correspondiente conjunto casi seguramente ganador. Entonces vale que
	$$
		\sup_{\picuadset} \inf_{\pidiamset} \ProbG (\alc W_R) \leq \sup_{\picuadset} \inf_{\pidiamset}  \ProbG (R)
	$$
\end{lemma}

