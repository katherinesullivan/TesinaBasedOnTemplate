\chapter{Verificación de modelos con juegos}
~\label{cap:modelos}
\vspace{-1cm}

En este capítulo presentaremos brevemente cómo se aborda la verificación de
modelos con juegos. Para ello comenzaremos dando un contexto sobre el campo de
la verificación de modelos y luego presentaremos cuatro modelos matemáticos que
sirven para la representación de distintos tipos de sistemas: las cadenas de
Markov, los procesos de decisión de Markov, los juegos estocásticos y los
juegos -de grafo- deterministas.

% En este capítulo presentaremos brevemente la verificación de modelos con juegos. Para ello
% comenzaremos dando un contexto sobre el campo de estudio de la verificación de
% modelos, luego presentaremos ciertos modelos matemáticos que sirven para la
% representación de distintos tipos de sistemas (las cadenas de Markov, los
% procesos de decisión de Markov, los juegos estocásticos y los juegos -de grafo-
% deterministas), y, por último, plantearemos cuáles son las preguntas de
% investigación que podrían ser de interés en estos casos y presentamos con mayor
% precisión cuáles son las preguntas de investigación que competen a este
% trabajo.

\section{Sobre la verificación de modelos}

La verificación de modelos (\textit{model checking} en inglés) es una técnica
prominente dentro de la verificación formal que sirve para evaluar las
propiedades funcionales de los sistemas de información y comunicación de manera
automatizada. Esta técnica requiere de un modelo del sistema en cuestión y una
propiedad deseada, y consiste en verificar sistemáticamente si el modelo dado
satisface dicha propiedad.

Existen numerosas propiedades de interés para comprobar sobre sitemas (a
destacar podrían ser la ausencia de interbloqueos o propiedades de
solicitud-respuesta) y, a su vez, existen muchos modelos matemáticos que
permiten distintas representaciones de sistemas reales, cada uno con sus
particularidades. La idea de las próximas subsecciones será presentar varios de
estos modelos, sus características y ver, como motivación, un ejemplo de
sistema representable con ellos.

% Las propiedades típicas que se pueden comprobar son la ausencia de
% interbloqueos, los invariantes y las propiedades de solicitud-respuesta.

% Alternativamente, se la puede pensar como una técnica de depuración inteligente
% y eficaz.

% Existen distintos modelos matemáticos que permiten el análisis de
% representaciones de sistemas reales,,,

% blabla

% problema de sintesis de church

% idea de la necesidad de juegos y nuestro entendimiento de ellxs

% ¿Que preguntas nos hacemos cuando trabajamos con juegos? Mas que nada con juegos estocasticos
% tenemos las preguntas que se hace Kucera, las que presenta Chatterjee y se puede investigar mas
% Capaz esto iría mas en la introducción para platear la idea de trabajo

% Existen estrategias óptimas para todos los juegos de una determinada clase con
% un determinado objetivo? -> mm obj

% Cuál es el tipo de las estrategias óptimas? // ganadoras

% Podemos computar las estartegias óptimas? Sintésis de estr kinda // ganadoras

% Podemos computar o aproximar el valor de un vértice? -> mmm vértice o estado //
% quien gana en un vértice dado

% Estas preguntas se suelen hacer restringidas a objetivos de algún tipo en
% particular.

% Notas sobre vertices vs estados + sobre caminos/comportamientos + notacion
% juegos deterministas y estocasticos

\section{Cadenas de Markov}

El primer modelo matemático que veremos son las cadenas de Markov. Es
importante notar que trabajaremos viendo a las cadenas de Markov como sistemas
de transición anotados con probabilidades. Esto en contraposición al enfoque
que plantea a las cadenas de Markov como una familia de variables aleatorias.
Pasemos, entonces, a su definición.

\begin{definition}[Cadena de Markov]
	Una cadena de Markov es una tupla $M = (\St, P)$ donde $\St$ es un conjunto finito no vacío de estados y $P: \St \times \St \rightarrow [0,1]$ es una función tal que para todos los estados $s$ vale que
	$$ \sum_{s' \in \St} P(s, s') = 1$$
\end{definition}

A la función $P$ se la denomina la función de probabilidad de transición y cada
elemento $P(s, s')$ se interpreta como la probabilidad de moverse de $s$ a $s'$
en un solo paso. La restricción impuesta sobre $P$ asegura que esta función sea
una distribución

% La función de probabilidad de transición $P$ especifica para cada estado $s$ la
% probabilidad $P(s, s')$ de moverse de $s$ a $s'$ en un solo paso. La
% restricción impuesta en $P$ asegura que la función sea una distribución.

Una cadena de Markov induce un grafo donde los estados actúan como vértices y
hay una arista entre $s$ y $s'$ si y solo si $P(s, s') > 0$. Las cadenas de
Markov se suelen representar directamente por su grafo subyacente, donde sus
aristas estarán anotadas con las probabilidades en el intervalo $(0, 1]$.

Los caminos en una cadena de Markov son los caminos en el grafo subyacente. Es
decir, son definidos como secuencias infinitas de estados $\omega = (s_0, s_1,
	s_2, \dots) \in \St^\omega$ tales que $P(s_i, s_{i+1}) > 0 $ para todo $i \geq
	0$. A su vez, dado un camino infinito $\omega = (s_0, s_1, \dots)$, podemos
pensar en los prefijos finitos de ese camino, $\prefijo(\omega) = \{(s_0,
	\dots, s_n) | n \in \mathbb{N}\}$. Sea $M$ una cadena de Markov, notaremos con
$\Paths(M)$ al conjunto de todos los caminos en $M$ y con $\Pathsfin(M)$ al
conjunto de todos los prefijos finitos de caminos en $M$.

Veamos un pequeño ejemplo de cómo sería una cadena de Markov.

%% VER QUE ONDA ACA LO DEL BOX GRIS
\textbf{Randomito, el pequeño robot probabilístico}

Supongamos que queremos observar el comportamiento de un pequeño robot llamado
\textit{Randomito}.

\textit{Randomito} se encuentra en una grilla 2x2 y se comporta totalmente probabilísticamente siguiendo la siguiente descripción: desde su posición inicial de reposo en la esquina superior izquierda tiene una probabilidad de 0.5 de seguir allí y de 0.25 de moverse tanto a la izquierda como hacia abajo. Si sucede que en algún momento se encuentra en la esquina superior o inferior derecha, como a \emph{Randomito} no le gusta quedarse del lado de la derecha, tiene probabilidad 0 de quedarse allí o de ir hacia abajo o arriba. Desde esas esquinas, \emph{Randomito} se mueve con probabilidad 1 hacia la izquierda. Desde la esquina inferior izquierda tendrá una probabilidad de 0.3 de quedarse allí, 0.2 de ir a la derecha y 0.5 de ir hacia arriba.

El comportamiento de \emph{Randomito} se puede modelar con la cadena de Markov
representada por el siguiente grafo:

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[
			>=Stealth,
			node distance=3cm,
			state/.style={draw, circle, minimum size=1.2cm, font=\normalsize},
			every loop/.style={looseness=10},
			bend angle=20
		]

		% Definición de los cuatro nodos en forma de cuadrado
		\node[state] (SI) at (0,0)  {SI};
		\node[state] (SD) at (4,0)  {SD};
		\node[state] (II) at (0,-4) {II};
		\node[state] (ID) at (4,-4) {ID};

		% Lazos en cada nodo (loop)
		\draw[->] (SI) edge[loop above]   node[above]   {0.5} ();
		%\draw[->] (SD) edge[loop above]   node[above]   {0.5} ();
		\draw[->] (II) edge[loop below]   node[below]   {0.3} ();
		%\draw[->] (ID) edge[loop below]   node[below]   {0.5} ();

		% Flechas entre todos los pares de nodos
		% SI <-> SD
		\draw[->] (SI) to[bend left]  node[midway, below] {0.25} (SD);
		\draw[->] (SD) to[bend left]  node[midway, below] {1} (SI);

		% SI <-> II
		\draw[->] (SI) to[bend left]  node[midway, right]  {0.25} (II);
		\draw[->] (II) to[bend left]  node[midway, right] {0.5} (SI);

		% SD <-> ID
		%\draw[->] (SD) to[bend left]  node[midway, right] {0.5} (ID);
		%\draw[->] (ID) to[bend left]  node[midway, right]  {0.5} (SD);

		% II <-> ID
		\draw[->] (II) to[bend left]  node[midway, below] {0.2} (ID);
		\draw[->] (ID) to[bend left]  node[midway, below] {1} (II);

	\end{tikzpicture}
	\caption{Cadena de Markov que representa el comportamiento probabilístico de \emph{Randomito}.}
	\label{fig:cm-randomito}
\end{figure}

% Los 3 puntos están significando distintas cosas!!
Algunos caminos posibles en esta cadena de Markov son:
\begin{itemize}
	\item el que mantiene a \emph{Randomito} siempre en la esquina superior izquierda,
	      $(s_1, s_2, s_3, \dots)$ donde $\forall k > 0, s_k = SI$, o
	\item el camino cuyos primeros movimientos forman la letra ``C'' en el grafo y luego
	      se comporta de manera arbitraria, $(SI, SD, SI, II, ID, II, SI, \dots)$.
	      %\item  o algunos de los que mantienen a \emph{Randomito} en la parte superior de la grilla, como $(SI, SD, SI, SD, SI, SD, \dots)$ o $(SI, SD, SI, SD, SI, SI, SI, \dots)$.
\end{itemize}

Una pregunta natural que podría surgir es ``¿qué probabilidad hay de que
\emph{Randomito} siga alguno de esos caminos?''.

Para poder asociar probabilidades a eventos en cadenas de Markov (o, lo que es
lo mismo, pensar en preguntas sobre la probabilidad de que \emph{Randomito}
siga algún tipo particular de camino), la noción intuitiva de probabilidades en
ellas debe ser formalizada. Lograremos esto asociándole un espacio de
probabilidad, es decir, una $\sigma$-álgebra y una medida de probabilidad, a
cada cadena de Markov.

Sea $M$ una cadena de Markov, los caminos infinitos de $M$ jugarán el rol de
resultados, esto es $Outc^M = \Paths(M)$, y la $\sigma$-álgebra asociada a $M$
será la generada por los conjuntos cilindro formados por los fragmentos de
caminos finitos en $M$.

\begin{definition}[Conjunto cilindro]
	%El conjunto cilindro de $\hat \omega = (s_0, \dots, s_n) \in \Pathsfin(M)$ está definido como
	Sea $\hat \omega = (s_0, \dots, s_n)$ un camino finito en una cadena de Markov
	$M$. El conjunto cilindro de $\hat \omega$ está definido como: $$\Cyl(\hat
		\omega) = \{\omega \in \Paths(M) \mid \hat \omega \in \prefijo(\omega)\}$$
\end{definition}

\begin{definition}[$\sigma$-álegra de una cadena de Markov]
	La $\sigma$-álgebra $\eventE^M$ asociada a la cadena de Markov $M$ es la $\sigma$-álgebra más pequeña que contiene todos los conjuntos cilindro de todos los prefijos de caminos finitos de $M$.
\end{definition}

De conceptos clásicos de teoría de probabilidad se deriva que para cada estado
$s$ existe una única medida de probabilidad $\Prob^M_{s}$ en la
$\sigma$-álgebra $\eventE^M$ asociada a $M$, donde las probabilidades para los
conjunto cilindros (es decir, los eventos) están dadas por:
\begin{equation}
	\label{probMC}
	\Prob^M_{s}(Cyl(s_0, \dots, s_n)) = \iota(s,s_0) \cdot \prod_{0 \leq i < n} P(s_i, s_{i+1})\text{, donde }\iota(s,s_0) =
	\begin{cases}
		1 &\text{si } s = s_0 \\
		0 &\text{en otro caso}
	\end{cases}
\end{equation}

\textbf{¿Qué probabilidad hay de que Randomito siga este camino?}

Ahora, con las definiciones vistas, podemos pensar en la probabilidad que tiene
\emph{Randomito} de tomar algunos caminos específicos. Por ejemplo,

\begin{itemize}
	\item la probabilidad de que \emph{Randomito} se quede siempre en la esquina superior
	      izquierda, aún empezando desde allí, resulta 0 porque $\Prob^M_{SI}((SI, SI,
		      SI, \dots)) = \lim_{n\to \infty} (0.5)^n = 0$.
	\item la probabilidad de que \emph{Randomito} haga un primer movimiento en ``C'' y
	      luego se comporte de cualquier manera será $\Prob^M_{SI}(\Cyl(SI, SD, SI, II,
		      ID, II, SI)) = 0.25 \cdot 1 \cdot 0.25 \cdot 0.2 \cdot 1 \cdot 0.5 = 0.00625$.
\end{itemize}

% \textbf{Notación}: en lo que sigue, usaremos notación LTL para describir ciertos eventos en cadenas de Markov. Por ejemplo, para un conjunto $B \subseteq \St$ de estados, $\alc B$ denota el evento de llegar eventualmente a (algún estado en) $B$, mientras que $siempevent B$ describe el evento en el que $B$ es visitado infinitamente a menudo. \hl{Algo más? sí}

% Decir algo mas de notacion LTL? Algo más de cadenas de markov?

\section{Procesos de Decisión de Markov}

Las cadenas de Markov resultan útiles cuando queremos modelar sistemas en donde
existen decisiones probabilistas, pero existen situaciones en donde además
necesitamos modelar elecciones, es decir, decisiones no-deterministas. Para
ello, existen los llamados procesos de decisión de Markov.

Un proceso de decisión de Markov (MDP, por sus siglas en inglés) es una
generalización de una cadena de Markov donde un conjunto de acciones posibles
es asociado a cada estado. A cada par estado-acción le corresponde una
distribución de probabilidad que es usada para seleccionar el próximo estado. A
su vez, una cadena de Markov se corresponde con un MDP que tiene exactamente
una acción asociada a cada estado.

A continuación presentamos la definición de un proceso de decisión de Markov,
en ella asumiremos un conjunto fijo de acciones $\Act$.

% Asumiremos la existencia de un conjunto fijo de acciones $\Act$ y a
% continuación presentaremos la definición de un proceso de decisión de Markov:

\begin{definition}[Proceso de Decisión de Markov]
	Un proceso de decisión de Markov $\M =(\St, \Act, \theta)$ consiste de un conjunto finito de estados $\St$ y de dos componentes, $\Act$ y $\theta$, que especifican la estructura de transición entre los estados:
	\begin{itemize}
		\item Para cada $s \in \St$, $\Act(s) \subseteq \Act$ es el conjunto finito de
		      acciones disponibles en $s$. Para cada estado $s \in \St$ se requiere que
		      $\Act(s) \neq \emptyset$. %% Pensar la notacion de A(s) si no confunde por pensar en funcion
		\item $\theta : \St \times \Act \times \St \rightarrow [0,1]$ es una función de transición probabilística. Para todo estado $s \in \St$, si $a \in \Act(s)$ tenemos que $\sum_{s' \in \St} \theta(s, a, s') = 1$, mientras que si $a \notin \Act(s)$, $\sum_{s' \in \St} \theta(s, a, s') = 0$. Para cada $s, t \in \St$ y $a \in \Act(s)$, $\theta(s, a, t)$ es la probabilidad de transicionar de $s$ a $t$ cuando la acción $a$ es seleccionada.
	\end{itemize}
\end{definition}

El concepto de caminos en MDPs que mostraremos en este trabajo difiere del
concepto de camino que se puede encontrar en alguna literatura y del que
presentamos para cadenas de Markov, donde estos eran solo secuencias de
estados. Un camino en un proceso de decisión de Markov es una secuencia
alternante infinita de estados y acciones, construida iterativamente mediante
un proceso de dos pasos. Primero, dado un estado $s$, una acción $a \in
	\Act(s)$ es seleccionada no-determinísticamente. Luego, el sucesor $t$ de $s$
es seleccionado de acuerdo a la distribución asociada a la acción $a$. La
definición formal es la siguiente:

\begin{definition}[Camino en un MDP]
	Un camino en un MDP $\M$ es una secuencia infinita $\omega = (s_0, a_0, s_1, a_1, \dots)$ tal que $s \in \St$, $a_i \in \Act(s_i)$ y $a_i(s_{i+1}) > 0$ para todo $i \geq 0$.

	Dado un estado $s$, indicaremos con $\Paths_s$ al conjunto de todos los caminos
	que se originan en $s$, con $\Paths$ al conjunto de todos los caminos en $\M$ y
	con $\Pathsfin$ al conjunto de todos los prefijos finitos de caminos en $\M$
	que terminan en un estado (y no en una acción).
\end{definition}

Veamos un ejemplo de un proceso de decisión de Markov.

\textbf{Roborto, el robot controlable}

Ahora supongamos que tenemos un robot en una grilla 2x2, \emph{Roborto}, al que
podemos manejar a través de un control remoto. Con este control, podemos
decidir si se mueve a la izquierda, derecha, arriba o abajo (dependiendo de lo
que permita su posición en la grilla). Sin embargo, por irregularidades que
puede haber en la grilla cuando seleccionamos una acción habrá una probabilidad
de que la acción no tenga el resultado deseado. Por ejemplo, si estando en la
esquina superior izquierda, elijo que \emph{Roborto} se mueva a la derecha,
esto sucederá con una probabilidad $p$, pero por las irregularidades del
terreno, también puede suceder que se mueva hacia abajo a la derecha con una
probabilidad $1 - p$.

Situaremos a \emph{Roborto} en una grilla particular con las siguientes
características:

\begin{itemize}
	\item Desde la esquina superior izquierda, si se elije ir a la derecha, habrá una
	      probabilidad de $0.85$ de efectivamente ir a la esquina superior derecha y
	      habrá una probabilidad de $0.15$ de ir a la esquina inferior derecha; mientras
	      que si se elije ir hacia abajo, con una probabilidad de $0.8$ se irá hacia
	      abajo, pero con una probabilidad de $0.2$ se irá a la esquina inferior derecha.
	\item Desde la esquina inferior derecha otra vez tendremos esta probabilidad de que
	      el terreno nos juegue una mala pasada: si se elije ir hacia arriba se irá hacia
	      arriba con una probabilidad de $0.95$, pero habrá una probabilidad de $0.05$ de
	      ir a la esquina superior izquierda; mientras que si se elije ir a la izquierda
	      con una probabilidad de $0.85$ se irá a la izquierda y con una probabilidad de
	      $0.15$ se irá a la esquina superior izquierda.
	\item Desde las esquinas inferior izquierda y superior derecha, el terreno no influye
	      y se irá con probabilidad 1 en la dirección seleccionada.
\end{itemize}

Tomando el conjunto de acciones, $\Act = \{\rightarrow, \leftarrow, \uparrow,
	\downarrow\}$ que representan, respectivamente, el moverse a la derecha, a la
izquierda, arriba y abajo, podemos ver el proceso dibujado en la
figura~\ref{fig:mdp-roborto}.

Otra vez, tendremos la duda de con qué probabilidades ocurren ciertos
comportamientos de nuestro Robot, y para eso, otra vez, deberemos formalizar
ciertas nociones.

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[
			>=Stealth,
			node distance=3cm,
			state/.style={draw, circle, minimum size=1.2cm, font=\normalsize},
			every loop/.style={looseness=10},
			bend angle=20
		]

		% Definición de los cuatro nodos en forma de cuadrado
		\node[state] (SI) at (0,0)  {SI};
		\node[state] (SD) at (4,0)  {SD};
		\node[state] (II) at (0,-4) {II};
		\node[state] (ID) at (4,-4) {ID};

		% FLECHAS

		% Desde SI
		\draw[->, draw=violet] (SI) to  node[midway, above, text=violet] {$\rightarrow$,0.85} (SD);
		\draw[->, draw=violet]  (SI) to[bend left=40]  node[above, text=violet] {$\rightarrow$,0.15} (ID);
		\draw[->, draw=orange] (SI) to[bend left=20]  node[text=orange] {$\downarrow$, 0.2}  (ID);
		\draw[->, draw=orange] (SI) to  node[midway, left, text=orange]  {$\downarrow$, 0.8} (II);

		% Desde ID
		\draw[->, draw=teal] (ID) to  node[midway, right, text=teal]  {$\uparrow$,0.95} (SD);
		\draw[->, draw=teal] (ID) to[bend left=20] node[above, text=teal] {$\uparrow$,0.05} (SI);
		\draw[->, draw=olive]  (ID) to[bend left=40] node[below, text=olive] {$\leftarrow$,0.15} (SI);
		\draw[->, draw=olive] (ID) to  node[midway, below, text=olive] {$\leftarrow$,0.85} (II);

		% Desde II
		\draw[->, draw=teal] (II) to[bend left=60]  node[midway, left, text=teal] {$\uparrow$, 1} (SI);
		\draw[->, draw=violet] (II) to[bend right=60]  node[midway, below, text=violet] {$\rightarrow$,1} (ID);

		% Desde SD
		\draw[->, draw=olive] (SD) to[bend right=60]  node[midway, below, text=olive] {$\leftarrow$,1} (SI);
		\draw[->, draw=orange] (SD) to[bend left=60]  node[midway, right, text=orange] {$\downarrow$,1} (ID);
	\end{tikzpicture}
	\caption{Proceso de decisión de Markov representando el comportamiento de \emph{Roborto}.}
	\label{fig:mdp-roborto}
\end{figure}

Para cadenas de Markov, el conjunto de caminos está equipado con una
$\sigma$-álgebra y una medida de probabilidad que refleja la noción intuitiva
de probabilidad para conjuntos de caminos. Para los MDPs, esto es levemente
distinto. Como no hay restricciones en la resolución de las elecciones no
deterministas, no hay una única medida de probabilidad asociada a cada estado.

Para poder razonar sobre probabilidades de conjuntos de caminos en un MDP
necesitamos resolver de alguna manera el no determinismo, y para ello
introduciremos el concepto de estrategia.

\begin{definition}[Estrategia en un MDP]
	Sea $\M = (\St, \Act, \theta)$ un MDP. Una estrategia para $\M$ es una función $\pi: \Pathsfin \rightarrow \dist (\Act)$ que asigna una distribución de probabilidad a cada prefijo finito de camino verificando que $\pi(\hat \omega) (a) > 0$ solo si $a \in \Act(s)$.
\end{definition}

\kathy{Comentar algo de que en la literatura se suelen bajar las acciones de la definición de estrategia? O capaz la idea sería dropear las acciones de la definición de estrategia acá? Esto último implicaría cambiar tmb para sg y psg}

Como una estrategia resuelve todas las elecciones no deterministas en un MDP,
induce una cadena de Markov. Esto es, el funcionamiento de un MDP $\M$
siguiendo las decisiones de una estrategia $\pi$ puede ser formalizado por una
cadena de Markov $\M_\pi$, donde los estados son los prefijos finitos de
caminos en $\M$. \kathy{Me hace un poco de ruido por la parte de la def de la
	MC con infinitos estados. La otra opción es ir más con el enfoque de de Alfaro
	y presentar la $\sigma$-álgebra sin mencionar nada de MCs}.

\begin{definition}[Cadena de Markov de un MDP inducida por una estrategia]
	Sea $\M = (\St, \Act, \theta)$ un MDP y $\pi$ una estrategia en $\M$. La cadena de Markov $\M_\pi$ está dada por
	$$\M_\pi = (\Pathsfin, P_\pi)$$
	donde para $\hat \omega = (s_0, a_0, s_1, a_1, \dots, s_n)$:
	$$P_\pi(\hat \omega, \hat \omega a s_{n+1}) = \pi(\hat \omega)(a) \cdot \theta(s_n, a, s_{n+1})$$
\end{definition}

Nótese que $\M_\pi$ cuenta con un espacio de estados infinito, aun cuando el
MDP $\M$ es finito.

Como $\M_\pi$ es una cadena de Markov, uno ahora puede razonar sobre las
probabilidades de los conjuntos medibles de caminos que siguen la estrategia
$\pi$, simplemente usando las distintas medidas de probabilidad
$\Prob^{\M_\pi}_s$ asociadas a la cadena de Markov $\M_\pi$ (véase
\ref{probMC}).

% \hl{Responder preguntas de probabilidad de cosas en MDP robot}

Intuitivamente, el estado $(s_0, a_0, \dots, s_n)$ de $\M_\pi$ representa la
configuración donde el MDP $\M$ está en el estado $s_n$ y cuenta con la
historia $(s_0, a_0, \cdots, s_{n-1}, a_{n-1})$. Según la definición que vimos
las estrategias pueden depender de la historia en su totalidad, produciendo
resultados distintos si al menos una acción o estado en su historia cambia,
pero no es usual el definir comportamientos distintos si solo una acción o
estado en su historia cambia. Presentamos algunas clases de estrategias que no
hacen esto:

\begin{definition}[Estrategias sin memoria]
	Sea $\M$ un MDP con espacio de estados $\St$. Una estrategia $\pi$ en $\M$ es sin memoria sii para cada par de caminos $(s_0, a_0, \dots, s_n)$ y $(t_0, a'_0, \dots, t_m)$ con $s_n = t_m$ vale que:
	$$\pi(s_0, a_0, \dots, s_n) = \pi(t_0, a'_0, \dots, t_m)$$
	En este caso, $\pi$ puede ser vista como una función $\pi : \St \rightarrow \dist(\Act)$.
\end{definition}

Coloquialmente, una estrategia es sin memoria si no recuerda nada de la
historia y solo elige probabilidades para las acciones basándose en el estado
actual. Esto puede ser bastante extremo en ciertos casos, por eso existe una
variante que busca reflejar la idea de finitud sin ser tan restrictiva: las
estrategias de memoria finita. Una estrategia de memoria finita puede ser
pensada intuitivamente como que solo puede guardar hasta una cantidad finita de
información de la historia, por lo que no podrá ser distinta para \textbf{todo}
prefijo finito de camino. Formalmente, la definiremos a través de un autómata
determinista finito (DFA). La distribución de probabilidad de las acciones será
seleccionada a partir del estado actual en $\M$ y el estado actual del autómata
(al que llamaremos modo). Veamos su definición:

\begin{definition}[Estrategias con memoria finita]
	Sea $\M$ un MDP con espacio de estados $\St$ y conjunto de acciones $\Act$. Una estrategia de memoria finita para $\M$ es una tupla $\pi = (Q, f_\pi, \Delta, start)$ donde
	\begin{itemize}
		\item $Q$ es un conjunto finito de modos,
		\item $\Delta : Q \times \Act \times \St \rightarrow Q$ es la función de transición del autómata,
		\item $start: \St \rightarrow Q$ es la función que determina el modo en el que empieza el automáta para un estado inicial $s$,
		\item $f_\pi : Q \times \St \rightarrow \dist(\Act)$ es la función que, desde un estado y modo, asigna la distribución de probabilidad sobre las acciones asociada a este estado y modo, es decir, cumpliría el rol de lo que veníamos entendiendo como estrategia.
	\end{itemize}
	El funcionamiento del MDP bajo la estrategia de memoria finita sería como sigue. En principio, se inicializa el modo del DFA a $q_0 = start(s_0)$. Luego, desde cada estado $s_i$ posterior el proceso será iterativo. Primero, se seleccionará la distribución de probabilidad sobre las acciones a partir del modo actual $q_i$ del autómata con $f_\pi((q_i,s_i))$. Una vez tomada la decisión, se determina probabilísticamente la siguiente acción $a_{i+1}$, y, a partir de ella, se determina también probabilísticamente el siguiente estado $s_{i+1}$. Con la nueva acción y estado se selecionará el próximo modo del DFA $q_{i+1} = \Delta(q_i, a_{i+1}, s_{i+1})$ y se repetirá el proceso.

	Para $\hat \omega \in \Pathsfin$, notaremos con $\pi(\hat \omega)$ a la
	distribución obtenida al realizar el proceso explicado anteriormente con $\hat
		\omega$.
	%\hl{pero qué pasa con la elección probabilística de las acciones? No
	% habría que tenerlo? Y que hay de sin memoria? Ahi en realidad entonces no
	% tendria que ser ultimo estado y penultima accion?}.
\end{definition}

A su vez, existe otro tipo de categorización de las estrategias que depende de
cómo es la distribución que realiza la estrategia sobre las acciones
disponibles desde un estado.

% Además de su categorización en base a qué tanto dependen de su historia, existe
% otro tipo de caracterización de las estrategias dependiendo en cómo es la
% distribución que realiza $\pi$ sobre las acciones disponibles desde un estado.

Una estrategia es pura\footnote{o determinista, pero nos abstendremos de usar
	este nombre por las confunsiones que pueda llegar a generar el extensivo uso de
	la palabra determinista} cuando para cada $\hat \omega \in \Pathsfin$ la
distribución $\pi(\hat \omega)$ es una distribución de Dirac (es decir, una
distribución $\delta_a$ tal que $\delta_a(a) = 1$ y $\delta_a(b) = 0$ para todo
$b \neq a$). Y, por otro lado, una estrategia se dice randomizada cuando no es
pura, es decir, cuando la distribución $\pi(\hat \omega)$ no es una
distribución de Dirac (i.e., existen al menos dos $s, s' \in St$ distintos
tales que $\pi(\hat \omega)(s) > 0$ y $\pi(\hat \omega)(s') > 0$).

En la literatura (véase \cite{BaierKatoen, AlfaroThesis}) es usual encontrarse
con el estudio de estrategias solo en su variante pura y, en ese caso, se puede
pensar a las estrategias como una función $\pi: \Pathsfin \rightarrow \Act$.

Volviendo al ejemplo que presentamos de Roborto, ahora presentaremos distintos
tipos de estrategias para él.

\textbf{Controlando a Roborto}

Podemos pensar distintas estrategias para controlar a Roborto. Veamos algunas
de ellas:

\begin{itemize}
	\item Una estrategia pura y sin memoria podría ser la que siempre decide moverse
	      lateralmente. Es decir, dada cualquier historia elegirá desde $SI$ o $II$ con
	      probabilidad 1 tomar la acción $\rightarrow$ y desde $SD$ o $ID$, la acción
	      $\leftarrow$.
	\item Una estrategia randomizada podría ser una que desde cada estado, asigne iguales
	      probabilidades de tomar alguna de las dos acciones disponibles. Es decir, por
	      ejemplo tendríamos, $\forall \hat \omega \in \Pathsfin$ tal que $\hat \omega$
	      termina en $SI$, $\pi(\hat \omega)(\rightarrow) = 0.5$ y $\pi(\hat
		      \omega)(\downarrow) = 0.5$.
	      %, siendo $\alpha$ una acción habilitada desde el último estado de $\hat \omega$ que da una probabilidad positiva
	      En este caso, la estrategia también resulta sin memoria. Veamos ahora un
	      ejemplo en el que esto no sucede.
	\item Una estrategia con memoria finita podría ser una $\pi$ tal que dado un $\hat
		      \omega \in \Pathsfin$ que termina en un estado $s$, si la cantidad de veces que
	      se visitó $s$ en $\hat \omega$ es par, entonces $\pi$ con probabilidad uno
	      elegirá la acción de moverse lateralmente, mientras que si la cantidad de veces
	      que se visitó $s$ en $\hat \omega$ es impar, entonces $\pi$ con probabilidad
	      uno elegirá la acción de moverse verticalmente. Un ejemplo de esto sería que
	      para el prefijo de camino $\hat \omega' = (SI, II, SI)$ tendremos que $\pi(\hat
		      \omega')(\rightarrow) = 1$ y para el prefijo de camino $\hat \omega''=(SI, II,
		      SI, SD, ID, SD, SI)$ tendremos que $\pi(\hat \omega'')(\downarrow) = 1$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{definition}[Conjunto cilindro]
% 	El conjunto cilindro de $\hat \omega = (s_0, a_0, \dots, s_n) \in \Omega^{\textit{fin}}_\M$ está definido como
% 	$$Cyl(\hat \omega) = \{\omega \in \Omega_\M \mid \hat \omega \in pref(\omega)\}$$
% \end{definition}

% \begin{definition}[$\sigma$-álegra de un MDP]
% 	La $\sigma$-álgebra $\eventE^\M$ asociada al proceso de decisión de Markov $\M$ es la $\sigma$-álgebra más pequeña que contiene todos los conjuntos cilindro $Cyl(\hat \omega)$ donde $\hat \omega \in \Omega^{\textit{fin}}_\M$.

% 	Para cada estado $s \in S$, definiremos como $\B_s \subseteq 2^{\Omega_s}$ a la
% 	$\sigma$-álgebra más pequeña que contiene a todos los conjuntos cilindros
% 	$Cyl(\hat \omega)$ donde $\hat \omega \in \Omega^{\textit{fin}}_{s}$.
% \end{definition}

% \hl{Todavía no sé muy bien cómo manejar lo de estados iniciales}

% Para poder hablar de probabilidad de conjuntos de comportamientos querríamos
% asociar a cada $\gamma \in \B_s$ su medida de probabilidad

\section{Juegos estocásticos}

Además de elecciones, podríamos también querer modelar comportamiento
adversarial. En este tipo de comportamiento, surgen distintos agentes con
distintos objetivos, por lo cual se ha pensado históricamente a través de
juegos. Para modelar estos juegos cuando existe un comportamiento
probabilístico suelen usarse los llamados juegos estocásticos.
% Si estos juegos exhiben también comportamiento probabilístico los modelos que se
% suele usar son los llamados juegos estocásticos.

% \hl{Definir cómo queda con lo de acciones habilitadas esto o MDP} ¿qué?
\begin{definition}[Juego estocástico]
	Un juego estocástico (SG) es una tupla $\G = (\St, (\St_\cuad, \St_\diam), \Act, \theta)$ donde:
	\begin{itemize}
		\item $\St$ es un conjunto finito de estados con $\St_\cuad, \St_\diam \subseteq \St$ siendo una partición de él,
		\item $\Act$ es un conjuno finito de acciones, y
		\item $\theta : \St \times \Act \times \St \rightarrow [0,1]$ es una función de transición probabilística tal que para cada $s \in \St$, $\theta(s, a, \cdot) \in \dist(\St)$ o  $\theta(s, a, \St) = 0$.
	\end{itemize}
	Notaremos con $\Act(s) = \{a \in \Act \mid \theta(s,a,\St) = 1\}$ al conjunto de acciones habilitadas en $s$.
\end{definition}

Se puede notar que si $\St_\cuad = \emptyset$ o $\St_\diam = \emptyset$,
entonces $\G$ es un proceso de decisión de Markov, y, como vimos antes, si a la
vez $\forall s \in \St$, $\abs{\Act(s)} = 1$ entonces $\G$ es una cadena de
Markov.

Otra manera de verlo a los juegos estocásticos es pensarlos como procesos de
decisión de Markov donde el conjunto de estados está particionado en dos: los
estados correspondientes al jugador $\cuad$ y los estados correspondientes al
jugador $\diam$. Esta partición de estados genera que haya dos entes que tomen
decisiones no-deterministas, en lugar de uno, con lo cual tendremos dos tipos
de estrategias, una por cada tipo de jugador.
% La idea de esta anotación de a quién pertence los estados es
% lo que introduce esta idea adversarial: no solo habrá un ente que tome las
% decisiones no deterministas, sino dos. Esto quiere decir que tendremos dos
% tipos de estrategias, una por cada tipo de jugador.

Al igual que con los MDPs, antes de definir las estrategias, presentamos el
concepto de camino en un juego estocástico, el cual seguirá la misma idea del
definido para MDPs: además de estados, posee acciones.

\begin{definition}[Camino en un SG]
	Un camino en un SG $\G$ es una secuencia infinita $\omega = (s_0, a_0, s_1, a_1, \dots)$ tal que $s_i \in \St$, $a_i \in \Act(s_i)$ y $a_i(s_{i+1}) > 0$ para todo $i \geq 0$.

	Dado un estado $s$, indicaremos con $\Paths_s$ el conjunto de todos los caminos
	que se originan en $s$, con $\Paths$ el conjunto de todos los caminos en $\G$ y
	con $\Pathsfin^i$ el conjunto de todos los prefijos finitos de caminos en $\G$
	que terminan en un estado $s \in \St_i$, con $i \in \{\cuad, \diam\}$.
\end{definition}

Ahora sí, podemos definir las dos clases de estrategias que tendremos en un
juego estocástico.

\begin{definition}[Estrategia en un juego estocástico]
	Sea $\G = (\St, (\St_\cuad, \St_\diam), \Act, \theta)$ un SG. Una estrategia $\pi_i$ para el jugador $i$ en $\G$ es una función $\pi: \Pathsfin^i \rightarrow \dist (\Act)$ que asigna una distribución de probabilidad a cada prefijo finito de camino que termina en un estado del jugador $i$ tal que $\pi(\hat \omega) (a) > 0$ solo si $a \in \Act(s)$.

	Llamaremos $\Pi_\cuad$ al conjunto de todas las estrategias del jugador $\cuad$
	y $\Pi_\diam$ al conjunto de todas las estrategias del jugador $\diam$.
\end{definition}

Podemos ver que las estrategias en un juego estocástico se definen igual a cómo
se las definen para procesos de decisión de Markov con la salvedad de que
pertenecerán a un jugador específico. Los distintos tipos de estrategias
presentadas en la sección anterior se extienden naturalemente a SG. Llamaremos
$\Pi^{M}_i$ al conjunto de las estrategias sin memoria del jugador $i$,
$\Pi^{F}_i$ al conjunto de las estrategias de memoria finita del jugador $i$,
$\Pi^{D}_i$ al conjunto de las estrategias puras del jugador $i$, $\Pi^{R}_i$
al conjunto de las estrategias randomizadas del jugador $i$ y las podremos
combinar diciendo por ejemplo que $\Pi^{MD}_i$ es el conjunto de las
estrategias puras sin memoria del jugador $i$ y $\Pi^{FR}_i$ es el conjunto de
las estrategias randomizadas con memoria finita del jugador $i$.

De manera similar a como lo razonamos para procesos de decisión de Markov, si
fijamos dos estrategias $\pi_\cuad \in \Pi_\cuad$ y $\pi_\diam \in \Pi_\diam$
en un juego estocástico $\G$ obtenemos una cadena de Markov a la que
denotaremos $\G^{\picuad, \pidiam}$. Esta cadena de Markov, para cada $s \in
	\St$ definirá una medida de probabilidad $\ProbG$ en la $\sigma$-álgebra de
Borel del conjunto de caminos en $\G$. Si $\varepsilon$ es un conjunto medible
en la $\sigma$-álgebra de Borel, $\ProbG(\varepsilon)$ será la probabilidad de
que las estrategias $\picuad$ y $\pidiam$ sigan un comportamiento en
$\varepsilon$ empezando desde el estado $s$.

\textbf{Un ejemplo de juego estocástico: Roborta vs Rigoborto}

La idea ahora será presentar un juego estocástico. Para eso pensaremos que
mientras que nosotros tenemos a Roborta, la hermana robot de Roborto, un amigo
nuestro tiene un controlador que le permite decidir los movimientos de
Rigoborto, un robot también de características similares a las de Roborto.

Ambos vamos a tener a nuestros robots en un terreno como sobre el que antes
estaba Roborto, con ciertas inclinaciones que pueden hacer que nuestros
comandos no se ejecuten de manera certera. Nuestra idea va a ser modelizar esta
situación a través de un juego estocástico.

Lo que tenemos que tener en cuenta aquí es que, al estar modelizando un juego
en donde tenemos dos actores, no podremos simplemente tener como estado algo al
estilo ``esquina superior izquierda'' por el hecho de que nuestro estado tiene
que codificar donde se encuentran dos robots (que es posible que sean lugares
distintos), además de representar de quién es el turno (que lo indicaremos con
la pertenencia del estado a $\St_\cuad$ o $\St_\diam$).

Entonces, una manera de modelar esta situación en la que tenemos a Roborta y
Rigoborto es hacer que el estado sea un reflejo de las posiciones donde se
encuentran los dos robots con dos pares de coordenadas. Si suponemos que la
grilla 2x2 tiene coordenadas $(0,0)$ para representar la esquina superior
izquierda, $(0,1)$ para la esquina superior derecha, $(1,0)$ para la esquina
inferior izquierda y $(1,1)$ para la inferior derecha, entonces podemos pensar
al estado que representa que Roborta se encuentra en la esquina superior
izquierda y Rigoborto se encuentra en la esquina inferior izquierda como el par
$(0,0) \\ (1,0)$.

Si considerasemos en todas las posiciones que se podrían encontrar Roborta y
Rigoborto tendríamos $16$ ($2^4$) estados. Sin embargo, como dijimos, el estado
debería representar de quién es el turno (es decir, quién elije desde ahí la
próxima acción), por eso, en realidad, si pensamos que desde todas las
posiciones pueden elegir ambos jugadores tendríamos $32$ ($16 \cdot 2$) estados
(por cada configuración en donde pueden estar los robots hay una copia de ese
estado que pertenece a $\St_\cuad$ y otra que pertence a $\St_\diam$).

Ahora, si decidimos con nuestro amigo hacer que los turnos sean intercalados,
es decir, nosotros movemos a Roborta, él mueve a Rigoborto, nosotros otra vez,
él devuelta y así, y definimos que nosotros seamos el jugador $\cuad$ y él el
$\diam$, en la figura~\ref{fig:sg} podemos ver cómo sería un fragmento de este
juego estocástico en donde ambos robots empiezan del lado izquierdo de la
grilla, Roborta en la esquina superior y Rigoborto en la esquina inferior. Las
probabilidades de que las transiciones fallen seguirán la misma caracterización
que dimos para el ejemplo de Roborto.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[->]
		\tikzset{
			state/.style={draw, circle, minimum size=1.2cm, font=\normalsize},
			square state/.style={draw, rectangle, minimum size=1.4cm, font=\normalsize, align=center},
			diamond state/.style={draw, diamond, aspect=1.5, minimum size=1.5cm, font=\normalsize, align=center},
			every node/.style={inner sep=4pt}
		}

		% Nodo raíz a la izquierda
		\node[square state] (left) at (0, 0) {$(0,0)$\\$(1,0)$};

		% Nivel intermedio (rombos) centrados horizontalmente, espaciados verticalmente
		\node[diamond state] (d1) at (3, 4) {$(0,1)$\\$(1,0)$};
		\node[diamond state] (d2) at (3, 0) {$(1,1)$\\$(1,0)$};
		\node[diamond state] (d3) at (3, -4) {$(1,0)$\\$(1,0)$};

		% Nodos hojas (cuadrados), suficientemente separados
		\node[square state] (s1) at (6, 5) {$(0,1)$\\$(0,0)$};
		\node[square state] (s2) at (6, 3) {$(0,1)$\\$(1,1)$};

		\node[square state] (s3) at (6, 1.0) {$(1,1)$\\$(0,0)$};
		\node[square state] (s4) at (6, -1.0) {$(1,1)$\\$(1,1)$};

		\node[square state] (s5) at (6, -3) {$(1,0)$\\$(0,0)$};
		\node[square state] (s6) at (6, -5) {$(1,0)$\\$(1,1)$};

		% Aristas desde left a cada rombo
		\draw[->, draw=violet] (left) to  node[midway, left, text=violet] {$\rightarrow$,0.85} (d1);
		\draw[->, draw=violet]  (left) to[bend left=20]  node[above, text=violet] {$\rightarrow$,0.15} (d2);
		\draw[->, draw=orange] (left) to[bend right=20]  node[below, text=orange] {$\downarrow$, 0.2}  (d2);
		\draw[->, draw=orange] (left) to  node[midway, left, text=orange]  {$\downarrow$, 0.8} (d3);
		% \draw (left) -- (d1);
		% \draw (left) -- (d2);
		% \draw (left) -- (d3);

		% Aristas desde cada rombo a dos cuadrados
		% Desde II
		\draw[->, draw=teal] (d1) to  node[midway, above, text=teal] {$\uparrow$, 1} (s1);
		\draw[->, draw=violet] (d1) to  node[midway, below, text=violet] {$\rightarrow$,1} (s2);
		% \draw (d1) -- (s1);
		% \draw (d1) -- (s2);

		\draw[->, draw=teal] (d2) to  node[midway, above, text=teal] {$\uparrow$, 1} (s3);
		\draw[->, draw=violet] (d2) to  node[midway, below, text=violet] {$\rightarrow$,1} (s4);
		% \draw (d2) -- (s3);
		% \draw (d2) -- (s4);

		\draw[->, draw=teal] (d3) to  node[midway, above, text=teal] {$\uparrow$, 1} (s5);
		\draw[->, draw=violet] (d3) to  node[midway, below, text=violet] {$\rightarrow$,1} (s6);
		% \draw (d3) -- (s5);
		% \draw (d3) -- (s6);

		% Puntos suspensivos para indicar que continúa
		\node at (7,5) {$\cdots$};
		\node at (7,3) {$\cdots$};

		\node at (7,1) {$\cdots$};
		\node at (7,-1) {$\cdots$};

		\node at (7, -3) {$\cdots$};
		\node at (7, -5) {$\cdots$};
	\end{tikzpicture}
	\caption{Fragmento de juego estocástico Roborta vs Rigoborto.}
	\label{fig:sg}
\end{figure}

% Tal como fue aclarado en las dos secciones anteriores, usaremos notación LTL
% para representar conjunto específicos de comportamientos. Sin embargo, estando
% en el análisis de juegos nos interesan hacernos preguntas sobre probabilidades
% de ganar. Para poder hablar de ellas necesitamos formalizar ciertos conceptos
% que presentamos a continuación.

% \subsubsection{Valor en un juego y maneras de ganar}

% \subsubsection{Determinismo}

% \subsubsection{Objetivos en juegos estocásticos}

% Lo primero que será importante notar es que vamos a trabajar con juegos de suma
% cero.

% \subsubsection{Recompensas cualitativas}

% Sea \(C = \{c_1, \ldots, c_n\}\) un conjunto finito de colores, y \(\nu : V
% \rightarrow 2^C\) una valuación. Una clase importante y bien estudiada de
% recompensas cualitativas son las \textbf{funciones características de
% 	subconjuntos \(\omega\)-regulares de \textit{Run}(G)}.

% La pertenencia a un conjunto \(\omega\)-regular de ejecuciones se determina por
% una de las \textbf{condiciones de aceptación} listadas a continuación. Estas
% condiciones corresponden a criterios de aceptación de autómatas de estado
% finito sobre palabras infinitas (ver Sección 5.2.2 para más detalles).

% \begin{itemize}
% 	\item \textbf{Alcanzabilidad y seguridad (Reachability and safety).} Una ejecución \(w \in \textit{Run}(G)\) satisface la condición de \textbf{alcanzabilidad} determinada por un color \(c \in C\) si \(c \in \nu(w(i))\) para algún \(i \geq 0\). La condición de \textbf{seguridad} determinada por \(c\) es dual, es decir, \(c \notin \nu(w(i))\) para todo \(i \geq 0\).

% 	\item \textbf{Büchi y co-Büchi.} Una ejecución \(w \in \textit{Run}(G)\) satisface la condición de \textbf{Büchi} determinada por un color \(c \in C\) si \(c \in \nu(w(i))\) para infinitos \(i \geq 0\). La condición de \textbf{co-Büchi} es dual, es decir, solo hay un número finito de \(i \geq 0\) tal que \(c \in \nu(w(i))\).

% 	\item \textbf{Rabin, Rabin-chain y Street.} Sea \textit{Pairs} un conjunto finito de pares de colores \newline \(\{(c_1, d_1), \ldots, (c_m, d_m)\}\). Una ejecución \(w \in \textit{Run}(G)\) satisface la condición de \textbf{Rabin} determinada por \textit{Pairs} si existe \((c, d) \in \textit{Pairs}\) tal que \(w\) satisface la condición de Büchi determinada por \(d\) y la condición de co-Büchi determinada por \(c\).

% 	      La condición de \textbf{Street} determinada por \textit{Pairs} es dual a Rabin,
% 	      es decir, para cada \((c, d) \in \textit{Pairs}\) tenemos que \(w\) satisface
% 	      la condición de co-Büchi determinada por \(d\) ó la condición de Büchi
% 	      determinada por \(c\).

% 	      Para un color dado \(c\), sea \(V(c)\) el conjunto de todos los \(v \in V\)
% 	      tales que \(c \in \nu(v)\). La condición de \textbf{Rabin-chain} (o paridad) es
% 	      un caso especial de la condición de Rabin donde \textit{Pairs} y \(\nu\)
% 	      satisfacen \(V(c_1) \subset V(d_1) \subset \cdots \subset V(c_m) \subset
% 	      V(d_m)\).

% 	\item \textbf{Muller.} Sea \(M \subseteq 2^C\) un conjunto de subconjuntos de colores. Una ejecución \(w \in \textit{Run}(G)\) satisface la condición de \textbf{Muller} determinada por \(M\) si el conjunto de todos los \(c \in C\) tales que \(w\) satisface la condición de Büchi determinada por \(c\) es un elemento de \(M\). (En criollo, ``los conjuntos de colores que w visita infinitas veces están en M")
% \end{itemize}

% Nótese que los conjuntos \(\omega\)-regulares de ejecuciones son relativamente
% simples en el sentido de que están contenidos en los dos primeros niveles de la
% jerarquía de Borel (los conjuntos de ejecuciones que satisfacen las condiciones
% de alcanzabilidad y seguridad están en el primer nivel).

% distintos tipos y organizacion / ver de que manera presentarlos

% Kucera, omega reg, Chatterjee

% \subsection{Una breve historia sobre los juegos estocásticos}

% Enfoque mc -> mdp -> sg -> psg

% Enfoque juegos deterministas -> juegos estocasticos (Shapley etc)

\section{Juegos deterministas}

Ahora bien, dijimos que cuando queremos modelar tanto comportamiento
adversarial como probabilístico es cuando necesitamos de juegos estocásticos,
pero resulta que también es muy común querer modelar simplemente comportamiento
adversarial. Para esto, existen muchas modelizaciones matemáticas del concepto
de juego (al que podemos considerar ``determinista'' por no ser estocástico).
Para las definiciones que presentaremos a continuación nos basaremos en lo que
se suele conocer en la literatura como juegos de grafo de dos jugadores. Dentro
del estudio de juegos de grafos se presentan varias categorías que se pueden
paralelizar con las definiciones que vimos: un juego de grafo de un jugador
sería simplemente un sistema de transición, un juego de grafo de 1 jugador y
medio sería lo que se entiende por proceso de decisión de Markov y un juego de
grafo de 2 jugadores y medio sería lo que presentamos como juegos estocásticos.

Procedemos entonces con la definición de juego de grafo de dos jugadores:

\begin{definition}[juego de grafo de 2 jugadores]
	Definimos a un juego de grafo de dos jugadores (2G) como una tupla $G = (V, V_\cuad,V_\diam, E)$ donde $V = V_\cuad \biguplus V_\diam$ es un conjunto de vértices (o estados) particionado en $V_\cuad$ y $V_\diam$, y $E \subseteq (V\times V)$ es una relación que denota el conjunto de aristas (dirigidas) que representan transiciones de un estado a otro del juego.

	Los 2 jugadores son llamados $\cuad$ y $\diam$ y controlan los vértices
	$V_\cuad$ y $V_\diam$, respectivamente.
\end{definition}

Puede ser de interés notar que en el contexto de juegos de grafo hablamos de
vértices y no de estados como en los juegos estocásticos. Además, para los
juegos deterministas volveremos a la noción de camino como secuencia de
vertices solamente, a los cuales notaremos generalmente con $\rho$ en
contraposición a los caminos siendo notados con $\omega$ para juegos
estocásticos.

\begin{definition}[camino sobre un 2G]
	Un camino (o jugada) en un juego de grafo de dos jugadores es una secuencia infinita de vértices $\rho = v_0v_1v_2 \dots \in V^\omega$, donde para todo $i \in \N_0$ tenemos que $v_i \in V$ y $(v_i, v_{i+1}) \in E$.

	% Esta definición de respeta la damos con 
	% Sean $\sigma_\cuad$ y $\sigma_\diam$ un par de estrategias y sea $v_0$ un
	% vértice inicial, la jugada que cumple con $\sigma_\cuad$ y $\sigma_\diam$ es la
	% única jugada $\rho = v_0 v_1 v_2 \dots$ para la cual cada $i \in \N_0$, si $v_i
	% 	\in V_\cuad$ entonces $v_{i+1} = \sigma_\cuad(v_0 \dots v_i)$, y si $v_i \in
	% 	V_\diam$ entonces $v_{i+1} = \sigma_\diam(v_0 \dots v_i)$.
\end{definition}

También para el análisis de juegos deterministas introduciremos el concepto de
estrategia. Esta vez no para poder definir una medida de probabilidad ya que no
necesitaremos una, sino para poder estudiar clases de comportamiento
adversarial y poder formular y responder preguntas de investigación que se
hacen en el contexto del estudio de juegos, las que precisaremos con más
detalle en el capítulo siguiente.

% No necesitamos las estrategias para definir una medida de probabilidad pero si
% para analizar el comportamiento adversarial -> algo capaz de preguntas acá?

\begin{definition}[estrategia sobre un 2G]
	Una estrategia para un jugador $i$ con $i \in \{\cuad, \diam\}$ es una función $\sigma_i: V^*V_i \rightarrow V$ con la restricción de que $\sigma_i(wv) \in E(v)$ para todo $wv \in V^*V_i$.
\end{definition}

Si otra vez queremos pensar en cómo se podrían categorizar las estrategias,
podemos ver fácilmente que las nociones de con memoria finita o sin memoria se
pueden extender de manera intuitiva, pero que todas las estrategias serían
puras (o deterministas).

% \begin{definition}[estrategia sin memoria en un 2G]

% \end{definition}

% \begin{definition}[estrategia con memoria finita en un 2G]

% \end{definition}
% % algo de randomizadas y deterministas?

\textbf{Roborta y Rigoborto llegan a mejor terreno}

Supongamos que en vez de estar en el mismo terreno que Roborto, ahora con
nuestro amigo, movemos a Roborta y Rigoborto a un terreno súper llano, que nos
asegura que las indicaciones que enviamos a nuestros robots se realizarán de
manera certera. Entonces, ahora podemos modelar nuestra situación simplemente
como un juego determinista.

Podemos modelar los estados de igual manera, plantear los turnos de igual
manera y ver ya cómo se simplifica el fragmento de juego que antes habíamos
propuesto en la figura~\ref{fig:juego-determinista}.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[->]
		\tikzset{
			state/.style={draw, circle, minimum size=1.2cm, font=\normalsize},
			square state/.style={draw, rectangle, minimum size=1.4cm, font=\normalsize, align=center},
			diamond state/.style={draw, diamond, aspect=1.5, minimum size=1.5cm, font=\normalsize, align=center},
			every node/.style={inner sep=4pt}
		}

		% Nodo raíz a la izquierda
		\node[square state] (left) at (0, 0) {$(0,0)$\\$(1,0)$};

		% Nivel intermedio (rombos) centrados horizontalmente, espaciados verticalmente
		\node[diamond state] (d1) at (3, 2) {$(0,1)$\\$(1,0)$};
		%\node[diamond state] (d2) at (3, 0) {$(1,1)$\\$(1,0)$};
		\node[diamond state] (d3) at (3, -2) {$(1,0)$\\$(1,0)$};

		% Nodos hojas (cuadrados), suficientemente separados
		\node[square state] (s1) at (6, 3) {$(0,1)$\\$(0,0)$};
		\node[square state] (s2) at (6, 1) {$(0,1)$\\$(1,1)$};

		% \node[square state] (s3) at (6, 1.0) {$(1,1)$\\$(0,0)$};
		% \node[square state] (s4) at (6, -1.0) {$(1,1)$\\$(1,1)$};

		\node[square state] (s5) at (6, -1) {$(1,0)$\\$(0,0)$};
		\node[square state] (s6) at (6, -3) {$(1,0)$\\$(1,1)$};

		% Aristas desde left a cada rombo
		\draw (left) -- (d1);
		%\draw (left) -- (d2);
		\draw (left) -- (d3);

		% Aristas desde cada rombo a dos cuadrados
		\draw (d1) -- (s1);
		\draw (d1) -- (s2);

		% \draw (d2) -- (s3);
		% \draw (d2) -- (s4);

		\draw (d3) -- (s5);
		\draw (d3) -- (s6);

		% Puntos suspensivos para indicar que continúa
		\node at (7,3) {$\cdots$};
		\node at (7,1) {$\cdots$};

		% \node at (7,1) {$\cdots$};
		% \node at (7,-1) {$\cdots$};

		\node at (7, -1) {$\cdots$};
		\node at (7, -3) {$\cdots$};

	\end{tikzpicture}
	\caption{Fragmento de juego determinista: Roborta vs Rigoborto}
	\label{fig:juego-determinista}
\end{figure}

Ahora bien, una pregunta que podría surgir con este modelado es, ¿para qué lo
hacemos?, si aquí no tendremos la oportunidad de preguntarnos por la
probabilidad de tomar ciertos caminos. Bueno, la realidad es que los juegos
(tanto estocásticos como deterministas) suelen venir equipado con lo que se
llaman objetivos. A partir de estos objetivos es que podremos pensar en la
pregunta típica adversarial, ``¿quién gana?''. Profundizaremos sobre estas
nociones en el próximo capítulo.

% capaz cambiar medidas por los nodos que se van

% \section{Comparación de los distintos juegos}

% Incluir cuadros comparativos entre juegos // ejemplos para todas las nociones
% nuevas explicadas

% (Capaz agregar un apéndice con cuadritos de esto ?)

% (estaría bueno ver problemas y agregar un apéndice con ejemplos?)