\chapter{Objetivos de Rabin en juegos estocásticos politópicos}
~\label{cap:results}

\input{Capitulos/chapter5-subchapter1-pmdp.tex}

\section{Juegos justos: la respuesta a la pregunta cualitativa}

\subsection{Draft}

\subsection{Juegos de adversario justo}

Sea $G$ un juego de grafo de dos jugadores y sea $E^{l} \subseteq (V_1 \times
	V) \cap E$ un conjunto dado de aristas vivas \hl{ver traducción}. Sea $V^{l} :=
	\mathrm{dom}(E^{l})$ el conjunto de vértices del jugador $\diam$ que están en
el dominio de $E^{l}$. Intuitivamente, las aristas en $E^{l}$ representan
suposiciones de justicia sobre el jugador $\diam$: para cada arista $(v, v')
	\in E^{l}$, si $v$ es visitado infinitamente a menudo en una jugada, se espera
que la arista $(v, v')$ sea elegida también infinitamente a menudo por el
jugador $\diam$. Es decir, si un vértice $v$ es visitado infinitamente, se
espera también que toda arista viva saliente de $v$ sea tomada infinitamente a
menudo.

Denotamos por $G^{l} = \langle G, E^{l} \rangle$ a un juego de grafo de dos
jugadores con aristas vivas, y extendemos nociones como jugadas, estrategias,
condiciones ganadoras, regiones ganadoras, etc., de juegos de grafo a juegos de
grafos con aristas vivas. Una jugada $\pi$ sobre $G^{l}$ es fuertemente justa
si satisface la siguiente fórmula en lógica temporal lineal (LTL):

\[
	\alpha := \bigwedge_{(v, v') \in E^{l}} \left( \siempevent v \rightarrow \siempevent (v \wedge \bigcirc v') \right).
\]

Dado $G^{l}$ y una condición de victoria $\varphi$, el jugador $\cuad$ gana el
juego de adversario justo sobre $G^{l}$ con respecto a la condición de victoria
$\varphi$ desde un vértice $v_0 \in V$ si gana el juego sobre $G^{l}$ para la
condición de victoria $\alpha \rightarrow \varphi$ desde $v_0$.

Hay dos observaciones interesantes sobre los juegos de adversario justo.

Primero, las aristas vivas permiten descartar ciertas estrategias del jugador
$\diam$, facilitando que el jugador $\cuad$ gane en determinadas situaciones.
Por ejemplo, consideremos un grafo de juego (figura X, parte superior) con dos
vértices $p$ y $q$. El vértice $p$ pertenece al jugador $\diam$ y el vértice
$q$ al jugador $\cuad$. La arista $(p, q)$ es una arista viva (representada con
línea discontinua). Supongamos que la especificación para el jugador $\cuad$ es
$\varphi = \siempevent q$. Si la arista $(p, q)$ no fuera viva, el jugador
$\cuad$ no ganaría desde $p$, porque el jugador $\diam$ podría mantener el
juego atrapado en $p$ eligiéndose a sí mismo como sucesor en cada turno. En
contraste, el jugador $\cuad$ gana desde $p$ en el juego adversarial justo,
porque la suposición de liveness sobre la arista $(p, q)$ fuerza al jugador
$\diam$ a elegir infinitamente a menudo la transición hacia $q$.

Segundo, las suposiciones de justicia modeladas por aristas vivas restringen
las elecciones de estrategias del jugador $\diam$ menos que lo que
restringirían la suposición de que el jugador $\diam$ elige probabilísticamente
entre estas aristas. Consideremos, por ejemplo, un juego de adversario justo
con un único vértice del jugador $\diam$, $p$ (cuadrado) con dos aristas vivas
salientes hacia los estados $q$ y $q'$, como se muestra en la Figura 1 (parte
inferior). Si el jugador $\diam$ elige aleatoriamente entre las aristas $(p,
	q)$ y $(p, q')$, toda secuencia finita de visitas a los estados $q$ y $q'$
ocurrirá infinitamente a menudo con probabilidad uno. Esto no es cierto en el
juego de adversario justo. Aquí el jugador $\diam$ puede elegir una secuencia
particular de visitas a $q$ y $q'$ (por ejemplo, simplemente $qq'qq'qq'\dots$),
siempre que ambos sean visitados infinitamente a menudo.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[>=Stealth, node distance=3.5cm, font=\scriptsize]
		% Definición de estilos para los nodos
		\tikzset{
			state/.style={draw, circle, minimum size=1.2cm, font=\normalsize},
			square state/.style={draw, rectangle, minimum size=1.2cm, font=\normalsize},
			diamond state/.style={draw, diamond, aspect=1.5, minimum size=1.5cm,font=\normalsize}
		}

		% --- Figura superior ---.
		\node[diamond state] (p1) at (0,0) {$p$};
		\node[square state] (q1) [right of=p1] {$q$};

		% Transiciones:
		\draw[->] (p1) edge[loop left] (p1);
		\draw[->, dashed] (p1) to[bend left=20] (q1);
		\draw[->] (q1) edge[bend left=20] (p1);

		% --- Figura inferior ---
		\node[diamond state] (p2) at (0,-3.5) {$p$};
		\node[square state] (q2) [above right=0.5cm and 1cm of p2] {$q$};
		\node[square state] (q2p) [below right=0.5cm and 1cm of p2]  {$q'$};

		% Transiciones:
		\draw[->, dashed] (p2) to[left] (q2);
		\draw[->, dashed] (p2) edge[left] (q2p);
		\draw[->] (q2) edge[bend left=20] (p2);
		\draw[->] (q2p) edge[bend left=20] (p2);

	\end{tikzpicture}
	\caption{Dos juegos de adversario justo.}
	\label{fig:juego-justo}
\end{figure}

\subsection{Desrandomización de juegos estocásticos politópicos}

Dada la interpretación de un juego estocástico politópico $\Gk = (\St,
	(\St_\cuad, \St_\diam), \Act, \theta)$, se define la \textit{desrandomización
	de} $\Gk$, $\DGk := (( V, V_\cuad, V_\diam, E ), E^l )$, como el (vamos con
esta traducción?) juego de grafo de 2 jugadores extremadamente equitativo con:

$$
	V = \St \cup \left[ \newV \right]
$$

$$
	V_\cuad = \St_\cuad
$$

$$
	V_\diam = \St_\diam \cup \left[ \newV \right]
$$

$$
	E = \{(s, v_{V'}) : V' \in V_s\}
$$

$$
	E^l = \{(v_{V'}, s') : s' \in V'\}
$$

%donde $V_s$ es el conjunto formado por los subconjuntos de vértices que forman las distintas distribuciones que definirían el próximo estado desde $s$:

y para el cual se cumple la siguiente condición de equidad:

$$
	\varphi^l := \bigwedge_{(v_{V'},s') \in E^l} (\siempevent v_{V'} \rightarrow \siempevent (v_{V'} \wedge \bigcirc s' ))
$$

con esto de comportamiento, los caminos serían iguales y las estrategias quedan
bien. lo que cambia es lo de determinista que ya estaba

pero capaz va bien hacer unos ejemplos de caminos y estrategias para ver la
relación entre PSG y su desrandomización.

\hl{¿Cómo represento gráficamente la idea de politopos? ¿Capaz dándoles forma a las acciones?}

\subsection{Prueba de igualdad sobre los conjuntos ganadores}

Agg cambiando lo de las acciones

\section{Transformar Rabin en alcanzabilidad: la respuesta a varias preguntas}

\subsection{Draft}

La idea es intentar hacer algo al estilo

\begin{theorem}[Reducción de Rabin]
	Sea $\Gk$ un juego estocástico politópico y sea $\Hk$ su interpretación extrema tal como se presenta en \cite{Polytopal}. Si tenemos una propiedad de Rabin $R$, y su respectivo conjunto de estados \textit{almost sure winning} $W_R$ entonces valen las siguientes ecuaciones:

	\begin{align*}
		&\inf_{\piGdiam} \sup_{\piGcuad} \ProbG(R) = \\
		= &\inf_{\piGdiam} \sup_{\piGcuad} \ProbG(\alc W_R) = \\
		= &\inf_{\piHdiamMD} \sup_{\piHcuadMD} \ProbH(\alc W_R) = \\
		= &\sup_{\piHcuadMD} \inf_{\piHdiamMD} \ProbH(\alc W_R) = \\
		= &\sup_{\piGcuad} \inf_{\piGdiam} \ProbG(\alc W_R) = \\
		= &\sup_{\piGcuad} \inf_{\piGdiam} \ProbG(R)
	\end{align*}

\end{theorem}

\begin{proof}
	\begin{align*}
		\inf_{\piGdiam} &\sup_{\piGcuad} \ProbG(R) \\
		&\leq \inf_{\piGdiam} \sup_{\piGcuad} \ProbG(\alc W_R)
		& \text{(por lema \ref{*1})}
		\\
		&\leq \inf_{\piHdiamMD} \sup_{\piHcuadMD} \ProbH(\alc W_R)
		& \text{(por Teorema 1)}
		\\
		&\leq \sup_{\piHcuadMD} \inf_{\piHdiamMD} \ProbH(\alc W_R)
		& \text{(por Teorema 1)}
		\\
		&\leq \sup_{\piGcuad} \inf_{\piGdiam} \ProbG(\alc W_R)
		& \text{(por Teorema 1)}
		\\
		&\leq \sup_{\piGcuad} \inf_{\piGdiam} \ProbG(R)
		& \text{(por lema \ref{**2})}
		\\
		&\leq \inf_{\piGdiam} \sup_{\piGcuad} \ProbG(R)
		& \text{(por prop de sup e inf)}
	\end{align*}
\end{proof}

\begin{lemma}
	\label{*1}
	Sea $\Gk$ la interpretación (?) de un juego estocástico politópico y sea $R$ una propiedad límite y $W_R$ su correspondiente conjunto casi seguramente ganador. Entonces vale que
	$$
		\inf_{\pidiamset} \sup_{\picuadset} \ProbG (R) \leq \inf_{\pidiamset} \sup_{\picuadset} \ProbG (\alc W_R)
	$$
\end{lemma}

\begin{proof}
	Nombremos $\pidiam^*$ a una estrategia tal que $$ \sup_{\picuadset} \Prob_{\Gk, s}^{\picuad, \pidiam^*} (\alc W_P)= \inf_{\pidiamset} \sup_{\picuadset} \ProbG(\alc W_P)$$ (explicar algo más hablado? decir algo sobre su existencia?). Ahora podemos hacer las siguientes deducciones:

	\begin{align*}
		\inf_{\pidiamset} &\sup_{\picuadset} \ProbG(P) \\
		&\leq \sup_{\picuadset} \Prob_{\Gk, s}^{\picuad, \pidiam^*} (P) &\text{(por def. de inf)} \\
		&= \sup_{\picuadset} \Prob_{\Gk, s}^{\picuad, \pidiam^*} (\alc W_P) &\text{(por lema \ref{PMDP-supWR})} \\
		&= \inf_{\pidiamset} \sup_{\picuadset} \ProbG(\alc W_P) &\text{(por def. de $\pidiam^*$)}
	\end{align*}

	y con esto hemos probado el enunciado.
\end{proof}

\begin{lemma}
	\label{PMDP-supWR}
	Sea $\M$ un PMDP, $\pi$ una estrategia en $\M$, $s$ un estado en $\M$, $R$ una propiedad de Rabin y $W_R$ el conjunto casi seguramente ganador de $R$ (en el juego estocástico debería ser, no?), entonces vale que:

	% CAMBIAR WR por conjunto desde el que existe una estr que da prob 1 de ganar

	\hl{¿Cambiar wr por conjunto desde el que eciste una estrategia que da probabilidad 1 de ganar?}

	$$
		\sup_{\pi \in \Pi} \ProbPMDP (R) = \sup_{\pi \in \Pi} \ProbPMDP (\alc W_R)
	$$
\end{lemma}

\begin{proof}
	Por definición de conjunto casi seguramente ganador
	$$
		\Prob_{\M,s}^\pi (R) = \Prob_{\M,s}(\{ \omega \in \paths(s) \mid \inft(\omega) \models R\})
	$$

	Claramente los caminos que hacen que valga la propiedad también son caminos
	desde donde llego a algún estado desde donde existen estrategias para ganar con
	probabilidad 1. Por lo tanto, $\ProbPMDP (R) \leq \ProbPMDP (\alc W_R)$.

	Para ver que vale la igualdad, veremos que existe una estrategia de memoria
	finita $\pi$ que hace que $\ProbPMDP (R) = \sup_{\pi \in \Pi} \ProbPMDP (\alc
		W_R) $. Para ello, consideremos la estrategia sin memoria $\pi_0$ que maximiza
	las probabilidades de alcanzar $W_R$ desde todos los estados $s \in \M$
	(sabemos que esta estrategia existe por \cite{Polytopal, CONDON1992}). A su
	vez, sabemos que para cada estado $s \in W_R$ existe una estrategia $\pi_s$ que
	asegura ganar con probabilidad 1 frente al objetivo $R$.

	Sea entonces $\pi$ la estrategia que primero se comporta como $\pi_0$, hasta
	llegar a un estado $t$ en $W_R$, y a partir de allí $\pi$ se comporta como
	$\pi_t$. Con eso tenemos que:

	\begin{align*}
		\ProbPMDP (R) &= \sum_{t \in W_R} \Prob_{\M,s}^{\pi_0} ((\neg W_R \until t)) \cdot \underbrace{\Prob_{\M,t}^{\pi_t}(R)}_{=1} \\
		&= \sup_{\pi \in \Pi} \ProbPMDP (W_R)
	\end{align*}

	Como $\sup_{\pi \in \Pi} \ProbPMDP (W_R)$ es una cota superior las
	probabilidades para $R$ bajo todas las estrategias, con esto podemos concluir
	la igualdad $\sup_{\pi \in \Pi} \ProbPMDP (R) = \sup_{\pi \in \Pi} \ProbPMDP
		(\alc W_R)$.
\end{proof}

\hl{Esto es lo importante que habría que probar y no está probado.}

\begin{lemma}
	\label{**2}
	Sea $\Gk$ la interpretación (?) de un juego estocástico politópico y sea $R$ una propiedad límite y $W_R$ su correspondiente conjunto casi seguramente ganador. Entonces vale que
	$$
		\sup_{\picuadset} \inf_{\pidiamset} \ProbG (\alc W_R) \leq \sup_{\picuadset} \inf_{\pidiamset}  \ProbG (R)
	$$
\end{lemma}

