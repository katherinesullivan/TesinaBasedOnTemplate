\section{Juegos justos: la respuesta a la pregunta cualitativa}
\label{sec:prueba}

A fin de responder la pregunta ``¿desde qué estados existe probabilidad uno de
ganar?", presentaremos un tipo nuevo de juegos deterministas: los juegos
justos. Probaremos que el conjunto de vértices casi seguramente ganadores en un
juego estocástico politópico con un objetivo de Rabin es igual al conjunto de
vértices ganadores de un juego justo construido a partir del PSG.

Para eso primero presentaremos el concepto de juego justo y la construcción del
juego justo a partir del PSG. Luego, explicitaremos un poco más la relación
entre el PSG y el juego justo construido, al cual llamaremos su
desrandomización. Seguido de eso, expondremos la prueba formal de la igualdad
entre los conjuntos. Y, por último, mostraremos un algoritmo para la sintésis
de estrategias de memoria finita casi seguramente ganadoras para un PSG con un
objetivo de Rabin.

\subsection{Juegos de adversario justo}

Sea $G$ un juego de grafo de dos jugadores y sea $E^{l} \subseteq (V_\diam
	\times V) \cap E$ un conjunto dado de aristas que deben ser tomadas
infinitamente a menudo si el estado del que parten es visitado infinitamente a
menudo. Nombraremos $V^{l} := \mathrm{dom}(E^{l})$ al conjunto de vértices del
jugador $\diam$ que está en el dominio de $E^{l}$. Las aristas en $E^{l}$
representarán suposiciones de equidad sobre el jugador $\diam$: para cada
arista $(v, v') \in E^{l}$, si $v$ es visitado infinitamente a menudo en una
jugada, se espera que la arista $(v, v')$ sea elegida también infinitamente a
menudo por el jugador $\diam$. Es decir, si un vértice $v$ es visitado
infinitas veces, se espera también que toda arista en $E^l$ saliente de $v$ sea
tomada una cantidad infinita de veces.

Denotamos por $G^{l} = \langle G, E^{l} \rangle$ a un juego de adversario
justo, y extendemos nociones como jugadas, estrategias, condiciones ganadoras,
regiones ganadoras, etc. de juegos deterministas de manera natural. Una jugada
$\rho$ sobre $G^{l}$ se dice fuertemente justa si satisface la siguiente
fórmula en lógica temporal lineal (LTL):
\[
	\alpha := \bigwedge_{(v, v') \in E^{l}} \left( \siempevent v \rightarrow \siempevent (v \wedge \bigcirc v') \right).
\]
Dado $G^{l}$ y una condición de victoria $\varphi$, el jugador $\cuad$ gana el
juego de adversario justo sobre $G^{l}$ con respecto a la condición de victoria
$\varphi$ desde un vértice $v_0 \in V$ si gana el juego sobre $G^{l}$ para la
condición de victoria $\alpha \rightarrow \varphi$ desde $v_0$.

Hay dos observaciones interesantes para hacer sobre los juegos de adversario
justo:

Primero, las aristas en $E^l$ permiten descartar ciertas estrategias del
jugador $\diam$, facilitando que el jugador $\cuad$ gane en determinadas
situaciones. Por ejemplo, consideremos un grafo de juego (figura X, parte
superior) con dos vértices $p$ y $q$. El vértice $p$ pertenece al jugador
$\diam$ y el vértice $q$ al jugador $\cuad$. La arista $(p, q)$ es una arista
en $E^l$ (representada con línea discontinua). Supongamos que la especificación
para el jugador $\cuad$ es $\varphi = \siempevent q$. Si la arista $(p, q)$ no
estuviera en $E^l$, el jugador $\cuad$ no ganaría desde $p$, porque el jugador
$\diam$ podría mantener el juego atrapado en $p$ eligiéndose a sí mismo como
sucesor en cada turno. En contraste, el jugador $\cuad$ gana desde $p$ en el
juego adversarial justo, porque la suposición de equidad sobre la arista $(p,
	q)$ fuerza al jugador $\diam$ a elegir infinitamente a menudo la transición
hacia $q$.

Segundo, las suposiciones de equidad modeladas por aristas en $E^l$ restringen
las elecciones de estrategias del jugador $\diam$ menos que lo que
restringirían la suposición de que el jugador $\diam$ elige probabilísticamente
entre estas aristas. Consideremos, por ejemplo, un juego de adversario justo
con un único vértice del jugador $\diam$, $p$ con dos aristas en $E^l$
salientes hacia los estados $q$ y $q'$, como se muestra en la Figura 1 (parte
inferior). Si el jugador $\diam$ elige aleatoriamente entre las aristas $(p,
	q)$ y $(p, q')$, toda secuencia finita de visitas a los estados $q$ y $q'$
ocurrirá infinitamente a menudo con probabilidad uno. Esto no es cierto en el
juego de adversario justo. Aquí el jugador $\diam$ puede elegir una secuencia
particular de visitas a $q$ y $q'$ (por ejemplo, simplemente $qq'qq'qq'\dots$),
siempre que ambos sean visitados infinitamente a menudo.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[>=Stealth, node distance=3.5cm, font=\scriptsize]
		% Definición de estilos para los nodos
		\tikzset{
			state/.style={draw, circle, minimum size=1.2cm, font=\normalsize},
			square state/.style={draw, rectangle, minimum size=1.2cm, font=\normalsize},
			diamond state/.style={draw, diamond, aspect=1.5, minimum size=1.5cm,font=\normalsize}
		}

		% --- Figura superior ---.
		\node[diamond state] (p1) at (0,0) {$p$};
		\node[square state] (q1) [right of=p1] {$q$};

		% Transiciones:
		\draw[->] (p1) edge[loop left] (p1);
		\draw[->, dashed] (p1) to[bend left=20] (q1);
		\draw[->] (q1) edge[bend left=20] (p1);

		% --- Figura inferior ---
		\node[diamond state] (p2) at (0,-3.5) {$p$};
		\node[square state] (q2) [above right=0.5cm and 1cm of p2] {$q$};
		\node[square state] (q2p) [below right=0.5cm and 1cm of p2]  {$q'$};

		% Transiciones:
		\draw[->, dashed] (p2) to[left] (q2);
		\draw[->, dashed] (p2) edge[left] (q2p);
		\draw[->] (q2) edge[bend left=20] (p2);
		\draw[->] (q2p) edge[bend left=20] (p2);

	\end{tikzpicture}
	\caption{Dos juegos de adversario justo.}
	\label{fig:juego-justo}
\end{figure}

\subsection{Desrandomización de PSGs}

Dada la interpretación de un juego estocástico politópico $\Gk = (\St,
	(\St_\cuad, \St_\diam), \Act, \theta)$, se define la \textit{desrandomización
	de} $\Gk$, $\DGk := (( V, V_\cuad, V_\diam, E ), E^l )$, como el (vamos con
esta traducción?) juego de grafo de 2 jugadores extremadamente equitativo con:

\begin{alignat*}{2}
	&\tilde V &&= \newV \\
	&V &&= \St \cup \tilde V \\
	&V_\cuad &&= \St_\cuad \\
	&V_\diam &&= \St_\diam \cup \tilde V \\
	&E &&= \{(s, v_{V}) : V \in V_s\} \\
	&E^l &&= \{(v_{V}, s') : s' \in V\}
\end{alignat*}

y para el cual se cumple la siguiente condición de equidad: $$ \varphi^l :=
	\bigwedge_{(v_{V'},s') \in E^l} (\siempevent v_{V'} \rightarrow \siempevent
	(v_{V'} \wedge \bigcirc s' )) $$

Esta misma condición de equidad se puede expresar como

\begin{center}
	$ \varphi^l : = \bigwedge_{v_V \in \tilde V} \varphi^V $, donde

	$ \varphi^V := \bigwedge_{s' \in V} (\siempevent v_{V'} \rightarrow \siempevent (v_{V'} \wedge \bigcirc s' ))$
\end{center}

\subsection{Relación entre un PSG y su desrandomización}

Nos será útil pensar en transformaciones entre los distintos juegos para la
prueba que tenemos más adelante, así que veremos algunas definiciones. Antes,
fijemos la interpretación de un juego estocástico politópico $\Gk$ y su
desrandomización $\DGk$.

\textbf{Condición de equidad en el juego estocástico politópico}

Podemos expresar fácilmente la condición de equidad en el juego estocástico
politópico de la siguiente forma:

\begin{center}
	$
		\hat \varphi^l := \bigwedge_{\alpha \in \Act} \hat \varphi^{\supp(\alpha)}
	$, con

	$
		\hat \varphi^{\supp(\alpha)} := \bigwedge_{s' \in \supp(\alpha)} (\siempevent \alpha \rightarrow \siempevent (\alpha \wedge \bigcirc s' ))
	$
\end{center}

\textbf{Comportamientos y caminos}

Dado un comportamiento $\omega = (s_0, \alpha_0, s_1, \alpha_1, \dots)$ en
$\Gk$, podemos obtener un único camino en $\DGk$ al que llamaremos
$\derand(\omega)$ y tendrá la siguiente forma: $$\derand(\omega) = (s_0,
	v_{\supp(\alpha_0)}, s_1, v_{\supp(\alpha_1), \dots})$$ Por otro lado, si
tenemos un camino $\rho = (s_0, v_{V_0}, s_1, v_{V_1}, \dots)$ en $\DGk$,
existen varios comportamientos en $\Gk$ que se corresponderían con él.
Llamaremos a este conjunto de comportamientos $\rand(\rho)$. Es decir,
$$\rand(\rho) = \{(s_0, \alpha_0, s_1, \alpha_1, \dots) \in \Omega_{\Gk,s} \mid
	\forall i \geq 0, \ \supp(\alpha_i) = V_i\}$$

Estas definiciones también se extienden a prefijos finitos de comportamientos y
caminos de manera natural.

\textbf{Randomización de estrategias}

Supongamos que tenemos una estrategia $\sigma_i$ en $\DGk$ y queremos construir
una estrategia $\pi_i$ en $\Gk$ que tenga un comportamiento similar. $\sigma_i$
está definida para todos los prefijos finitos de caminos $\rho$ en $\DGk$ y
nuestra idea es definir $\pi_i$ para todos los prefijos finitos de
comportamiento $\omega$ en $\Gk$. La idea, entonces, será definir $\pi_i$ de
igual manera para todos los elementos del conjunto $\rand(\rho)$.

Solo nos va a interesar ver cómo $\sigma_i$ se comporta en los prefijos finitos
de caminos $\rho$ que terminen en un estado $s \in \St$ \footnote{Desde los
	prefijos finitos de caminos donde el último elemento es un vértice $v_V$, las
	decisiones son tomadas por el jugador $\diam$ en $\DGk$, pero solo reflejan lo
	que sería la decisión probabilística en $\Gk$.}. Para cada uno de estos $\rho$,
tenemos que $\sigma_i(\rho) = v_{\widehat{V}}$ para algún $v_V$. Lo que haremos
para la construcción de $\pi_i$ es para cada $\omega \in \derand(\rho)$ definir
$\pi_i(\omega)(\hat\alpha) = 1$ para algún $\hat\alpha$ particular tal que
$\supp(\hat\alpha) = \widehat{V}$.

Llamaremos a esta nueva estrategia $\pi_i$ obtenida a partir de $\sigma_i$,
$\rand(\sigma_i)$.

\begin{center}
	$\sigma_i(s_0, v_{V_0}, \dots, s_k) = v_{V_k} \implies \rand(\sigma_i)(s_0, \alpha_0, \dots, s_k)(\alpha_k) = 1$ donde \\ $\forall i \ \supp(\alpha_i) = V_i$.
	\captionof{figure}{Una forma de ver la randomización de estrategias en $\DGk$}
\end{center}
% \begin{figure}[ht]
% 	\centering
% 	\begin{equation*}
% 		\sigma_i(s_0, v_{V_0}, \dots, s_k) = v_{V_k} \implies
% 		\rand(\sigma_i)(s_0, \alpha_0, \dots, s_k)(\alpha_k) = 1
% 	\end{equation*}
% 	\caption{Una forma de ver la randomización de estrategias}
% \end{figure}

\textbf{Desrandomización de estrategias}

\hl{Todavía no me convence la redacción de esto}

Supongamos ahora que tenemos una estrategia $\pi_i$ en $\Gk$ y queremos
construir una estrategia $\sigma_i$ en $\DGk$ que tenga un comportamiento
similar.

Para cada prefijo finito de camino $\rho$ debemos definir qué vértice será el
que elija $\sigma_i(\rho)$. La idea será que elegiremos un vértice $v_V$ que se
corresponde al soporte de una acción a la que $\pi_i(\omega)$ (siendo $\omega$
tal que $\derand(\omega) = \rho$) le asigne una probabilidad positiva.

Como existen varios prefijos de comportamiento que podrían cumplir con la
condición de $\omega$, la definición de $\derand(\pi_i)$ dependerá de la
elección de un prefijo de comportamiento $\omega$ para cada prefijo de camino
$\rho$ tal que $\derand(\omega) = \rho$.

Además, como cada acción le puede dar probabilidad positiva a varias acciones
con distintos soportes, la definición de $\derand(\pi_i)$ también dependerá de
la elección de una acción a la que $\pi_i(\omega)$ le asigne una probabilidad
positiva (por cada $\omega$ seleccionado en el paso anterior).

\begin{boxgris}[¿Cómo se podrían tomar esas elecciones?]{}
	Suponiendo que tenemos una estrategia $\pi_i$ en $\Gk$, es probable que a su vez tengamos comportamientos $\omega'_1, \omega'_2, \dots$ en $\Gk$ específicos en que estemos interesados que $\sigma_i$ replique. En ese caso, una manera de tomar las decisiones antes planteadas es usando los prefijos finitos de los distintos $\omega'_k$ para las decisiones de $\sigma_i$. Esto sería: si tenemos un prefijo de comportamiento $\hat \omega = (s_1, \alpha_1, \dots, s_m, \alpha_m)$ (que respeta $\pi_i$), entonces haremos que para $\hat \rho = (s_1, v_{\supp(\alpha_1)}, \dots, s_m)$, $\sigma_i(\hat \rho) = v_{\supp(\alpha_m)}$.
\end{boxgris}

Con estas decisiones tomadas, podemos definir como se comportará $\sigma_i$
para cada prefijo de camino que termina en un estado $s \in \St$.

Ahora bien, para el caso de $\sigma_\diam$ también hay que definir cómo se
comporta la estrategia en los caminos que terminan en los vértices de la forma
$v_V$. Es decir, debemos elegir qué estado $s_{k+1}$ será el que cumpla
$\sigma_\diam(s_0, v_{V_0}, \dots, s_k, v_{V_k}) = s_{k+1}$ para cada $k$. En
este caso, lo que nos debemos asegurar es que se cumpla la condición de
equidad.

% Se podría pensar como tener esto aparte o enmarcado en una box. Se podría agregar otra box con lo que sería medio la precontinuacion del punto 2
\begin{boxgris}[¿Cómo asegurar la condición de equidad?]{}
	Presentamos dos maneras fáciles con las cuales se podría asegurar esto, pero
	podrían existir infinidades de ellas:
	\begin{enumerate}
		\item como cada $V_k$ es finito, podemos numerar cada $s^{V_k} \in V_k$ de manera
		      $s_0^{V_k}, s_1^{V_k}, \dots$. Esto nos permite seleccionar el próximo estado
		      en base a la cantidad de veces que se visitó $v_{V_k}$. Si $n$ fueron las veces
		      que se visitó $v_{V_k}$ en el prefijo finito de camino $\rho '$, entonces
		      podemos definir $\sigma_\diam(\rho ' v_{V_k}) = s_n^{V_k}$.
		\item si tenemos un comportamiento $\hat \omega$ válido que respeta la estrategia
		      $\pi_\diam$, y a partir de los prefijos de este $\hat \omega$ es que estamos
		      definiendo las elecciones de prefijos y acciones de $\sigma_\diam$, podemos
		      también usar este $\hat \omega$ para las decisiones desde los estados
		      $v_{V_k}$, eligiendo como próximo vértice $s_{k+1}$ al que se eligió
		      probabilísticamente en $\hat \omega$.
	\end{enumerate}
\end{boxgris}

Con estas decisiones ya tomadas resulta la construcción de la $\sigma_i$ que
queríamos, a la cual llamaremos $\derand(\pi_i)$.

\textbf{Respetar una estrategia}

Al hablar de caminos o comportamientos específicos es natural pensar que estos
fueron producto de una partida en la que los jugadores siguieron determinadas
estrategias. Para la prueba que plantearemos a continuación querremos
formalizar esta relación entre caminos/comportamientos y las estrategias que se
siguieron en ellos y lo haremos a través de la idea de que un
comportamiento/camino ``respeta" determinadas estrategias.

Sea $\omega = (s_0, \alpha_0, s_1, \alpha_1, \dots)$ un comportamiento en un
juego estocástico poliópico y sean $\picuad$ y $\pidiam$ dos estrategias en el
mismo juego. Decimos que $\omega$ respeta las estrategias $\picuad$ y $\pidiam$
si $\forall i \geq 0$ vale
\begin{align*}
	&\bigl( s_i \in \St_\cuad \wedge \picuad(s_0, \alpha_0, \dots, s_i)(\alpha_i) > 0 \wedge s_{i+1} \in \supp(\alpha_i) \bigr) \bigvee \\
	&\left(s_i \in \St_\diam \wedge \pidiam(s_0, \alpha_0, \dots, s_i)(\alpha_i) > 0 \wedge
	s_{i+1} \in \supp(\alpha_i) \right)
\end{align*}
% \begin{align*}
% 	&\bigl( s_i \in \St_\cuad \wedge \picuad(\hat
% 	\omega_i)(\alpha_i) > 0 \wedge s_{i+1} \in \supp(\alpha_i) \bigr) \bigvee \\
% 	&\left(s_i \in \St_\diam \wedge \pidiam(\hat \omega_i)(\alpha_i) > 0 \wedge
% 	s_{i+1} \in \supp(\alpha_i) \right)
% \end{align*}

Sea $\rho = (s_0, v_{V_0}, s_1, v_{V_1}, \dots)$ un camino en la
desrandomización de un juego estocástico politópico y sean $\sigma_\cuad$ y
$\sigma_\diam$ dos estrategias en el mismo juego. Decimos que $\rho$ respeta
las estrategias $\sigma_\cuad$ y $\sigma_\diam$ si $\forall i \geq 0$ vale

\begin{align*}
	\sigma_\diam&(s_0, v_{V_0}, \dots, s_i, v_{V_i})=s_{i+1} \bigwedge \\
	&\bigl( \left( s_i \in \St_\cuad \wedge \sigma_\cuad(s_0, v_{V_0}, \dots, s_i)=v_{V_i} \right) \bigvee \\
	&\left( s_i \in \St_\diam \wedge \sigma_\diam(s_0, v_{V_0}, \dots, s_i)=v_{V_i} \right) \bigr)
\end{align*}
%$ \sigma_\diam(\hat \rho_{v_{V_i}})=s_{i+1} \bigwedge \left( \left( s_i \in \St_\cuad \wedge \sigma_\cuad(\hat \rho_{s_i})=v_{V_i} \right) \bigvee \left( s_i \in \St_\diam \wedge \sigma_\diam(\hat \rho_{s_i})=v_{V_i} \right) \right)$

\subsection{Prueba de igualdad sobre los conjuntos ganadores}

\begin{theorem}
	\label{teocuali}
	Sea $\Gk = (\St, (\St_\cuad, \St_\diam), \Act, \theta)$ la interpretación de un juego estocástico politópico, $\widetilde{\mathcal{R}} = \{ \langle G_1, R_1 \rangle, ..., \langle G_k, R_k \rangle \}$ una condición de Rabin sobre $\St$, con su especificación LTL $\varphi$,

	$$
		\varphi := \bigvee_{j \in [1, k]} \left( \eventsiemp \overline{R_j} \wedge \eventsiemp G_j \right)
	$$

	y sea $\DGk$ su desrandomización.

	Sea $\W \subseteq \St$ el conjunto de todos los estados desde los cuales el
	jugador $\cuad$ gana en $\DGk$ y sea $\W^{as}$ el conjunto de vértices desde
	los cuales el jugador $\cuad$ gana casi seguramente con estrategias de memoria
	finita en $\Gk$. Entonces, $\W = \W^{as}$.

	Es más, a partir de una estrategia ganadora en $\DGk$ se puede construir
	fácilmente una estrategia ganadora en $\Gk$, y viceversa.
\end{theorem}

\begin{proof}
	Probaremos la doble contención:

	\textbf{Primera contención: } $\W \subseteq \W^{as}$

	Sea $s \in \W$. Entonces, sabemos que existe al menos una estrategia del
	jugador $\cuad$ ganadora desde $s$ en $\DGk$. Llamemos a esta
	$\sigma_\cuad^{*}$.

	Queremos ver que $s \in \W^{as}$, lo que requiere ver que existe una estrategia
	$\picuadGk$ tal que:
	\begin{equation}
		\label{pisirve}
		\inf_{\pidiam \in \Pidiam} \Prob_{\Gk,s}^{\picuadGk, \pidiam}(\varphi) = 1
	\end{equation}

	Proponemos $\picuadGk = \rand(\sigma_\cuad^*)$ y veremos que vale \ref{pisirve}
	por reducción al absurdo.

	Supongamos que no vale \ref{pisirve}, es decir,
	\begin{equation}
		\inf_{\pidiam \in \Pidiam} \Prob_{\Gk, s}^{\picuadGk, \pidiam} (\varphi) < 1.
	\end{equation}

	Esto significa que existe una estrategia $\pidiam^*$ que hace que $\Prob_{\Gk,
			s}^{\picuadGk, \pidiam} (\varphi) < 1$. A su vez, esto significa que existe un
	comportamiento $\omega^*$ que empieza desde $s$, respeta las estrategias
	$\picuadGk$ y $\pidiam^*$, pero no cumple $\varphi$.

	Pensemos, entonces, en $\sigma_\diam^*$ una desrandomización válida de
	$\pidiam^*$ que sigue las elecciones de acciones que toman los prefijos de
	$\omega^*$.

	Luego, $\rho^* = \derand(\omega^*)$ respeta las estrategias $\sigma_\cuad^*$ y
	$\sigma_\diam^*$. Como $\rho^* \cap \St = \omega^* \cap \St$ y $\omega^*$ no
	cumple $\varphi$, $\rho^*$ no cumple $\varphi$. Es decir, existe un camino que
	empieza desde $s$, respeta $\sigma_\cuad^*$, pero no es ganador para $\varphi$.
	Esto contradice que la estrategia $\sigma_\cuad^*$ sea ganadora desde $s$.

	Llegamos a esta contradicción por suponer que no vale \ref{pisirve}. Entonces
	la ecuación sí vale y, consecuentemente, tenemos que $s \in \W^{as}$, como
	queríamos probar.
	% -------------------------------------------------------------------------------------------------------------------------------------------------

	\textbf{Segunda contención: } $\W^{as} \subseteq \W$

	Sea $s \in \W^{as}$, veamos que $s \in \W$.

	Como $s \in \W^{as}$, entonces existe $\picuad^*$ en $\Gk$ tal que
	$\inf_{\pidiam \in \Pidiam}\Prob_{\Gk,s}^{\picuad^*, \pidiam}(\varphi) = 1$.

	Lo que queremos ver para probar que $s \in \W$ es que existe una estrategia
	$\sigma_\cuad^*$ tal que $\cuad$ gana con ella desde $s$ en $\DGk$.

	Definimos $\sigma_\cuad^* = \derand(\picuad^*)$

	Entonces ahora queremos ver que $\sigma_\cuad^*$ es ganadora en $\DGk$ desde
	$s$.

	Una manera de ver esto es probar que para un comportamiento cualquiera $\rho$
	generado por $\sigma_\cuad^*$ y una estrategia válida $\pidiam \in \Pidiam$
	arbitraria, $\inft(\rho)$ cumple con la especificación $\varphi$. Es decir, que
	vale la siguiente fórmula:

	%(Ver que quede la definición de Inf antes).

	\begin{equation}
		\label{varphi}
		\varphi' := \bigvee_{j = 1}^{k} \left((\inft(\rho) \cap R_j = \emptyset) \wedge (\inft(\rho) \cap G_j \neq \emptyset)  \right)
	\end{equation}

	Si podemos probar que $\inft(\rho)$ es un sub-PMDP alcanzable en el PMDP que se
	forma al fijar la estrategia de memoria finita $\picuad^*$ en $\Gk$, por el
	teorema \ref{adaptB30} podemos ver que vale la ecuación \ref{varphi}.

	Primero, podemos ver que $\inft(\rho)$ \hl{(entendiendo los vértices de la
		forma $v_V$ como el conjunto resultado $V$)} es una componente final en el
	polytopal markov decision process que se forma al fijar la estrategia
	$\picuad^*$ en $\Gk$, llamémoslo $(C,D)$, viendo que cumple las dos
	propiedades:

	\begin{itemize}
		\item Para cada $V^i$ que aparece como subíndice de vértices \textit{especiales} en
		      $\inft(\rho)$, vale que $V^i \subseteq C$. En el caso de que existiese algún
		      $s_x \in V^i$ tal que $s_x \notin C$, se estaría contradiciendo que $\pidiam$
		      sea una estrategia válida en $\DGk$, puesto que visitaría infinitas veces
		      $v_{V^i}$ y solo finitas veces $s_x$, uno de sus sucesores.
		\item El grafo dirigido inducido por $\inft(\rho)$ es fuertemente conexo. Si fuese de
		      otra manera habría dos vértices $u, v \in \inft(\rho)$ tales que $v$ no sería
		      alcanzable desde $u$, contradiciendo así que $u$ y $v$ son visitados infinitas
		      veces por $\sigma$.
	\end{itemize}

	Luego, podemos ver que $\inft(\rho)$ es alcanzable en $\Gk$ viendo que existe
	una estrategia $\pidiam^*$ en $\Gk$ que lo posibilita.

	Definamos $\pidiam^* = \rand(\sigma_\diam)$.

	Esta estrategia permite llegar con probabilidad positiva a $\inft(\rho)$,
	puesto que tanto $\pidiam^*$ como $\picuad^*$ le da probabilidad positiva a los
	mismos vértices que $\pidiam$ asegura visitar infinitamente. \hl{Capaz esto se
		podría explicar mejor}.

	Con lo cual hemos probado que vale \ref{varphi} y, por lo tanto, que
	$\sigma_\cuad*$ es ganadora en $\DGk$ desde $s$, con lo que hemos probado que
	$s \in \W$.

\end{proof}

Haber podido probar esta igualdad nos permite calcular el conjunto de estados
casi seguramente ganadores para $\cuad$ en $\Gk$ mediante el algoritmo que se
propone en \cite{Banerjee} para calcular el conjunto de vértices ganadores en
un juego de adversario justo $G$ con un objetivo de Rabin $R$. Este algoritmo
tiene una complejidad de $O(n^2 k!)$ donde $n$ es la cantidad de vértices en
$G$ y $k$ la cantidad de pares en $R$.

Esto quiere decir que podemos calcular el conjunto de estados casi seguramente
ganadores para $\cuad$ en un PSG $\K$ con un objetivo de Rabin $R$ con una
complejidad de $O((n l)^2 k! )$ donde $n$ es la cantidad de estados en $\K$,
$k$ es la cantidad de pares en $R$ y $l = \max \{\abs{V_s} \mid s \in \St \}$
es la cantidad máxima de conjuntos soporte para las acciones que puede haber
desde un estado $s$ en $\K$.

\hl{¿Agregar algo sobre cómo es el algoritmo? ¿Sobre el teorema en sí? ¿Dejar el segundo párrafo como teorema? ¿Qué vamos a hablar de complejidad en PSG Rabin?}