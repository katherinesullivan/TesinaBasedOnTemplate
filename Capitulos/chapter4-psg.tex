\chapter{Juegos Estocásticos Politópicos - PSGs}
~\label{cap:psg}
\vspace{-1cm}

El propósito de este capítulo es presentar las definiciones y los hallazgos que
nos son más relevantes del paper \textit{Polytopal Stochastic Games}
\cite{Polytopal} haciéndoles ciertos cambios que fueron necesarios para los
resultados que mostraremos más adelante. Este trabajo es el que sirvió de
inspiración para esta tesina y toda la producción aquí es una extensión del
mismo.

El paper, publicado en 2025, es el primero en presentar el concepto de
\textit{juego estocástico politópico}, respondiendo a la necesidad de juegos
estocásticos que puedan capturar mayor incertidumbre sobre las distribuciones
de probabilidad que determinan las acciones, haciendo esto a través de
inecuaciones lineales cuyas soluciones forman un politopo.

% respondiendo a la necesidad de juegos
% estocásticos que puedan capturar mayor incertidumbre sobre las distribuciones
% de probabilidad, haciendo esto a través de inecuaciones lineales cuyas
% soluciones forman un politopo.

%Discutiremos más sobre la necesidad de creación de este tipo de juegos y qué ventajas proveen y qué motiva su estudio ?) y dividiremos ...
Dividiremos el capítulo en dos secciones: definiciones y resultados.

\section{Definiciones}

Siendo $S$ un conjunto, en lo siguiente, $\mathscr{P}(S)$ representará el
conjunto partes de $S$ y $\mathscr{P}_f(S)$ denotará el conjunto de los
subconjuntos finitos de $S$.

\subsection*{Politopos}

Un \textbf{politopo convexo} en $\mathbb{R}^n$ es un conjunto acotado $K = \{ x
	\in \mathbb{R}^n | Ax \leq b\}$ con $A \in \mathbb{R}^{m\times n}$ y $ b \in
	\mathbb{R}^m$, para algún $m \in \mathbb{N}$.

Por acotado nos referimos a que $\exists \ M \in \mathbb{R}_{\geq 0}$ tal que
$\sum_{i=1}^{n} |x_i| \leq M \ \forall x \in K$, y siendo $S$ un conjunto
finito, como las funciones en $\mathbb{R}^S$ pueden ser vistas como vectores en
$\mathbb{R}^{|S|}$, generalmente, nos referiremos a politopos en
$\mathbb{R}^S$.

Sea \( \textbf{Poly}(S) \) el conjunto de todos los politopos convexos en \(
\mathbb{R}^S \). Nótese que el conjunto de todas las funciones de probabilidad
en \( S \) forma el politopo convexo

\( \text{Dist}(S) = \{ \mu \in \mathbb{R}^S \mid \sum_{s \in S} \mu(s) = 1 \text{ y } \forall s \in S : \mu(s) \geq 0 \} \)

Definimos \( \textbf{DPoly}(S) = \{ K \cap \text{Dist}(S) \mid K \in
\text{Poly}(S) \} \). Cada \( K \in \text{DPoly}(S) \) es un politopo convexo
cuyos elementos son también funciones de probabilidad sobre \( S \), y su
conjunto de desigualdades característico \( Ax \leq b \) ya codifica las
desigualdades \( \sum_{s \in S} x_s = 1 \) y \( x_s \geq 0 \) para todo \( s
\in S \).

Cualquier politopo convexo \( K \in \text{Poly}(S) \) puede caracterizarse
alternativamente como la envolvente convexa de su conjunto finito de vértices.
Sea \( \mathbb{V}(K) \) el conjunto de todos los vértices del politopo \( K \).
Si \( \mathbb{V}(K) = \{v^1, \dots, v^k\} \), entonces todo \( x \in K \) es
una combinación convexa de \( \{v^1, \dots, v^k\} \), es decir, \( x =
\sum_{i=1}^{k} \lambda_i v_i \) con \( \lambda_i \geq 0 \), para \( i \in
[1..k] \), y \( \sum_{i=1}^{k} \lambda_i = 1 \).

% \subsection{Juegos estocásticos}

% \hl{Esto mepa que no va pq ya está en el capítulo 3}

% Un juego estocástico es una tupla \( \G = (\St, (\St_\square,
% \St_\diamondsuit), \Act, \theta) \), donde \( \St \) es un conjunto finito de
% estados, con \( \St_\square, \St_\diamondsuit \subseteq \St \) siendo una
% partición de \( \St \), y \( \theta : \St \times \Act \times \St \to [0, 1] \)
% es una función de transición probabilística tal que para todo \( s \in \St \) y
% \( a \in \Act \), \( \theta(s, a, \cdot) \in \text{Dist}(\St) \) o \( \theta(s,
% a, \St) = 0 \) (dados un estado $s$ y una acción $a$, $\theta$ define una
% distribución en $s$ o no hay estados alcanzables por $a$ desde $s$).

% Sea \( \Act(s) = \{ a \in \Act \mid \theta(s, a, \St) = 1 \} \) el conjunto de
% acciones habilitadas en el estado \( s \). Si \( \St_\square = \emptyset \) o
% \( \St_\diamondsuit = \emptyset \), entonces \( \G \) es un proceso de decisión
% de Markov (o MDP). Si, además, \( |\Act(s)| = 1 \) para todo \( s \in \St \),
% \( \G \) es una cadena de Markov (o MC).

% Un camino en el juego \( \G \) es una secuencia infinita de estados \( \rho =
% s_0 s_1 \dots \) tal que \( \theta(s_k, a, s_{k+1}) > 0 \) para alguna \( a \in
% \Act \) y para todo \( k \in \mathbb{N} \). Para \( i \geq 0 \), \( \rho_i \)
% indica el \( i \)-ésimo estado en el camino \( \rho \) (nótese que \( \rho_0 \)
% es el primer estado en \( \rho \)). Denotamos por \( \text{Paths}_\G \) el
% conjunto de todos los caminos, y por \( \text{FPaths}_\G \) el conjunto de los
% prefijos finitos de caminos. De manera similar, \( \text{Paths}_{\G,s} \) y \(
% \text{FPaths}_{\G,s} \) denotan el conjunto de caminos y el conjunto de caminos
% finitos que comienzan en el estado \( s \).

% Una estrategia para el jugador \( i \) (para \( i \in \{\square, \diamondsuit\}
% \)) en un juego \( G \) es una función \( \pi_i : \St^\ast \St_i \to
% \text{Dist}(\Act) \) que asigna una distribución probabilística a cada
% secuencia finita de estados, tal que \( \pi_i(\hat{\rho}s)(a) > 0 \) solo si \(
% a \in \Act(s) \). El conjunto de todas las estrategias para el jugador \( i \)
% se denomina \( \Pi_i \). Cuando sea conveniente, indicamos que el conjunto de
% estrategias \( \Pi_i \) pertenece al juego \( \G \) escribiendo \( \Pi_{\G,i}
% \).

% Una estrategia \( \pi_i \) se dice pura o determinista si, para todo \(
% \hat{\rho}s \in \St^\ast \St_i \), \( \pi_i(\hat{\rho}s) \) es una distribución
% de Dirac (es decir, una distribución \( \delta_a \) tal que \( \delta_a(a) = 1
% \) y \( \delta_a(b) = 0 \) para todo \( b \neq a \)), y se llama sin memoria si
% \( \pi_i(\hat{\rho}s) = \pi_i(s) \), para todo \( \hat{\rho} \in \St^\ast \).
% Sea \( \Pi^M_i \) el conjunto de todas las estrategias sin memoria para el
% jugador \( i \), y \( \Pi^{MD}_i \) el conjunto de todas sus estrategias
% deterministas y sin memoria.

% Dadas las estrategias \( \pi_\square \in \Pi_\square \) y \( \pi_\diamondsuit
% \in \Pi_\diamondsuit \), y un estado inicial \( s \), el resultado del juego es
% una cadena de Markov denotada \( \G^{\pi_\square, \pi_\diamondsuit}_s \).

% La cadena de Markov \( \G^{\pi_\square, \pi_\diamondsuit}_s \) define
% una\textbf{ medida de probabilidad} \( \Prob^{\pi_\square,
% 	\pi_\diamondsuit}_{\G,s} \) sobre la \( \sigma \)-álgebra de Borel generada por
% los cilindros de \( \text{Paths}_{\G,s} \). Si \( \xi \) es un conjunto medible
% en dicha \( \sigma \)-álgebra de Borel, \( \Prob^{\pi_\square,
% 	\pi_\diamondsuit}_{\G,s}(\xi) \) es la probabilidad de que las estrategias \(
% \pi_\square \) y \( \pi_\diamondsuit \) sigan un camino en \( \xi \) partiendo
% del estado \( s \).

% Usamos la notación LTL para representar conjuntos específicos de caminos, en
% particular: \( D U^n C = \{ \rho \in \St^\omega \mid \rho_n \in C \text{ y }
% \forall j < n : \rho_j \in D \} = D^n \times C \times \St^\omega \) es el
% conjunto de caminos que alcanzan \( C \subseteq \St \) en exactamente \( n \geq
% 0 \) pasos, recorriendo antes solo estados en \( D \subseteq \St \); \(
% \diamondsuit^n C = \St U^n C \) es el conjunto de todos los caminos que
% alcanzan los estados en \( C \) en exactamente \( n \) pasos; \( \diamondsuit C
% = \bigcup_{n \geq 0} (\St \setminus C) U^n C \) es el conjunto de todos los
% caminos que alcanzan un estado en \( C \).

\subsection*{PSGs}

% Los juegos estocásticos politópicos fueron presentados en 2024 por Castro y
% D'Argenio \cite{Polytopal}. La idea de su creción ...

Un juego estocástico politópico se caracteriza a través de una estructura que
contiene un conjunto finito de estados divididos en dos conjuntos, cada uno
perteneciente a un jugador diferente. Además, a cada estado se le asigna un
conjunto finito de politopos convexos de distribuciones de probabilidad sobre
los estados. La definición formal es como sigue:

\begin{definition}[PSG]
	Un juego estocástico politópico (abreviado PSG, por sus siglas en inglés) es una estructura \( \K = (\St, (\St_\square, \St_\diamondsuit), \Theta) \) tal que \( \St \) es un conjunto finito de estados particionado en \( \St = \St_\square \biguplus \St_\diamondsuit \) y \( \Theta : \St \to \mathscr{P}_f(\textsf{DPoly}(\St)) \).
\end{definition}

Un juego estocástico politópico se diferencia de un juego estocástico
tradicional por el hecho de que desde un estado \( s \in \St_i \) (para \( i
\in \{ \square, \diamondsuit \} \)), el jugador \( i \) elige jugar un politopo
\( K \in \Theta(s) \) y una distribución \( \mu \in K \), en vez de elegir
directamente una acción de entre un conjunto finito. Al igual que en un juego
estocástico, el siguiente estado \( s' \) se selecciona de acuerdo con la
distribución \( \mu \) correspondiente a la acción seleccionada, y el juego
continuará desde \( s' \) repitiendo el mismo proceso.

Viendo esto, es natural pensar que desarrollo de un juego estocástico
politópico se puede interpretar en términos de un juego estocástico donde el
número de transiciones salientes desde los estados de los jugadores puede ser
no numerable. Formalmente, la interpretación de un PSG es la siguiente:

\begin{definition}[Interpretación de un PSG]
	La interpretación del juego estocástico politópico \( \K \) se define por el juego estocástico \( \Gk = (\St, (\St_\square, \St_\diamondsuit), \Act, \theta) \), donde \( \Act = \bigcup_{s \in \St} \Theta(s) \times \textsf{Dist}(\St) \) y
	\[
		\theta(s, (K, \mu), s') =
		\begin{cases}
			\mu(s') & \text{si } K \in \Theta(s) \text{ y } \mu \in K \\
			0 & \text{en otro caso}.
		\end{cases}
	\]
\end{definition}

Nótese que el conjunto de acciones \( \Act \) puede ser no numerable, al igual
que cada conjunto \( \Act(s) = \bigcup_{K \in \Theta(s)} \{K\} \times K \) de
todas las acciones realizables en el estado s, identificado por el politopo
elegido y la distribución seleccionada dentro del politopo. Por lo tanto,
necesitamos extender las estrategias a este dominio no numerable, que debe
estar dotado de una \( \sigma \)-álgebra adecuada.

Para esto, utilizamos una construcción estándar para darle a \(
\textsf{Dist}(S) \) una \( \sigma \)-álgebra: \( \Sigma_{\textsf{Dist}(S)} \)
se define como la \( \sigma \)-álgebra más pequeña que contiene los conjuntos
\( \{\mu \in \textsf{Dist}(S) \mid \mu(S) \geq p\} \) para todo \( S \subseteq
S \) y \( p \in [0, 1] \). Ahora, dotamos a \( \Act \) con la \( \sigma
\)-álgebra producto \( \Sigma_\Act = \mathscr{P} \left(\bigcup_{s \in S}
\Theta(s)\right) \otimes \Sigma_{\text{Dist}(S)} \), y llamamos \(
\textsf{PMeas}(A) \) al conjunto de todas las medidas de probabilidad sobre \(
\Sigma_\Act \). No es difícil comprobar que cada conjunto de acciones
habilitadas \( \Act(s) \) es medible (es decir, \( \Act(s) \in \Sigma_\Act \))
y que la función \( \theta(s, \cdot, s') \) es medible (es decir, \( \{a \in
\Act \mid \theta(s, a, s') \leq p\} \in \Sigma_\Act \) para todo \( p \in [0,
	1] \)).

Ahora bien, para dar la medida de probabilidad $\Prob^{\picuad, \pidiam}$
debemos primero presentar las nociones de camino y estrategia en un PSG.

Como lo hicimos para MDPs y juegos estocásticos a la definición de camino la
daremos como una secuencia de estados y acciones. Esto difiere de la
formulación original hecha para PSGs en \cite{Polytopal}, pero cambiaremos
acordemente las definiciones que se asocian a ello para mantener la formalidad
de los resultados.

\begin{definition}[Camino en un PSG]
	Un camino en un PSG es una secuencia infinita que alterna entre estados y conjuntos resultado. Formalmente un camino es un $\omega = (s_0, (K_0, \mu_0), s_1, (K_1, \mu_1), \dots)$ tal que $s_i \in \St$, $(K_i, \mu_i) \in \Act(s_i)$ y $\mu_i(s_{i+1}) > 0$ para todo $i \geq 0$.

	Al igual que para juegos estocásticos, notaremos al conjunto de todos los
	caminos de un PSG como $\Paths$. Dado un estado $s$, indicaremos con $\Paths_s$
	el conjunto de los caminos que se originan en $s$, y con $\Paths_s^n$ al
	conjunto de los caminos que se originan en $s$ y tiene un total de $n$ estados.
\end{definition}

\begin{definition}[Conjuntos soporte y acciones]
	Dada una acción $(K, \mu)$ definiremos su conjunto soporte, $\supp((K, \mu))$ como el conjunto formado por los estados a los cuales $(K, \mu)$ les asigna una probabilidad positiva. Es decir,
	$$supp((K, \mu)) = \{s \in \St \mid \mu(s) > 0\}$$.
	Dado un estado $s$ y un conjunto de estados $V$, definiremos como $acc(s, V)$ a
	las acciones que parten desde $s$ y tienen como conjunto soporte $V$. Es decir,
	$$acc(s, V) = \{\alpha \in \Act(s) \mid supp(\alpha) = V\} = \{(K, \mu) \in \Act(s) \mid \mu(V) = 1 \wedge \forall v \in V, \ \mu(v) > 0\}$$.
	Por último, dado un estado $s$, definiremos como $V_s$ al conjunto de todos los
	soportes que pueden tener las distintas acciones desde $s$. Es decir,
	\begin{align*}~\label{definicionVs}
		V_s &= \{supp(\mu) \mid \exists K \in \Theta(s): \mu \in K\} = \\
		&= \{V' \subseteq \St | \exists \mu \in K, K \in \Theta(s) \text{ tal que } \mu(V') = 1 \text{ y } \forall s' \in V', \mu(s') > 0\}.
	\end{align*}
\end{definition}

Entonces, ahora sí podemos extender el concepto de estrategia en un PSG:

\begin{definition}[Estrategia en un PSG]
	Una estrategia $\pi_i$ para el jugador $i$ ($i \in \{\cuad, \diam\}$) en un PSG será una función $\pi_i : (\St \times \Act)^* \St_i \rightarrow \textsf{PMeas}(\Act)$ que asigna una medida de probabilidad a cada $\omega s$ tal que $\pi_i(\omega s)(\Act(s)) = 1$.
\end{definition}

Los distintos tipos de estrategias que presentamos en el
capítulo~\ref{cap:modelos} se extienden naturalmente a este tipo de
estrategias, y los conjuntos de estrategias de ese tipo se denotarán de la
misma manera en la que se presentaron con juegos estocásticos. A estos tipos de
estrategias agregamos una particular de los juegos estocásticos politópicos:
las estrategias extremas.

\begin{definition}[Estrategia extrema en un PSG]
	Una estrategia $\pi_i$ para el jugador $i$ ($i \in \{\cuad, \diam\}$) en un PSG se dirá \textbf{extrema} si para todo $\omega \in \Paths$, $\pi_i(\omega s)\{(K, \mu) \in \Act(s)| \mu \in \mathbb(V)(K)\} = 1$. Es decir, diremos que una estrategia es extrema si solo asigna probabilidades positivas a distribuciones en los vértices del politopo.

	Notaremos con $\Pi_i^X$ a la familia de estrategias extremas para el jugador
	$i$ y podremos combinar esta notación con las presentadas para otras familias
	de estrategias, por ejemplo, notando a la familia de estrategias extremas,
	puras y sin memoria del jugador $i$ como $\Pi_i^{XMD}$.
\end{definition}

% En este caso también tendremos dos clasificaciones distintas que se pueden dar
% de las estrategias. En primer lugar, se puede ver si una estrategia es
% determinista o randomizada:

% \kathy{Tengo que agregar definiciones}

% Y, en segundo lugar, se puede ver si una estrategia es sin memoria, con memoria
% finita.
%Hemos visto el concepto de las estrategias sin memoria y con memoria finita y la extensión de las mimsas a este contexto se da de manera natural

\begin{boxamarillo}[Medida de probabilidad que no va a ir probablemente]{}
	Con la formalización del concepto de estrategia ahora podemos presentar
	formalmente la definición de $\ProbG$, la medida de probabilidad definida por
	la cadena de Markov dada por $\Gk$ y las estrategias $\picuad$ y $\pidiam$.

	Para eso, primero para cada $n \geq 0$ y $s \in \St$ definimos $\Prob^{\picuad,
			\pidiam, n}_{\Gk, s} : \Paths^{n+1} \rightarrow [0, 1]$ para todo $s' \in \St$
	y $\hat{\omega} \in \Paths_s^{n+1}$ inductivamente como sigue:

	\begin{align*}
		&\Prob^{\picuad, \pidiam, 0}_{\Gk, s}(s') = \delta_s(s') \\
		&\Prob^{\picuad, \pidiam, n+1}_{\Gk, s}(\hat{\omega}s_nV's') = \Prob^{\picuad, \pidiam, n}_{\Gk, s}(\hat{\omega} s_n) \int_{\{ a \in A(s_n) \mid \supp(a) = V'\}} \theta(s_n, a, s') \, d(\pi_i(\hat{\omega}s_n)(a)) \\ & \qquad \qquad \qquad \qquad \qquad \text{si } s_n \in \St_i \text{ con } i \in \{\square, \diamondsuit\}
	\end{align*}

	(supp(a) = V' significa que el soporte de a es V')

	(capaz restringirse sobre las acciones con soporte V'? No sé bien cómo va a quedar esto)

	y extendemos \( \Prob^{\pi_\square, \pi_\diamondsuit, n}_{\Gk, s} :
	\mathcal{P}(\Paths^{n+1}) \to [0, 1] \) a conjuntos como la suma de la medida
	sobre los elementos del conjunto.

	Sea \( \Sigma_\Paths \) la \( \sigma \)-álgebra discreta sobre \( \Paths \)
	(pues tanto $\St$ como los conjuntos soportes serán conjuntos finitos) y \(
	\Sigma_{\Paths^\omega} \) la \( \sigma \)-álgebra producto usual sobre \(
	\Paths^\omega \). Por el teorema de extensión de Carathéodory, \(
	\Prob^{\picuad, \pidiam}_{\Gk, s} : \Sigma_{\Paths^\omega} \to [0, 1] \) se
	define como la única medida de probabilidad tal que para todo \( n \geq 0 \), y
	\( SV_i \in \Sigma_\Paths \), \( 0 \leq i \leq n \),

	\[
		\Prob^{\picuad, \pidiam}_{\Gk, s}(SV_0 \times \dots \times SV_n \times \Paths^\omega) = \Prob^{\picuad, \pidiam, n}_{\Gk, s}(SV_0 \times \dots \times SV_n).
	\]
\end{boxamarillo}

Así se logra interpretar un PSG como un SG pero con un conjunto infinito de
acciones. \kathy{Pedro prometió hacer esto que está en el box amarillo bien :)}
En \cite{Polytopal} se demuestra cómo ciertos preguntas que nos podemos hacer
sobre esta interpretación infinita se pueden responder analizando en la
interpretación con una cantidad finita de acciones que presentamos a
continuación.

\begin{definition}[Interpretación extrema de un PSG]
	Dada $\Gk = (\St, (\St_\cuad, \St_\diam), \Act, \theta)$ la interepretación de un PSG $\K$, la interpretación extrema de $\K$ es el juego estocástico $\Hk = (\St, (\St_\cuad, \St_\diam), \V(\Act), \theta_{\Hk})$ donde $\theta_{\Hk}$ es la restricción de  $\theta$ a las acciones en $\V(\Act) = \{(K, \mu) \in \Act \mid \mu \in \V(K)\}$. Es decir, para todos $s, s' \in \St$ y $a \in \V(\Act)$, $\theta_{\Hk}(s, a, s') = \theta(s, a, s')$.

	Como $\V(\Act)$ es finito, $\Hk$ es un juego estocástico finito.
\end{definition}

\textbf{Roborta vs Rigoborto en un terreno no tan estudiado}

Existen muchas razones por las cuales podríamos no saber las probabilidades
exactas de cómo es que podrían fallar nuestras indicaciones a

\section{Teoremas}

En esta sección presentamos los resultados principales desarrollados en
\cite{Polytopal}.

Si bien el paper aborda 4 tipos distintos de objetivos (a saber, de
alcanzabilidad, de recompensa media, de recompensa total acumulada y de
recompensa total descontada), solo nos concentraremos en los resultados que el
paper presenta para objetivos de alcanzabilidad.

El primer teorema, establece la determinación de los juegos estocásticos a la
vez de que establece que los objetivos de alcanzabilidad en PSGs puede ser
resueltos de manera equivalente en la interpretación extrema del PSG.

\begin{theorem}[Reducción de PSGs]
	Sean $\Gk$ y $\Hk$ la interpretación y la interpretación extrema, respectivamente, de un juego estocástico politópico $\K$. Sea $C$ un subconjunto de estados en $\K$. Entonces vale que:
	\begin{align*}
		&\inf_{\pidiam \in \Pidiam} \sup_{\picuad \in \Picuad} \ProbG(\alc C) = \\
		&\inf_{\pidiam \in \Pidiam^{MD}} \sup_{\picuad \in \Picuad^{MD}} \ProbH(\alc C) = \\
		&\sup_{\picuad \in \Picuad^{MD}} \inf_{\pidiam \in \Pidiam^{MD}} \ProbH(\alc C) = \\
		&\sup_{\picuad \in \Picuad} \inf_{\pidiam \in \Pidiam} \ProbG(\alc C)
	\end{align*}
\end{theorem}

Dado que las interpretaciones extremas son finitas, los valores se pueden
calcular siguiendo algoritmos conocidos \cite{CONDON1992,filar}. Por lo tanto,
este teorema también proporciona inmediatamente una solución algorítmica para
los PSGs.

El segundo resultado, se deriva del primero y presenta la complejidad del
estudio cuantitativo de los juegos estocásticos politópicos. Enunciado sobre lo
que nos interesa y notando como $\val_{\Gk,s}(\alc G)$ al valor del estado $s$
(es decir, la probabilidad máxima de que $\cuad$ gane desde $s$) en el juego
$\Gk$ con un objetivo de alcanzabilidad sobre un conjunto $G$, el teorema
sería:

\begin{theorem}[Complejidad en un PSG]
	Para todo PSG $\K$, $q \in \mathbb{Q}$, $G \subseteq \St$ y $s \in \St$, el problema de decidir si $\val_{\Gk, s}(\alc G) \geq q$ está en $NP \cap coNP$.
\end{theorem}

Con estos resultados como contexto surge el interés de estudiar los juegos
estocásticos politópicos con otros objetivos. Por lo expuesto en el capítulo
anterior, decidimos que sería una buena idea estudiar los objetivos de Rabin.
En el próximo capítulo nos explayamos sobre los hallazgos que pudimos hacer
durante el desarrollo de esta tesina.

% \hl{¿Agregar algo en prelim o apéndice de clases de complejidad?}

% El número de vértices de un politopo crece exponencialmente en la dimensión del
% politopo [16]. Más precisamente, si \( d \) es la dimensión de un politopo \( K
% \) y \( m \) es el número de desigualdades que lo definen, entonces \( V(K)
% \sim \Paths(m^{\lfloor d/2 \rfloor}) \). Esto implica que la interpretación
% extrema \( \gameH_\gameK \) crece exponencialmente con el tamaño más grande de
% los conjuntos de soporte de las distribuciones involucradas en el PSG original
% \( \gameK \), lo cual esperamos que no sea demasiado grande. (En nuestro
% ejemplo de la Sección 2, \( \lfloor d/2 \rfloor = 2 \)). \duda{¿Qué son los
% 	conjuntos soporte?}

% Condon [9] demostró que decidir la alcanzabilidad en los juegos estocásticos
% está en \( \textbf{NP} \cap \textbf{coNP} \). A pesar del crecimiento
% exponencial, este sigue siendo nuestro caso, como mostramos a continuación. Sea
% \( \text{Val}_s(\gameK) \) el valor del juego en el estado \( s \), es decir,
% es igual a
% \[
% 	\sup_{\pi_\square \in \Pi_\square} \inf_{\pi_\diamondsuit \in \Pi_\diamondsuit} P^{\pi_\square, \pi_\diamondsuit}_{\gameG_\gameK,s}(\diamondsuit G), \quad \text{o} \quad \sup_{\pi_\square \in \Pi_\square} \inf_{\pi_\diamondsuit \in \Pi_\diamondsuit} \mathbb{E}^{\pi_\square, \pi_\diamondsuit}_{\gameG_\gameK,s}[\text{rew}_\textsf{f}],
% \]
% el problema es entonces decidir si \( \text{Val}_s(\gameK) \geq q \), para un
% dado \( q \in \mathbb{Q} \) y \( s \in \states \).

% Dado que en todos los casos (recompensa total, recompensa descontada,
% recompensa promedio y objetivos de alcanzabilidad bajo las respectivas
% condiciones) el valor \( \text{Val}_s(\gameK) \) del juego puede lograrse con
% estrategias determinísticas y sin memoria extremas, podemos razonar de la
% siguiente manera:
% \begin{enumerate}
% 	\item Adivinar (proponer?) una estrategia determinística y sin memoria para cada
% 	      jugador,
% 	\item En la cadena de Markov resultante, calcular la medida correspondiente (es
% 	      decir, recompensa total, recompensa descontada, recompensa promedio o
% 	      alcanzabilidad) en el conjunto respectivo de ecuaciones lineales, lo cual se
% 	      puede hacer en tiempo polinómico (para recompensa, se necesita una suma lineal
% 	      extra) [19],
% 	\item Verificar si es un punto fijo de las ecuaciones de Bellman (para recompensas
% 	      descontadas o totales), o un punto fijo del Algoritmo 5.1.1 de [12], en el caso
% 	      de recompensa promedio, \duda{¿Qué son las ecuaciones de punto fijo de
% 		      Bellman?}
% 	\item Comprobar si \( \text{Val}_s(\gameK) \geq q \).
% \end{enumerate}

% Esto coloca nuestro problema en \( \textbf{NP} \). Con el mismo proceso,
% podemos verificar si \( \text{Val}_s(\gameK) < q \), lo que también pone el
% problema en \( \textbf{coNP} \). Por lo tanto, tenemos el siguiente teorema:

% \textbf{Teorema 2.} Para cualquier PSG \( \gameK \), \( q \in \mathbb{Q} \) y \( s \in \states \), el problema de decidir si \( \text{Val}_s(\gameK) \geq q \) está en \( \textbf{NP} \cap \textbf{coNP} \). Para \( \text{rew}_\textsf{f} \in \{\text{rew}_\textsf{t}, \text{rew}_\textsf{a}\} \), el problema de decisión está restringido a \( \gameG_\gameK \) siendo casi seguramente detenedor e irreducible, respectivamente.
