\chapter{Juegos Estocásticos Politópicos - PSGs}
~\label{cap:psg}

\section{Definiciones}

\subsection{Politopos}

\subsection{PSGs}

Los juegos estocásticos politópicos fueron presentados en 2024 por Castro y
D'Argenio \cite{Polytopal}. La idea de su creción ...

Un juego estocástico politópico se caracteriza a través de una estructura que
contiene un conjunto finito de estados divididos en dos conjuntos, cada uno
perteneciente a un jugador diferente. Además, a cada estado se le asigna un
conjunto finito de politopos convexos de distribuciones de probabilidad sobre
los estados. La definición formal es como sigue:

\begin{definition}[PSG]
	Un juego estocástico politópico (abreviado PSG, por sus siglas en inglés) es una estructura \( \K = (\St, (\St_\square, \St_\diamondsuit), \Theta) \) tal que \( \St \) es un conjunto finito de estados particionado en \( \St = \St_\square \biguplus \St_\diamondsuit \) y \( \Theta : \St \to \mathcal{P}_f(\textsf{DPoly}(\St)) \).
\end{definition}

La idea de un PSG es la esperada: en un estado \( s \in \St_i \) (para \( i \in
\{ \square, \diamondsuit \} \)), el jugador \( i \) elige jugar un politopo \(
K \in \Theta(s) \) y una distribución \( \mu \in K \). El siguiente estado \(
s' \) se selecciona de acuerdo con la distribución \( \mu \), y el juego
continúa desde \( s' \) repitiendo el mismo proceso.

El desarrollo de un juego estocástico politópico se interpreta en términos de
un juego estocástico donde el número de transiciones salientes desde los
estados de los jugadores puede ser no numerable. Formalmente, la interpretación
de un PSG es la siguiente.

\begin{definition}[Interpretación de un PSG]
	La interpretación del juego estocástico politópico \( \K \) se define por el juego estocástico \( \Gk = (\St, (\St_\square, \St_\diamondsuit), \Act, \theta) \), donde \( \Act = \bigcup_{s \in \St} \Theta(s) \times \textsf{Dist}(\St) \) y
	\[
		\theta(s, (K, \mu), s') =
		\begin{cases}
			\mu(s') & \text{si } K \in \Theta(s) \text{ y } \mu \in K \\
			0 & \text{en otro caso}.
		\end{cases}
	\]
\end{definition}

Nótese que el conjunto de acciones \( \Act \) puede ser no numerable, al igual
que cada conjunto \( \Act(s) = \bigcup_{K \in \Theta(s)} \{K\} \times K \) de
todas las acciones realizables en el estado s, identificado por el politopo
elegido y la distribución seleccionada dentro del politopo. Por lo tanto,
necesitamos extender las estrategias a este dominio no numerable, que debe
estar dotado de una \( \sigma \)-álgebra adecuada.

Para esto, utilizamos una construcción estándar para darle a \(
\textsf{Dist}(S) \) una \( \sigma \)-álgebra: \( \Sigma_{\textsf{Dist}(S)} \)
se define como la \( \sigma \)-álgebra más pequeña que contiene los conjuntos
\( \{\mu \in \textsf{Dist}(S) \mid \mu(S) \geq p\} \) para todo \( S \subseteq
S \) y \( p \in [0, 1] \). Ahora, dotamos a \( \Act \) con la \( \sigma
\)-álgebra producto \( \Sigma_\Act = \mathscr{P} \left(\bigcup_{s \in S}
\Theta(s)\right) \otimes \Sigma_{\text{Dist}(S)} \), y llamamos \(
\textsf{PMeas}(A) \) al conjunto de todas las medidas de probabilidad sobre \(
\Sigma_\Act \). No es difícil comprobar que cada conjunto de acciones
habilitadas \( \Act(s) \) es medible (es decir, \( \Act(s) \in \Sigma_\Act \))
y que la función \( \theta(s, \cdot, s') \) es medible (es decir, \( \{a \in
\Act \mid \theta(s, a, s') \leq p\} \in \Sigma_\Act \) para todo \( p \in [0,
	1] \)).

Ahora bien, para definir formalmente las estrategias, introduciremos el
\hl{concepto de comportamiento en PSGs}. Esto difiere de la formulación
original hecha para PSGs en \cite{Polytopal}, pero no afecta a los resultados
que se mostrarán en este trabajo.

\begin{definition}[Comportamiento en un PSG]
	Un comportamiento en un PSG es una secuencia infinita que alterna entre estados y conjuntos resultado. Formalmente un comportamiento es un $\omega = (s_0, (K_0, \mu_0), s_1, (K_1, \mu_1), \dots)$ tal que $s_i \in \St$, $(K_i, \mu_i) \in \Act(s_i)$ y $\mu_i(s_{i+1}) > 0$ para todo $i \geq 0$.

	Notaremos al conjunto de todos los comportamientos de un PSG como $\Omega$.
	Dado un estado $s$, indicaremos con $\Omega_s$ el conjunto de los
	comportamientos que se originan en $s$, y con $\Omega_s^n$ al conjunto de los
	comportamientos que se originan en $s$ y tiene un total de $n$ estados.
\end{definition}

\begin{definition}[Conjuntos soporte y acciones]
	Dada una acción $(K, \mu)$ definiremos su conjunto soporte, $supp((K, \mu))$ como el conjunto formado por los estados a los cuales $(K, \mu)$ les asigna una probabilidad positiva. Es decir,

	$$supp((K, \mu)) = \{s \in \St \mid \mu(s) > 0\}$$

	Dado un estado $s$ y un conjunto de estados $V$, definiremos como $acc(s, V)$ a
	las acciones que parten desde $s$ y tienen como conjunto soporte $V$. Es decir,

	$$acc(s, V) = \{\alpha \in \Act(s) \mid supp(\alpha) = V\} = \{(K, \mu) \in \Act(s) \mid \mu(V) = 1 \wedge \forall v \in V, \ \mu(v) > 0\}$$

	Por último, dado un estado $s$, definiremos como $V_s$ al conjunto de todos los
	soportes que pueden tener las distintas acciones desde $s$. Es decir,

	\begin{align*}~\label{definicionVs}
		V_s &= \{supp(\mu) \mid \exists K \in \Theta(s): \mu \in K\} = \\
		&= \{V' \subseteq \St | \exists \mu \in K, K \in \Theta(s) \text{ tal que } \mu(V') = 1 \text{ y } \forall s' \in V', \mu(s') > 0\}
	\end{align*}

\end{definition}

Entonces, ahora sí podemos extender el concepto de estrategia en un PSG:

\begin{definition}[Estrategia en un PSG]
	Una estrategia $\pi_i$ para el jugador $i$ ($i \in \{\cuad, \diam\}$) en un PSG será una función $\pi_i : (\St \times \Act) \St_i \rightarrow \textsf{PMeas}(\Act)$ que asigna una medida de probabilidad a cada $\omega s$ tal que $\pi_i(\omega s)(\Act(s)) = 1$.
\end{definition}

\hl{Habría que agregar alguna justificación de esta idea de comportamientos?}

\begin{boxamarillo}[Medida de probabilidad que no va a ir probablemente]{}
	Con la formalización del concepto de estrategia ahora podemos presentar
	formalmente la definición de $\ProbG$, la medida de probabilidad definida por
	la cadena de Markov dada por $\Gk$ y las estrategias $\picuad$ y $\pidiam$.

	Para eso, primero para cada $n \geq 0$ y $s \in \St$ definimos $\Prob^{\picuad,
			\pidiam, n}_{\Gk, s} : \Omega^{n+1} \rightarrow [0, 1]$ para todo $s' \in \St$
	y $\hat{\omega} \in \Omega_s^{n+1}$ inductivamente como sigue:

	\begin{align*}
		&\Prob^{\picuad, \pidiam, 0}_{\Gk, s}(s') = \delta_s(s') \\
		&\Prob^{\picuad, \pidiam, n+1}_{\Gk, s}(\hat{\omega}s_nV's') = \Prob^{\picuad, \pidiam, n}_{\Gk, s}(\hat{\omega} s_n) \int_{\{ a \in A(s_n) \mid \text{sop}(a) = V'\}} \theta(s_n, a, s') \, d(\pi_i(\hat{\omega}s_n)(a)) \\ & \qquad \qquad \qquad \qquad \qquad \text{si } s_n \in \St_i \text{ con } i \in \{\square, \diamondsuit\}
	\end{align*}

	(sop(a) = V' significa que el soporte de a es V')

	(capaz restringirse sobre las acciones con soporte V'? No sé bien cómo va a quedar esto)

	y extendemos \( \Prob^{\pi_\square, \pi_\diamondsuit, n}_{\Gk, s} :
	\mathcal{P}(\Omega^{n+1}) \to [0, 1] \) a conjuntos como la suma de la medida
	sobre los elementos del conjunto.

	Sea \( \Sigma_\Omega \) la \( \sigma \)-álgebra discreta sobre \( \Omega \)
	(pues tanto $\St$ como los conjuntos soportes serán conjuntos finitos) y \(
	\Sigma_{\Omega^\omega} \) la \( \sigma \)-álgebra producto usual sobre \(
	\Omega^\omega \). Por el teorema de extensión de Carathéodory, \(
	\Prob^{\picuad, \pidiam}_{\Gk, s} : \Sigma_{\Omega^\omega} \to [0, 1] \) se
	define como la única medida de probabilidad tal que para todo \( n \geq 0 \), y
	\( SV_i \in \Sigma_\Omega \), \( 0 \leq i \leq n \),

	\[
		\Prob^{\picuad, \pidiam}_{\Gk, s}(SV_0 \times \dots \times SV_n \times \Omega^\omega) = \Prob^{\picuad, \pidiam, n}_{\Gk, s}(SV_0 \times \dots \times SV_n).
	\]
\end{boxamarillo}

Entonces, logramos interpretar un PSG como un SG pero con un conjunto infinito
de acciones. La idea de este paper va a ser demostrar cómo se pueden probar
ciertos resultados de esta interpretación infinita con resultado en una
interpretación finita, que presentamos a continuación.

\begin{definition}[Interpretación extrema de un PSG]
	Dada $\Gk = (\St, (\St_\cuad, \St_\diam), \Act, \theta)$ la interepretación de un PSG $\K$, la interpretación extrema de $\K$ es el juego estocástico $\Hk = (\St, (\St_\cuad, \St_\diam), \V(\Act), \theta_{\Hk})$ donde $\theta_{\Hk}$ es la restricción de  $\theta$ a las acciones en $\V(\Act) = \{(K, \mu) \in \Act \mid \mu \in \V(K)\}$. Es decir, para todos $s, s' \in \St$ y $a \in \V(\Act)$, $\theta_{\Hk}(s, a, s') = \theta(s, a, s')$.

	Como $\V(\Act)$ es finito, $\Hk$ es un juego estocástico finito.
\end{definition}

\section{Teoremas}

En esta sección presentamos los resultados principales desarrollados en
\cite{Polytopal}.

Si bien el paper aborda 4 tipos distintos de objetivos (a saber, ...), solo nos
concentraremos en los resultados que el paper presenta para objetivos de
alcanzabilidad.

El primer teorema, nos permite establecer la determinación de los juegos
estocásticos a la vez de que establece que los objetivos de alcanzabilidad en
PSGs puede ser resueltos de manera equivalente en la interpretación extrema del
PSG.

\begin{theorem}[Reducción de PSGs]
	Sean $\Gk$ y $\Hk$ la interpretación y la interpretación extrema, respectivamente, de un juego estocástico politópico $\K$. Sea $C$ un subconjunto de estados en $\K$. Entonces vale que:
	\begin{align*}
		&\inf_{\pidiam \in \Pidiam} \sup_{\picuad \in \Picuad} \ProbG(\alc C) = \\
		&\inf_{\pidiam \in \Pidiam^{MD}} \sup_{\picuad \in \Picuad^{MD}} \ProbH(\alc C) = \\
		&\sup_{\picuad \in \Picuad^{MD}} \inf_{\pidiam \in \Pidiam^{MD}} \ProbH(\alc C) = \\
		&\sup_{\picuad \in \Picuad} \inf_{\pidiam \in \Pidiam} \ProbG(\alc C)
	\end{align*}
\end{theorem}

Dado que las interpretaciones extremas son finitas, los valores se pueden
calcular siguiendo algoritmos conocidos \cite{CONDON1992} (agg refe filar). Por
lo tanto, este teorema también proporciona inmediatamente una solución
algorítmica para los PSGs.

El segundo resultado en el paper, se deriva fácilmente del primero y será el
que nos permitirá hablar de la complejidad de hallar un valor (referenciar la
definición) en un juego estocástico politópico.

\begin{theorem}[Complejidad en un PSG]
	Para todo PSG $\K$, $q \in \mathbb{Q}$, $G \subseteq \St$ y $s \in \St$, el problema de decidir si $\val_{\Gk, s}(\alc G) \geq q$ está en $NP \cap coNP$.
\end{theorem}

\hl{Agregar algo en prelim o apéndice de clases de complejidad?}

% El número de vértices de un politopo crece exponencialmente en la dimensión del
% politopo [16]. Más precisamente, si \( d \) es la dimensión de un politopo \( K
% \) y \( m \) es el número de desigualdades que lo definen, entonces \( V(K)
% \sim \Omega(m^{\lfloor d/2 \rfloor}) \). Esto implica que la interpretación
% extrema \( \gameH_\gameK \) crece exponencialmente con el tamaño más grande de
% los conjuntos de soporte de las distribuciones involucradas en el PSG original
% \( \gameK \), lo cual esperamos que no sea demasiado grande. (En nuestro
% ejemplo de la Sección 2, \( \lfloor d/2 \rfloor = 2 \)). \duda{¿Qué son los
% 	conjuntos soporte?}

% Condon [9] demostró que decidir la alcanzabilidad en los juegos estocásticos
% está en \( \textbf{NP} \cap \textbf{coNP} \). A pesar del crecimiento
% exponencial, este sigue siendo nuestro caso, como mostramos a continuación. Sea
% \( \text{Val}_s(\gameK) \) el valor del juego en el estado \( s \), es decir,
% es igual a
% \[
% 	\sup_{\pi_\square \in \Pi_\square} \inf_{\pi_\diamondsuit \in \Pi_\diamondsuit} P^{\pi_\square, \pi_\diamondsuit}_{\gameG_\gameK,s}(\diamondsuit G), \quad \text{o} \quad \sup_{\pi_\square \in \Pi_\square} \inf_{\pi_\diamondsuit \in \Pi_\diamondsuit} \mathbb{E}^{\pi_\square, \pi_\diamondsuit}_{\gameG_\gameK,s}[\text{rew}_\textsf{f}],
% \]
% el problema es entonces decidir si \( \text{Val}_s(\gameK) \geq q \), para un
% dado \( q \in \mathbb{Q} \) y \( s \in \states \).

% Dado que en todos los casos (recompensa total, recompensa descontada,
% recompensa promedio y objetivos de alcanzabilidad bajo las respectivas
% condiciones) el valor \( \text{Val}_s(\gameK) \) del juego puede lograrse con
% estrategias determinísticas y sin memoria extremas, podemos razonar de la
% siguiente manera:
% \begin{enumerate}
% 	\item Adivinar (proponer?) una estrategia determinística y sin memoria para cada
% 	      jugador,
% 	\item En la cadena de Markov resultante, calcular la medida correspondiente (es
% 	      decir, recompensa total, recompensa descontada, recompensa promedio o
% 	      alcanzabilidad) en el conjunto respectivo de ecuaciones lineales, lo cual se
% 	      puede hacer en tiempo polinómico (para recompensa, se necesita una suma lineal
% 	      extra) [19],
% 	\item Verificar si es un punto fijo de las ecuaciones de Bellman (para recompensas
% 	      descontadas o totales), o un punto fijo del Algoritmo 5.1.1 de [12], en el caso
% 	      de recompensa promedio, \duda{¿Qué son las ecuaciones de punto fijo de
% 		      Bellman?}
% 	\item Comprobar si \( \text{Val}_s(\gameK) \geq q \).
% \end{enumerate}

% Esto coloca nuestro problema en \( \textbf{NP} \). Con el mismo proceso,
% podemos verificar si \( \text{Val}_s(\gameK) < q \), lo que también pone el
% problema en \( \textbf{coNP} \). Por lo tanto, tenemos el siguiente teorema:

% \textbf{Teorema 2.} Para cualquier PSG \( \gameK \), \( q \in \mathbb{Q} \) y \( s \in \states \), el problema de decidir si \( \text{Val}_s(\gameK) \geq q \) está en \( \textbf{NP} \cap \textbf{coNP} \). Para \( \text{rew}_\textsf{f} \in \{\text{rew}_\textsf{t}, \text{rew}_\textsf{a}\} \), el problema de decisión está restringido a \( \gameG_\gameK \) siendo casi seguramente detenedor e irreducible, respectivamente.
