\chapter{Juegos Estocásticos Politópicos - PSGs}
~\label{cap:psg}
\vspace{-1cm}

El propósito de este capítulo es presentar las definiciones y los hallazgos que
nos son más relevantes del paper \textit{Polytopal Stochastic Games}
\cite{Polytopal} haciéndoles ciertos cambios que fueron necesarios para los
resultados que mostraremos más adelante. Este trabajo es el que sirvió de
inspiración para esta tesina y toda la producción aquí es una extensión del
mismo.

El paper, publicado en 2025, es el primero en presentar el concepto de
\textit{juego estocástico politópico}, respondiendo a la necesidad de juegos
estocásticos que puedan capturar mayor incertidumbre sobre las distribuciones
de probabilidad que determinan las acciones, haciendo esto a través de
inecuaciones lineales cuyas soluciones forman un politopo.

% respondiendo a la necesidad de juegos
% estocásticos que puedan capturar mayor incertidumbre sobre las distribuciones
% de probabilidad, haciendo esto a través de inecuaciones lineales cuyas
% soluciones forman un politopo.

%Discutiremos más sobre la necesidad de creación de este tipo de juegos y qué ventajas proveen y qué motiva su estudio ?) y dividiremos ...
Dividiremos el capítulo en dos secciones: definiciones y teoremas.

\section{Definiciones}

Siendo $S$ un conjunto, en lo siguiente, $\mathscr{P}(S)$ representará el
conjunto partes de $S$ y $\mathscr{P}_f(S)$ denotará el conjunto de los
subconjuntos finitos de $S$.

\subsection*{Politopos}

Un \textbf{politopo convexo} en $\mathbb{R}^n$ es un conjunto acotado $K = \{ x
	\in \mathbb{R}^n | Ax \leq b\}$ con $A \in \mathbb{R}^{m\times n}$ y $ b \in
	\mathbb{R}^m$, para algún $m \in \mathbb{N}$.

Por acotado nos referimos a que $\exists \ M \in \mathbb{R}_{\geq 0}$ tal que
$\sum_{i=1}^{n} |x_i| \leq M \ \forall x \in K$, y siendo $S$ un conjunto
finito, como las funciones en $\mathbb{R}^S$ pueden ser vistas como vectores en
$\mathbb{R}^{|S|}$, generalmente, nos referiremos a politopos en
$\mathbb{R}^S$.

Sea \( \Poly(S) \) el conjunto de todos los politopos convexos en \(
\mathbb{R}^S \). Nótese que el conjunto de todas las funciones de probabilidad
en \( S \) forma el politopo convexo

\( \Dist(S) = \{ \mu \in \mathbb{R}^S \mid \sum_{s \in S} \mu(s) = 1 \text{ y } \forall s \in S : \mu(s) \geq 0 \} \)

Definimos \( \DPoly(S) = \{ K \cap \Dist(S) \mid K \in \Poly(S) \} \). Cada \(
K \in \DPoly(S) \) es un politopo convexo cuyos elementos son también funciones
de probabilidad sobre \( S \), y su conjunto de desigualdades característico \(
Ax \leq b \) ya codifica las desigualdades \( \sum_{s \in S} x_s = 1 \) y \(
x_s \geq 0 \) para todo \( s \in S \).

Cualquier politopo convexo \( K \in \Poly(S) \) puede caracterizarse
alternativamente como la envolvente convexa de su conjunto finito de vértices.
Sea \( \mathbb{V}(K) \) el conjunto de todos los vértices del politopo \( K \).
Si \( \mathbb{V}(K) = \{v^1, \dots, v^k\} \), entonces todo \( x \in K \) es
una combinación convexa de \( \{v^1, \dots, v^k\} \), es decir, \( x =
\sum_{i=1}^{k} \lambda_i v_i \) con \( \lambda_i \geq 0 \), para \( i \in
[1..k] \), y \( \sum_{i=1}^{k} \lambda_i = 1 \).

% \subsection{Juegos estocásticos}

% \hl{Esto mepa que no va pq ya está en el capítulo 3}

% Un juego estocástico es una tupla \( \G = (\St, (\St_\square,
% \St_\diamondsuit), \Act, \theta) \), donde \( \St \) es un conjunto finito de
% estados, con \( \St_\square, \St_\diamondsuit \subseteq \St \) siendo una
% partición de \( \St \), y \( \theta : \St \times \Act \times \St \to [0, 1] \)
% es una función de transición probabilística tal que para todo \( s \in \St \) y
% \( a \in \Act \), \( \theta(s, a, \cdot) \in \text{Dist}(\St) \) o \( \theta(s,
% a, \St) = 0 \) (dados un estado $s$ y una acción $a$, $\theta$ define una
% distribución en $s$ o no hay estados alcanzables por $a$ desde $s$).

% Sea \( \Act(s) = \{ a \in \Act \mid \theta(s, a, \St) = 1 \} \) el conjunto de
% acciones habilitadas en el estado \( s \). Si \( \St_\square = \emptyset \) o
% \( \St_\diamondsuit = \emptyset \), entonces \( \G \) es un proceso de decisión
% de Markov (o MDP). Si, además, \( |\Act(s)| = 1 \) para todo \( s \in \St \),
% \( \G \) es una cadena de Markov (o MC).

% Un camino en el juego \( \G \) es una secuencia infinita de estados \( \rho =
% s_0 s_1 \dots \) tal que \( \theta(s_k, a, s_{k+1}) > 0 \) para alguna \( a \in
% \Act \) y para todo \( k \in \mathbb{N} \). Para \( i \geq 0 \), \( \rho_i \)
% indica el \( i \)-ésimo estado en el camino \( \rho \) (nótese que \( \rho_0 \)
% es el primer estado en \( \rho \)). Denotamos por \( \text{Paths}_\G \) el
% conjunto de todos los caminos, y por \( \text{FPaths}_\G \) el conjunto de los
% prefijos finitos de caminos. De manera similar, \( \text{Paths}_{\G,s} \) y \(
% \text{FPaths}_{\G,s} \) denotan el conjunto de caminos y el conjunto de caminos
% finitos que comienzan en el estado \( s \).

% Una estrategia para el jugador \( i \) (para \( i \in \{\square, \diamondsuit\}
% \)) en un juego \( G \) es una función \( \pi_i : \St^\ast \St_i \to
% \text{Dist}(\Act) \) que asigna una distribución probabilística a cada
% secuencia finita de estados, tal que \( \pi_i(\hat{\rho}s)(a) > 0 \) solo si \(
% a \in \Act(s) \). El conjunto de todas las estrategias para el jugador \( i \)
% se denomina \( \Pi_i \). Cuando sea conveniente, indicamos que el conjunto de
% estrategias \( \Pi_i \) pertenece al juego \( \G \) escribiendo \( \Pi_{\G,i}
% \).

% Una estrategia \( \pi_i \) se dice pura o determinista si, para todo \(
% \hat{\rho}s \in \St^\ast \St_i \), \( \pi_i(\hat{\rho}s) \) es una distribución
% de Dirac (es decir, una distribución \( \delta_a \) tal que \( \delta_a(a) = 1
% \) y \( \delta_a(b) = 0 \) para todo \( b \neq a \)), y se llama sin memoria si
% \( \pi_i(\hat{\rho}s) = \pi_i(s) \), para todo \( \hat{\rho} \in \St^\ast \).
% Sea \( \Pi^M_i \) el conjunto de todas las estrategias sin memoria para el
% jugador \( i \), y \( \Pi^{MD}_i \) el conjunto de todas sus estrategias
% deterministas y sin memoria.

% Dadas las estrategias \( \pi_\square \in \Pi_\square \) y \( \pi_\diamondsuit
% \in \Pi_\diamondsuit \), y un estado inicial \( s \), el resultado del juego es
% una cadena de Markov denotada \( \G^{\pi_\square, \pi_\diamondsuit}_s \).

% La cadena de Markov \( \G^{\pi_\square, \pi_\diamondsuit}_s \) define
% una\textbf{ medida de probabilidad} \( \Prob^{\pi_\square,
% 	\pi_\diamondsuit}_{\G,s} \) sobre la \( \sigma \)-álgebra de Borel generada por
% los cilindros de \( \text{Paths}_{\G,s} \). Si \( \xi \) es un conjunto medible
% en dicha \( \sigma \)-álgebra de Borel, \( \Prob^{\pi_\square,
% 	\pi_\diamondsuit}_{\G,s}(\xi) \) es la probabilidad de que las estrategias \(
% \pi_\square \) y \( \pi_\diamondsuit \) sigan un camino en \( \xi \) partiendo
% del estado \( s \).

% Usamos la notación LTL para representar conjuntos específicos de caminos, en
% particular: \( D U^n C = \{ \rho \in \St^\omega \mid \rho_n \in C \text{ y }
% \forall j < n : \rho_j \in D \} = D^n \times C \times \St^\omega \) es el
% conjunto de caminos que alcanzan \( C \subseteq \St \) en exactamente \( n \geq
% 0 \) pasos, recorriendo antes solo estados en \( D \subseteq \St \); \(
% \diamondsuit^n C = \St U^n C \) es el conjunto de todos los caminos que
% alcanzan los estados en \( C \) en exactamente \( n \) pasos; \( \diamondsuit C
% = \bigcup_{n \geq 0} (\St \setminus C) U^n C \) es el conjunto de todos los
% caminos que alcanzan un estado en \( C \).

\subsection*{PSGs}

% Los juegos estocásticos politópicos fueron presentados en 2024 por Castro y
% D'Argenio \cite{Polytopal}. La idea de su creción ...

Un juego estocástico politópico se caracteriza a través de una estructura que
contiene un conjunto finito de estados divididos en dos conjuntos, cada uno
perteneciente a un jugador diferente. Además, a cada estado se le asigna un
conjunto finito de politopos convexos de distribuciones de probabilidad sobre
los estados. La definición formal es como sigue:

\begin{definition}[PSG]
	Un juego estocástico politópico (abreviado PSG, por sus siglas en inglés) es una estructura \( \K = (\St, (\St_\square, \St_\diamondsuit), \Theta) \) tal que \( \St \) es un conjunto finito de estados particionado en \( \St = \St_\square \biguplus \St_\diamondsuit \) y \( \Theta : \St \to \mathscr{P}_f(\textsf{DPoly}(\St)) \).
\end{definition}

Un juego estocástico politópico se diferencia de un juego estocástico
tradicional por el hecho de que desde un estado \( s \in \St_i \) (para \( i
\in \{ \square, \diamondsuit \} \)), el jugador \( i \) elige jugar un politopo
\( K \in \Theta(s) \) y una distribución \( \mu \in K \), en vez de elegir
directamente una acción de entre un conjunto finito. Al igual que en un juego
estocástico, el siguiente estado \( s' \) se selecciona de acuerdo con la
distribución \( \mu \) correspondiente a la acción seleccionada, y el juego
continuará desde \( s' \) repitiendo el mismo proceso.

Viendo esto, es natural pensar que desarrollo de un juego estocástico
politópico se puede interpretar en términos de un juego estocástico donde el
número de transiciones salientes desde los estados de los jugadores puede ser
no numerable. Formalmente, la interpretación de un PSG es la siguiente:

\begin{definition}[Interpretación de un PSG]
	La interpretación del juego estocástico politópico \( \K \) se define por el juego estocástico \( \Gk = (\St, (\St_\square, \St_\diamondsuit), \Act, \theta) \), donde \( \Act = \bigcup_{s \in \St} \Theta(s) \times \textsf{Dist}(\St) \) y
	\[
		\theta(s, (K, \mu), s') =
		\begin{cases}
			\mu(s') & \text{si } K \in \Theta(s) \text{ y } \mu \in K \\
			0 & \text{en otro caso}.
		\end{cases}
	\]
\end{definition}

Nótese que el conjunto de acciones \( \Act \) puede ser no numerable, al igual
que cada conjunto \( \Act(s) = \bigcup_{K \in \Theta(s)} \{K\} \times K \) de
todas las acciones realizables en el estado s, identificado por el politopo
elegido y la distribución seleccionada dentro del politopo. Por lo tanto,
necesitamos extender las estrategias a este dominio no numerable, que debe
estar dotado de una \( \sigma \)-álgebra adecuada.

Para esto, utilizamos una construcción estándar para darle a \(
\textsf{Dist}(S) \) una \( \sigma \)-álgebra: \( \Sigma_{\textsf{Dist}(S)} \)
se define como la \( \sigma \)-álgebra más pequeña que contiene los conjuntos
\( \{\mu \in \textsf{Dist}(S) \mid \mu(S) \geq p\} \) para todo \( S \subseteq
S \) y \( p \in [0, 1] \). Ahora, dotamos a \( \Act \) con la \( \sigma
\)-álgebra producto \( \Sigma_\Act = \mathscr{P} \left(\bigcup_{s \in S}
\Theta(s)\right) \otimes \Sigma_{\Dist(S)} \), y llamamos \( \textsf{PMeas}(A)
\) al conjunto de todas las medidas de probabilidad sobre \( \Sigma_\Act \). No
es difícil comprobar que cada conjunto de acciones habilitadas \( \Act(s) \) es
medible (es decir, \( \Act(s) \in \Sigma_\Act \)) y que la función \( \theta(s,
\cdot, s') \) es medible (es decir, \( \{a \in \Act \mid \theta(s, a, s') \leq
p\} \in \Sigma_\Act \) para todo \( p \in [0, 1] \)).

Ahora bien, para dar la medida de probabilidad $\Prob^{\picuad, \pidiam}$
debemos primero presentar las nociones de camino y estrategia en un PSG.

Como lo hicimos para MDPs y juegos estocásticos a la definición de camino la
daremos como una secuencia de estados y acciones. Esto difiere de la
formulación original hecha para PSGs en \cite{Polytopal}, pero cambiaremos
acordemente las definiciones que se asocian a ello para mantener la formalidad
de los resultados.

\begin{definition}[Camino en un PSG]
	Un camino en un PSG es una secuencia infinita que alterna entre estados y acciones. Formalmente un camino es un $\omega = (s_0, (K_0, \mu_0), s_1, (K_1, \mu_1), \dots)$ tal que $s_i \in \St$, $(K_i, \mu_i) \in \Act(s_i)$ y $\mu_i(s_{i+1}) > 0$ para todo $i \geq 0$.

	Al igual que para juegos estocásticos, notaremos al conjunto de todos los
	caminos de un PSG como $\Paths$, con $\Pathsfin$ al conjunto de los prefijos de
	caminos finitos y con $\Pathsfin^i$ al conjunto de los prefijos de camino
	finitos que terminan en un estado $s \in \St_i$. Dado un estado $s$,
	indicaremos con $\Paths_s$ el conjunto de los caminos que se originan en $s$, y
	con $\Paths_s^n$ al conjunto de los caminos que se originan en $s$ y tiene un
	total de $n$ estados.
\end{definition}

\begin{definition}[Conjuntos soporte y acciones]
	Dada una acción $(K, \mu)$ definiremos su conjunto soporte, $\supp((K, \mu))$ como el conjunto formado por los estados a los cuales $(K, \mu)$ les asigna una probabilidad positiva. Es decir,
	$$supp((K, \mu)) = \{s \in \St \mid \mu(s) > 0\}$$.
	Dado un estado $s$ y un conjunto de estados $V$, definiremos como $acc(s, V)$ a
	las acciones que parten desde $s$ y tienen como conjunto soporte $V$. Es decir,
	$$acc(s, V) = \{\alpha \in \Act(s) \mid supp(\alpha) = V\} = \{(K, \mu) \in \Act(s) \mid \mu(V) = 1 \wedge \forall v \in V, \ \mu(v) > 0\}$$.
	Por último, dado un estado $s$, definiremos como $V_s$ al conjunto de todos los
	soportes que pueden tener las distintas acciones desde $s$. Es decir,
	\begin{align*}~\label{definicionVs}
		V_s &= \{supp(\mu) \mid \exists K \in \Theta(s): \mu \in K\} = \\
		&= \{V' \subseteq \St | \exists \mu \in K, K \in \Theta(s) \text{ tal que } \mu(V') = 1 \text{ y } \forall s' \in V', \mu(s') > 0\}.
	\end{align*}
\end{definition}

Entonces, ahora sí podemos extender el concepto de estrategia en un PSG:

\begin{definition}[Estrategia en un PSG]
	Una estrategia $\pi_i$ para el jugador $i$ ($i \in \{\cuad, \diam\}$) en un PSG será una función $\pi_i : \Pathsfin^i \rightarrow \textsf{PMeas}(\Act)$ que asigna una medida de probabilidad a cada $\omega s$ tal que $\pi_i(\omega s)(\Act(s)) = 1$.
\end{definition}

Los distintos tipos de estrategias que presentamos en el
capítulo~\ref{cap:modelos} se extienden naturalmente a este tipo de
estrategias, y los conjuntos de estrategias de ese tipo se denotarán de la
misma manera en la que se presentaron con juegos estocásticos. A estos tipos de
estrategias agregamos una particular de los juegos estocásticos politópicos:
las estrategias extremas.

\begin{definition}[Estrategia extrema en un PSG]
	Una estrategia $\pi_i$ para el jugador $i$ ($i \in \{\cuad, \diam\}$) en un PSG se dirá \textbf{extrema} si para todo $\hat \omega \in \Pathsfin$, $\pi_i(\hat \omega s)\{(K, \mu) \in \Act(s)| \mu \in \mathbb{V}(K)\} = 1$. Es decir, diremos que una estrategia es extrema si solo asigna probabilidades positivas a distribuciones en los vértices del politopo.

	Notaremos con $\Pi_i^X$ a la familia de estrategias extremas para el jugador
	$i$ y podremos combinar esta notación con las presentadas para otras familias
	de estrategias, por ejemplo, notando a la familia de estrategias extremas,
	puras y sin memoria del jugador $i$ como $\Pi_i^{XMD}$.
\end{definition}

% En este caso también tendremos dos clasificaciones distintas que se pueden dar
% de las estrategias. En primer lugar, se puede ver si una estrategia es
% determinista o randomizada:

% \kathy{Tengo que agregar definiciones}

% Y, en segundo lugar, se puede ver si una estrategia es sin memoria, con memoria
% finita.
%Hemos visto el concepto de las estrategias sin memoria y con memoria finita y la extensión de las mimsas a este contexto se da de manera natural

\begin{boxamarillo}[Medida de probabilidad que no va a ir probablemente]{}
	Con la formalización del concepto de estrategia ahora podemos presentar
	formalmente la definición de $\ProbG$, la medida de probabilidad definida por
	la cadena de Markov dada por $\Gk$ y las estrategias $\picuad$ y $\pidiam$ en el estado $s$.

	Para eso, primero para cada $n \geq 0$ y $s \in \St$ definimos $\Prob^{\picuad,
			\pidiam, n}_{\Gk, s} : \Paths^{n+1} \rightarrow [0, 1]$ para todo $s' \in \St$
	y $\hat{\omega} \in \Paths_s^{n+1}$ inductivamente como sigue:

	\begin{align*}
		&\Prob^{\picuad, \pidiam, 0}_{\Gk, s}(s') = \delta_s(s') \\
		&\Prob^{\picuad, \pidiam, n+1}_{\Gk, s}(\hat{\omega}s_nV's') = \Prob^{\picuad, \pidiam, n}_{\Gk, s}(\hat{\omega} s_n) \int_{\{ a \in A(s_n) \mid \supp(a) = V'\}} \theta(s_n, a, s') \, d(\pi_i(\hat{\omega}s_n)(a)) \\ & \qquad \qquad \qquad \qquad \qquad \text{si } s_n \in \St_i \text{ con } i \in \{\square, \diamondsuit\}
	\end{align*}

	(supp(a) = V' significa que el soporte de a es V')

	(capaz restringirse sobre las acciones con soporte V'? No sé bien cómo va a quedar esto)

	y extendemos \( \Prob^{\pi_\square, \pi_\diamondsuit, n}_{\Gk, s} :
	\mathcal{P}(\Paths^{n+1}) \to [0, 1] \) a conjuntos como la suma de la medida
	sobre los elementos del conjunto.

	Sea \( \Sigma_\Paths \) la \( \sigma \)-álgebra discreta sobre \( \Paths \)
	(pues tanto $\St$ como los conjuntos soportes serán conjuntos finitos) y \(
	\Sigma_{\Paths^\omega} \) la \( \sigma \)-álgebra producto usual sobre \(
	\Paths^\omega \). Por el teorema de extensión de Carathéodory, \(
	\Prob^{\picuad, \pidiam}_{\Gk, s} : \Sigma_{\Paths^\omega} \to [0, 1] \) se
	define como la única medida de probabilidad tal que para todo \( n \geq 0 \), y
	\( SV_i \in \Sigma_\Paths \), \( 0 \leq i \leq n \),

	\[
		\Prob^{\picuad, \pidiam}_{\Gk, s}(SV_0 \times \dots \times SV_n \times \Paths^\omega) = \Prob^{\picuad, \pidiam, n}_{\Gk, s}(SV_0 \times \dots \times SV_n).
	\]
\end{boxamarillo}

Así se logra interpretar un PSG como un SG pero con un conjunto infinito de
acciones. \kathy{Pedro prometió hacer esto que está en el box amarillo bien :)}
En \cite{Polytopal} se demuestra cómo ciertas preguntas que nos podemos hacer
sobre esta interpretación infinita se pueden responder analizando una
interpretación distinta con una cantidad finita de acciones, la cual
presentamos a continuación.

\begin{definition}[Interpretación extrema de un PSG]
	Dada $\Gk = (\St, (\St_\cuad, \St_\diam), \Act, \theta)$ la interepretación de un PSG $\K$, la interpretación extrema de $\K$ es el juego estocástico $\Hk = (\St, (\St_\cuad, \St_\diam), \V(\Act), \theta_{\Hk})$ donde $\theta_{\Hk}$ es la restricción de  $\theta$ a las acciones en $\V(\Act) = \{(K, \mu) \in \Act \mid \mu \in \V(K)\}$. Es decir, para todos $s, s' \in \St$ y $a \in \V(\Act)$, $\theta_{\Hk}(s, a, s') = \theta(s, a, s')$.

	Como $\V(\Act)$ es finito, $\Hk$ es un juego estocástico finito.
\end{definition}

\textbf{Roborta vs Rigoborto en un terreno muy variable}

Hablamos con nuestro amigo y ahora llevaremos a nuestros robots, Roborta y
Rigoborto a un terreno muy particular. En él, las probabiliadedes de fallo
están asociadas a unas restricciones que resolverán su valor dependiendo desde
dónde exactamente en el cuadrante actual decidimos que se mueva nuestro robot.

En este nuevo terreno, cada vez que un jugador elige mover a su robot, lo que
en realidad está eligiendo no es solo una acción concreta (como ``moverse a la
derecha'' o ``bajar''), sino también una distribución de posibles resultados
estocásticos de entre una cantidad continua de posibilidades, que pueden
incluir desvíos normales o fallos críticos.

La idea es que otra vez estamos en una grilla 2x2, con lo que los estados los
podemos seguir representando de igual manera, lo que cambiará en este juego,
con respecto al juego estocástico que planteamos para Roborta y Rigoborto, es
que desde los estados en los que les toca elegir a robots que se encuentran en
$(0,0)$ o $(1,1)$ tendremos una cantidad continua de opciones determinada por
un politopo.

Este politopo estará definido por tres variables que representan la
probabilidad de que el comando se ejecute correctamente $p_1$, la probabilidad
de que el comando sufra un fallo normal y el robot se dirija hacia la esquina
opuesta, $p_2$, y la probabilidad de que el comando sufra un fallo crítico y el
robot se dirija de manera opuesta a como fue propuesto por el comando (es
decir, si eligió desde $(0,0)$ ir a la derecha, ir hacia abajo y viceversa, y
si se eligió ir a la izquierda desde $(1,1)$ ir hacia arriba y viceversa),
$p_3$. Estas tres variables repetarán algunas restricciones formando así el
politopo $P$ de soluciones a las ecuaciones que mostramos a continuación.

\[
	P = \left\{ (p_1, p_2, p_3) \in \mathbb{R}^3 \;\middle|\;
	\begin{aligned}
		p_1 + p_2 + p_3 &= 1 \\
		0.05 \leq p_2 &\leq 0.3 \\
		0.0 \leq p_3 &\leq 0.2 \\
		p_1 &\geq 0
	\end{aligned}
	\right\}
\]

Por otro lado, en los otros estados tendremos politopos pero que representan un
punto en el espacio que definen $p1$, $p_2$ y $p_3$, haciendo simplemente que
$p_1 = 1$ y $p_2 = p_3 = 0$.

Luego, con todo esto definido, podríamos ver cómo se adaptaría el fragmento de
juego que estuvimos viendo para juegos estocásticos y deterministas, en donde
asumimos que partimos desde el estado en donde nos toca mover a Roborta, ella
se encuentra en la posición $(0,0)$ y Rigoborto se encuentra en la posición
$(1,0)$. Desde ese estado nosotros elegiremos si enviaremos a Roborta a la
derecha o abajo, pero cuando hacemos eso también debemos elegir qué
distribución dentro del único politopo que tenemos usaremos para movernos (esto
podría pensarse como elegir el punto exacto desde nuestro cuadrante sobre el
que ejecutamos la acción). Es decir, decidimos entre $\rightarrow$ y
$\downarrow$, y también fijamos los valores de $p_1$, $p_2$ y $p_3$ siguiendo
las restricciones impuestas. En la figura~\ref{fig:psg} podemos ver la
representación gráfica de esto.

\begin{figure}[ht]
	\centering
	\begin{tikzpicture}[->]
		\tikzset{
			state/.style={draw, circle, minimum size=1.2cm, font=\normalsize},
			square state/.style={draw, rectangle, minimum size=1.4cm, font=\normalsize, align=center},
			diamond state/.style={draw, diamond, aspect=1.5, minimum size=1.5cm, font=\normalsize, align=center},
			every node/.style={inner sep=4pt}
		}

		% Nodo raíz a la izquierda
		\node[square state] (left) at (0, 0) {$(0,0)$\\$(1,0)$};

		% Nivel intermedio (rombos) centrados horizontalmente, espaciados verticalmente
		\node[diamond state] (d1) at (3, 4) {$(0,1)$\\$(1,0)$};
		\node[diamond state] (d2) at (3, 0) {$(1,1)$\\$(1,0)$};
		\node[diamond state] (d3) at (3, -4) {$(1,0)$\\$(1,0)$};

		% Nodos hojas (cuadrados), suficientemente separados
		\node[square state] (s1) at (6, 5) {$(0,1)$\\$(0,0)$};
		\node[square state] (s2) at (6, 3) {$(0,1)$\\$(1,1)$};

		\node[square state] (s3) at (6, 1.0) {$(1,1)$\\$(0,0)$};
		\node[square state] (s4) at (6, -1.0) {$(1,1)$\\$(1,1)$};

		\node[square state] (s5) at (6, -3) {$(1,0)$\\$(0,0)$};
		\node[square state] (s6) at (6, -5) {$(1,0)$\\$(1,1)$};

		% Aristas desde left a cada rombo
		\draw[->, draw=violet] (left) to[bend left=10]  node[midway, left, text=violet] {$\rightarrow$,$p_1$} (d1);
		\draw[->, draw=violet]  (left) to[bend left=20]  node[above, text=violet] {$\rightarrow$,$p_2$} (d2);
		\draw[->, draw=violet] (left) to[bend left=10] node[midway, right, text=violet] {$\rightarrow$, $p_3$} (d3);

		\draw[->, draw=orange] (left) to[bend right=10] node[midway, right, text=orange] {$\downarrow$, $p_3$} (d1);
		\draw[->, draw=orange] (left) to[bend right=20]  node[below, text=orange] {$\downarrow$, $p_2$}  (d2);
		\draw[->, draw=orange] (left) to[bend right=10]  node[midway, left, text=orange]  {$\downarrow$, $p_1$} (d3);

		% Aristas desde cada rombo a dos cuadrados
		\draw[->, draw=teal] (d1) to  node[midway, above, text=teal] {$\uparrow$, 1} (s1);
		\draw[->, draw=violet] (d1) to  node[midway, below, text=violet] {$\rightarrow$,1} (s2);

		\draw[->, draw=teal] (d2) to  node[midway, above, text=teal] {$\uparrow$, 1} (s3);
		\draw[->, draw=violet] (d2) to  node[midway, below, text=violet] {$\rightarrow$,1} (s4);

		\draw[->, draw=teal] (d3) to  node[midway, above, text=teal] {$\uparrow$, 1} (s5);
		\draw[->, draw=violet] (d3) to  node[midway, below, text=violet] {$\rightarrow$,1} (s6);

		% Puntos suspensivos para indicar que continúa
		\node at (7,5) {$\cdots$};
		\node at (7,3) {$\cdots$};

		\node at (7,1) {$\cdots$};
		\node at (7,-1) {$\cdots$};

		\node at (7, -3) {$\cdots$};
		\node at (7, -5) {$\cdots$};

		\node[align=left, anchor=west] at (8.2, -4.8) {
			\small Restricciones:\\
			\textcolor{violet}{$p_1 + p_2 + p_3 = 1$}\\
			\textcolor{violet}{$0.05 \leq p_2 \leq 0.3$}\\
			\textcolor{violet}{$0 \leq p_3 \leq 0.2$}\\
			\textcolor{violet}{$p_1 \geq 0$} \\
			\textcolor{orange}{$p_1 + p_2 + p_3 = 1$}\\
			\textcolor{orange}{$0.05 \leq p_2 \leq 0.3$}\\
			\textcolor{orange}{$0 \leq p_3 \leq 0.2$}\\
			\textcolor{orange}{$p_1 \geq 0$}
		};
	\end{tikzpicture}
	\caption{Fragmento de juego estocástico politópico Roborta vs Rigoborto.}
	\label{fig:psg}
\end{figure}

Resulta también interesante ver algún ejemplo de una estrategia extremas aquí.
Estas son las que solo basan sus elecciones en acciones cuya distribución de
probabilidad se corresponde con alguno de los vértices del politopo. Para eso,
consideremos cuáles son los vértices para los politopos definidos. Estos serán
los valores extremos que pueden tomar las variables $p_1$, $p_2$ y $p_3$. Es
decir, nuestro conjunto de vértices pensando en triplas $(p_1, p_2, p_3)$ va a
ser $\{(0.95, 0.05, 0), (0.75, 0.05, 0.2), (0.7, 0.3, 0), (0.5, 0.3, 0.2)\}$.
Luego, un ejemplo de una estrategia sin memoria extrema en nuestro juego podría
ser la que siempre que se encuentra en un estado desde el que puede tomar una
decisión politópica, elige con $0.6$ de probabilidad hacer el movimiento
lateral con la distribución $(0.95, 0.05, 0)$ y con un $0.4$ de probabilidad
hacer el movimiento vertical con la distribución $(0.75, 0.05, 0.2)$.

Además, con lo que vimos en el capítulo anterior, sería interesante pensar en
algún objetivo asociado a este jeugo. En particular, resultaría de interés
presentar un objetivo de Rabin, así que eso es lo que haremos. Podríamos
definir el siguiente objetivo de Rabin con dos pares $R={(E_1,F_1),(E_2,F_2)}$
con los siguientes significados de los conjuntos:

\begin{itemize}
	\item $E_1$: todos los estados donde Roborta y Rigoborto están en la misma celda.
	\item $F_1$: estados donde Roborta está a la izquierda en la grilla.
	\item $E_2$: estados donde Roborta está en la parte inferior de la grilla.
	\item $F_2$: estados donde Rigoborto está en la parte superior de la grilla.
\end{itemize}

Entonces, con este objetivo, el jugador $\cuad$ gana si:
\begin{enumerate}
	\item Eventualmente se dejan de visitar colisiones, y Roborta se mantiene regresando
	      a la izquierda, o
	\item Eventualmente se dejan de visitar estados donde Roborta está en la parte
	      inferior, y Rigoborto se mantiene regresando arriba
\end{enumerate}

Este ejemplo de objetivo resulta interesante porque combina condiciones de
seguridad (evitar colisiones o estados indeseados) y condiciones de liveness
(garantizar que los robots regresen repetidamente a ciertas posiciones), lo que
refleja requisitos típicos en sistemas multiagente y robótica y la expresividad
de los objetivos de Rabin.

Con esto visto, antes de proceder, creemos que es importante remarcar que el
juego estocástico politópico que definimos es bastante simple para lo que
podría ser. En primer lugar, solo en los estados en los que les toca elegir a
robots que se encuentran en $(0,0)$ o $(1,1)$ tendremos una cantidad infinita
de distribuciones de las cuales elegir, en el resto la elección de la acción
determinará directamente un comportamiento certero. En segundo lugar, para los
estados que tenemos una cantidad infinita de soluciones, igual estas solo se
corresponden a un politopo (recordemos que según la definición que dimos, desde
un estado podemos tener una cantidad finita de politopos desde los cuales
elegir). Y, en tercer lugar, los politopos que presentamos no son
particularmente complejos. Es valioso recordar que los politopos pueden tener
mayor dimensión y las variables pueden estar más correlacionadas entre sí.
Tomamos la decisión de simplificar lo más posible el ejemplo a fin de que quede
una imagen clara y entendible de los que es un PSG antes de proceder con los
teoremas mostrados en el paper.

% Existen muchas razones por las cuales podríamos no saber las probabilidades
% exactas de cómo es que podrían fallar nuestras indicaciones a los robots. Podría suceder, por ejemplo, que estemos en un terreno un poco desconocido,

\section{Teoremas}

En esta sección presentamos los resultados principales desarrollados en
\cite{Polytopal}.

Si bien el paper aborda 4 tipos distintos de objetivos (a saber, de
alcanzabilidad, de recompensa media, de recompensa total acumulada y de
recompensa total descontada), solo nos concentraremos en los resultados que el
paper presenta para objetivos de alcanzabilidad.

El primer teorema, establece la determinación de los juegos estocásticos a la
vez de que establece que los objetivos de alcanzabilidad en PSGs puede ser
resueltos de manera equivalente en la interpretación extrema del PSG.

\begin{theorem}[Reducción de PSGs]
	Sean $\Gk$ y $\Hk$ la interpretación y la interpretación extrema, respectivamente, de un juego estocástico politópico $\K$. Sea $C$ un subconjunto de estados en $\K$. Entonces vale que:
	\begin{align*}
		&\inf_{\pidiam \in \Pidiam} \sup_{\picuad \in \Picuad} \ProbG(\alc C) = \\
		&\inf_{\pidiam \in \Pidiam^{MD}} \sup_{\picuad \in \Picuad^{MD}} \ProbH(\alc C) = \\
		&\sup_{\picuad \in \Picuad^{MD}} \inf_{\pidiam \in \Pidiam^{MD}} \ProbH(\alc C) = \\
		&\sup_{\picuad \in \Picuad} \inf_{\pidiam \in \Pidiam} \ProbG(\alc C)
	\end{align*}
\end{theorem}

Dado que las interpretaciones extremas son finitas, los valores se pueden
calcular siguiendo algoritmos conocidos \cite{CONDON1992,filar}. Por lo tanto,
este teorema también proporciona inmediatamente una solución algorítmica para
los PSGs.

El segundo resultado, se deriva del primero y presenta la complejidad del
estudio cuantitativo de los juegos estocásticos politópicos. Enunciado sobre lo
que nos interesa y notando como $\val_{\Gk,s}(\alc G)$ al valor del estado $s$
(es decir, la probabilidad máxima de que $\cuad$ gane desde $s$) en el juego
$\Gk$ con un objetivo de alcanzabilidad sobre un conjunto $G$, el teorema
sería:

\begin{theorem}[Complejidad en un PSG]
	Para todo PSG $\K$, $q \in \mathbb{Q}$, $G \subseteq \St$ y $s \in \St$, el problema de decidir si $\val_{\Gk, s}(\alc G) \geq q$ está en $NP \cap coNP$.
\end{theorem}

Con estos resultados como contexto surge el interés de estudiar los juegos
estocásticos politópicos con otros objetivos. Por lo expuesto en el capítulo
anterior, decidimos que sería una buena idea estudiar los objetivos de Rabin.
En el próximo capítulo nos explayamos sobre los hallazgos que pudimos hacer
durante el desarrollo de esta tesina.

% \hl{¿Agregar algo en prelim o apéndice de clases de complejidad?}

% El número de vértices de un politopo crece exponencialmente en la dimensión del
% politopo [16]. Más precisamente, si \( d \) es la dimensión de un politopo \( K
% \) y \( m \) es el número de desigualdades que lo definen, entonces \( V(K)
% \sim \Paths(m^{\lfloor d/2 \rfloor}) \). Esto implica que la interpretación
% extrema \( \gameH_\gameK \) crece exponencialmente con el tamaño más grande de
% los conjuntos de soporte de las distribuciones involucradas en el PSG original
% \( \gameK \), lo cual esperamos que no sea demasiado grande. (En nuestro
% ejemplo de la Sección 2, \( \lfloor d/2 \rfloor = 2 \)). \duda{¿Qué son los
% 	conjuntos soporte?}

% Condon [9] demostró que decidir la alcanzabilidad en los juegos estocásticos
% está en \( \textbf{NP} \cap \textbf{coNP} \). A pesar del crecimiento
% exponencial, este sigue siendo nuestro caso, como mostramos a continuación. Sea
% \( \text{Val}_s(\gameK) \) el valor del juego en el estado \( s \), es decir,
% es igual a
% \[
% 	\sup_{\pi_\square \in \Pi_\square} \inf_{\pi_\diamondsuit \in \Pi_\diamondsuit} P^{\pi_\square, \pi_\diamondsuit}_{\gameG_\gameK,s}(\diamondsuit G), \quad \text{o} \quad \sup_{\pi_\square \in \Pi_\square} \inf_{\pi_\diamondsuit \in \Pi_\diamondsuit} \mathbb{E}^{\pi_\square, \pi_\diamondsuit}_{\gameG_\gameK,s}[\text{rew}_\textsf{f}],
% \]
% el problema es entonces decidir si \( \text{Val}_s(\gameK) \geq q \), para un
% dado \( q \in \mathbb{Q} \) y \( s \in \states \).

% Dado que en todos los casos (recompensa total, recompensa descontada,
% recompensa promedio y objetivos de alcanzabilidad bajo las respectivas
% condiciones) el valor \( \text{Val}_s(\gameK) \) del juego puede lograrse con
% estrategias determinísticas y sin memoria extremas, podemos razonar de la
% siguiente manera:
% \begin{enumerate}
% 	\item Adivinar (proponer?) una estrategia determinística y sin memoria para cada
% 	      jugador,
% 	\item En la cadena de Markov resultante, calcular la medida correspondiente (es
% 	      decir, recompensa total, recompensa descontada, recompensa promedio o
% 	      alcanzabilidad) en el conjunto respectivo de ecuaciones lineales, lo cual se
% 	      puede hacer en tiempo polinómico (para recompensa, se necesita una suma lineal
% 	      extra) [19],
% 	\item Verificar si es un punto fijo de las ecuaciones de Bellman (para recompensas
% 	      descontadas o totales), o un punto fijo del Algoritmo 5.1.1 de [12], en el caso
% 	      de recompensa promedio, \duda{¿Qué son las ecuaciones de punto fijo de
% 		      Bellman?}
% 	\item Comprobar si \( \text{Val}_s(\gameK) \geq q \).
% \end{enumerate}

% Esto coloca nuestro problema en \( \textbf{NP} \). Con el mismo proceso,
% podemos verificar si \( \text{Val}_s(\gameK) < q \), lo que también pone el
% problema en \( \textbf{coNP} \). Por lo tanto, tenemos el siguiente teorema:

% \textbf{Teorema 2.} Para cualquier PSG \( \gameK \), \( q \in \mathbb{Q} \) y \( s \in \states \), el problema de decidir si \( \text{Val}_s(\gameK) \geq q \) está en \( \textbf{NP} \cap \textbf{coNP} \). Para \( \text{rew}_\textsf{f} \in \{\text{rew}_\textsf{t}, \text{rew}_\textsf{a}\} \), el problema de decisión está restringido a \( \gameG_\gameK \) siendo casi seguramente detenedor e irreducible, respectivamente.
