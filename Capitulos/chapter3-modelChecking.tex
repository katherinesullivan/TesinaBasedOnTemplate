\chapter{Verificación de modelos con juegos}
~\label{cap:modelos}

En este capítulo presentaremos la verificación de modelos con juegos. Para ello
comenzaremos con algunas definiciones que hacen al campo de la verificación de
modelos, luego presentaremos ciertos modelos matemáticos que sirven para la
representación de distintos tipos de sistemas (las cadenas de Markov, los
procesos de decisión de Markov, los juegos estocásticos y los juegos -de grafo-
deterministas), y, por último, plantearemos cuáles son las preguntas de
investigación que podrían ser de interés en estos casos y presentamos con mayor
precisión cuáles son las preguntas de investigación que competen a este
trabajo.

\section{Sobre la verificación de modelos}

La verificación de modelos (model checking) es una técnica prominente dentro de
la verificación formal que sirve para evaluar las propiedades funcionales de
los sistemas de información y comunicación de manera automatizada. Esta técnica
requiere de un modelo del sistema en cuestión y una propiedad deseada, y
verifica sistemáticamente si el modelo dado satisface dicha propiedad.

Las propiedades típicas que se pueden comprobar son la ausencia de
interbloqueos, los invariantes y las propiedades de solicitud-respuesta.

Alternativamente, se la puede pensar como una técnica de depuración inteligente
y eficaz.

Existen distintos modelos matemáticos que permiten el análisis de
representaciones de sistemas reales,,,

blabla

problema de sintesis de church

idea de la necesidad de juegos y nuestro entendimiento de ellxs

¿Que preguntas nos hacemos cuando trabajamos con juegos? Mas que nada con juegos estocasticos
tenemos las preguntas que se hace Kucera, las que presenta Chatterjee y se puede investigar mas
Capaz esto iría mas en la introducción para platear la idea de trabajo

Existen estrategias óptimas para todos los juegos de una determinada clase con
un determinado objetivo? -> mm obj

Cuál es el tipo de las estrategias óptimas? // ganadoras

Podemos computar las estartegias óptimas? Sintésis de estr kinda // ganadoras

Podemos computar o aproximar el valor de un vértice? -> mmm vértice o estado //
quien gana en un vértice dado

Estas preguntas se suelen hacer restringidas a objetivos de algún tipo en
particular.

Notas sobre vertices vs estados + sobre caminos/comportamientos + notacion
juegos deterministas y estocasticos

\section{Cadenas de Markov}

El primer modelo matemático que veremos son las cadenas de Markov. Es
importante notar que trabajaremos con el tratamiento de las cadenas de Markov
como sistemas de transición anotados con probabilidades. Esto en contraposición
al enfoque que plantea a las cadenas de markov como una familia de variables
aleatorias. Pasemos entonces a su definición.

\begin{definition}[Cadena de Markov]
	Una cadena de Markov es una tupla $M = (\St, P)$ donde $\St$ es un conjunto finito no vacío de estados y $P: \St \times \St \rightarrow [0,1]$ es una función tal que para todos los estados $s$ vale que
	$$ \sum_{s' \in \St} P(s, s') = 1$$
\end{definition}

La función de probabilidad de transición $P$ especifica para cada estado $s$ la
probabilidad $P(s, s')$ de moverse de $s$ a $s'$ en un solo paso. La
restricción impuesta en $P$ asegura que la función sea una distribución.

Una cadena de Markov induce un grafo subyacente, donde los estados actúan como
vértices y hay una arista entre $s$ y $s'$ si y solo si $P(s, s') > 0$. Las
cadenas de Markov se suelen representar directamente por su grafo subyacente,
donde sus aristas estarán anotadas con las probabilidades en el intervalo $(0,
	1]$.

Los caminos en una cadena de Markov son los caminos en el grafo subyacente. Es
decir, son definidos como secuencias infinitas de estados $\omega = (s_0, s_1,
	s_2, \dots) \in \St^\omega$ tales que $P(s_i, s_{i+1}) > 0 $ para todo $i \geq
	0$. A su vez, dado un camino infinito $\omega = (s_0, s_1, \dots)$, podemos
pensar en los prefijos finitos de ese camino, $\prefijo(\omega) = \{(s_0,
	\dots, s_n) | n \in \mathbb{N}\}$. Notaremos con $\Paths(M)$ al conjunto de
todos los caminos en $M$ y con $\Pathsfin(M)$ al conjunto de todos los prefijos
finitos de caminos en $M$.

Veamos un pequeño ejemplo de cómo sería una cadena de Markov.

%% VER QUE ONDA ACA LO DEL BOX GRIS
\textbf{Randomito, el peuqeño robot probabilístico}
Supongamos que queremos observar el comportamiento de un pequeño robot llamado \textit{Randomito}.

\textit{Randomito} se encuentra en una grilla 2x2 y se comporta totalmente probabilísticamente siguiendo la siguiente descripción: desde su posición inicial de quietud en la esquina superior izquierda tiene una probabilidad de 0.5 de seguir allí y 0.25 de moverse tanto a la izquierda como hacia abajo. Si sucede que en algún momento se encuentra en la esquina superior o inferior derecha, como a \emph{Randomito} no le gusta quedarse del lado de la derecha, tiene probabilidad 0 de quedarse allí o de ir hacia arriba o abajo, respectivamente. Desde esas esquinas, \emph{Randomito} se mueve con probabilidad 1 hacia la izquierda. Desde la esquina inferior izquierda tendrá una probabilidad de 0.3 de quedarse allí, 0.2 de ir a la derecha y 0.5 de ir hacia arriba.

El comportamiento de \emph{Randomito} se puede modelar con la cadena de Markov
que representa el siguiente grafo:

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[
			>=Stealth,
			node distance=3cm,
			state/.style={draw, circle, minimum size=1.2cm, font=\normalsize},
			every loop/.style={looseness=10},
			bend angle=20
		]

		% Definición de los cuatro nodos en forma de cuadrado
		\node[state] (SI) at (0,0)  {SI};
		\node[state] (SD) at (4,0)  {SD};
		\node[state] (II) at (0,-4) {II};
		\node[state] (ID) at (4,-4) {ID};

		% Lazos en cada nodo (loop)
		\draw[->] (SI) edge[loop above]   node[above]   {0.5} ();
		%\draw[->] (SD) edge[loop above]   node[above]   {0.5} ();
		\draw[->] (II) edge[loop below]   node[below]   {0.3} ();
		%\draw[->] (ID) edge[loop below]   node[below]   {0.5} ();

		% Flechas entre todos los pares de nodos
		% SI <-> SD
		\draw[->] (SI) to[bend left]  node[midway, below] {0.25} (SD);
		\draw[->] (SD) to[bend left]  node[midway, below] {1} (SI);

		% SI <-> II
		\draw[->] (SI) to[bend left]  node[midway, right]  {0.25} (II);
		\draw[->] (II) to[bend left]  node[midway, right] {0.5} (SI);

		% SD <-> ID
		%\draw[->] (SD) to[bend left]  node[midway, right] {0.5} (ID);
		%\draw[->] (ID) to[bend left]  node[midway, right]  {0.5} (SD);

		% II <-> ID
		\draw[->] (II) to[bend left]  node[midway, below] {0.2} (ID);
		\draw[->] (ID) to[bend left]  node[midway, below] {1} (II);

	\end{tikzpicture}
	\caption{Cadena de Markov representando el comportamiento probabilístico de \emph{Randomito}.}
	\label{fig:cm-randomito}
\end{figure}

Caminos posibles de esta cadena de Markov podrían ser:
\begin{itemize}
	\item el que mantiene a \emph{Randomito} siempre en la esquina superior izquierda,
	      $(SI, SI, SI, \dots)$, o
	\item el describe un primer movimiento en ``C" de \emph{Randomito} y luego se
	      comporta de cualquier manera, $(SI, SD, SI, II, ID, II, SI, \dots)$.
	      %\item  o algunos de los que mantienen a \emph{Randomito} en la parte superior de la grilla, como $(SI, SD, SI, SD, SI, SD, \dots)$ o $(SI, SD, SI, SD, SI, SI, SI, \dots)$.
\end{itemize}

Una duda natural entonces que podría surgir es ¿qué probabilidad hay de que
\emph{Randomito} siga alguno de esos caminos?

Para poder asociar probabilidades a eventos en cadenas de Markov (o, tal vez
pensar en preguntas sobre la probabilidad de que \emph{Randomito} siga algún
tipo particular de camino), la noción intuitiva de probabilidades en $M$ debe
ser formalizada. Lograremos esto asociándole un espacio de probabilidad a $M$.
Los caminos infinitos de $M$ juegarán el rol de resultados de la
$\sigma$-álgebra asociada. Esto es $Outc^M = \Paths(M)$. Y la $\sigma$-álgebra
asociada a $M$ será la generada por los conjuntos cilindro formados por los
fragmentos de caminos finitos en $M$.

\begin{definition}[Conjunto cilindro]
	El conjunto cilindro de $\hat \omega = (s_0, \dots, s_n) \in \Pathsfin(M)$ está definido como
	$$\Cyl(\hat \omega) = \{\omega \in \Paths(M) \mid \hat \omega \in \prefijo(\omega)\}$$
\end{definition}

\begin{definition}[$\sigma$-álegra de una cadena de Markov]
	La $\sigma$-álgebra $\eventE^M$ asociada a la cadena de Markov $M$ es la $\sigma$-álgebra más pequeña que contiene todos los conjuntos cilindro $\Cyl(\hat \omega)$ donde $\hat \omega \in \Pathsfin(M)$.
\end{definition}

De conceptos clásicos de teoría de probabilidad se sigue entonces que para cada
estado $s$ existe una única medida de probabilidad $\Prob^M_{s}$ en la
$\sigma$-álgebra $\eventE^M$ asociada a $M$, donde las probabilidades para los
conjunto cilindros (es decir, los eventos) están dadas por:
\begin{equation}
	\label{probMC}
	\Prob^M_{s}(Cyl(s_0, \dots, s_n)) = \iota(s,s_0) \cdot \prod_{0 \leq i < n} P(si, s_{i+1})\text{, donde }\iota(s,s_0) =
	\begin{cases}
		1 &\text{si } s = s_0 \\
		0 &\text{en otro caso}
	\end{cases}
\end{equation}

\textbf{¿Qué probabilidad hay de que Randomito siga este camino?}

Ahora, con las definiciones vistas, podemos pensar en la probabilidad que tiene
\emph{Randomito} de tomar algunos caminos en específico. Por ejemplo,

\begin{itemize}
	\item la probabilidad de que \emph{Randomito} se quede siempre en la esquina superior
	      izquierda, aún empezando desde allí, resulta 0 porque $\Prob^M_{SI}((SI, SI,
		      SI, \dots)) = \lim_{n\to \infty} (0.5)^n = 0$.
	\item la probabilidad de que \emph{Randomito} haga un primer movimiento en ``C'' y
	      luego se comporte de cualquier manera será $\Prob^M_{SI}(\Cyl(SI, SD, SI, II,
		      ID, II, SI)) = 0.25 \cdot 1 \cdot 0.25 \cdot 0.2 \cdot 1 \cdot 0.5 = 0.00625$.
\end{itemize}

\textbf{Notación}: en lo que sigue, usaremos notación LTL para describir ciertos eventos en cadenas de Markov. Por ejemplo, para un conjunto $B \subseteq \St$ de estados, $\alc B$ denota el evento de llegar eventualmente a (algún estado en) $B$, mientras que $siempevent B$ describe el evento en el que $B$ es visitado infinitamente a menudo. \hl{Algo más? sí}

Decir algo mas de notacion LTL? Algo más de cadenas de markov?

\section{Procesos de Decisión de Markov}

Las cadenas de Markov resultan útiles cuando queremos modelar sistemas en donde
existen decisiones probabilistas, pero existen situaciones en donde además de
decisiones probabilistas, necesitamos modelar elecciones, es decir, decisiones
no-deterministas. Para ello, existen los llamados procesos de decisión de
Markov.

Un proceso de decisión de Markov (MDP, por sus siglas en inglés) es una
generalización de una cadena de Markov donde un conjunto de acciones posibles
es asociado a cada estado. A cada par estado-acción le corresponde una
distribución de probabilidad en los estados, que es usada para seleccionar el
próximo estado. A su vez, una cadena de Markov se corresponde a un MDP donde
hay exactamente una acción asociada a cada estado.

Asumiremos la existencia de un conjunto fijo de acciones $\Act$ y a
continuación presentaremos la definición de un proceso de decisión de Markov:

\begin{definition}[Proceso de Decisión de Markov]
	Un proceso de decisión de Markov $\M =(\St, \Act, \theta)$ consiste de un conjunto finito de estados $\St$ y de dos componentes, $\Act$ y $\theta$, que especifican la estructura de transición:
	\begin{itemize}
		\item $\Act$ es un conjunto de acciones. Para cada $s \in \St$, $\Act(s) \subseteq \Act$ es el conjunto finito no vacío de
		      acciones disponibles en $s$. Para cada estado $s \in \St$ se requiere que $\Act(s) \neq \emptyset$.
		\item $\theta : \St \times \Act \times \St \rightarrow [0,1]$ es una función de transición probabilística. Para todo estado $s \in \St$, si $a \in \Act(s)$ tenemos que $\sum_{s' \in \St} \theta(s, a, s') = 1$, mientras que si $a \notin \Act(s)$, $\sum_{s' \in \St} \theta(s, a, s') = 0$. Para cada $s, t \in \St$ y $a \in \Act(s)$, $\theta(s, a, t)$ es la probabilidad de transicionar de $s$ a $t$ cuando la acción $a$ es seleccionada.
	\end{itemize}
\end{definition}

El análisis de eventos en los procesos de decisión de Markov se hará en base a
comportamientos en vez de caminos. Un comportamiento en un proceso de decisión
de Markov es una secuencia alternante infinita de estados y acciones,
construida iterativamente por un proceso de dos pasos. Primero, dado un estado
$s$, una acción $a \in \Act(s)$ es seleccionada no-determinísticamente. Luego,
el sucesor $t$ de $s$ es seleccionado de acuerdo a la distribución asociada a
la acción $a$. La definición formal es como sigue:

\begin{definition}[Comportamiento en un MDP]
	Un comportamiento en un MDP $\M$ es una secuencia infinita $\omega = (s_0, a_0, s_1, a_1, \dots)$ tal que $s \in \St$, $a_i \in \Act(s_i)$ y $a_i(s_{i+1}) > 0$ para todo $i \geq 0$.

	Dado un estado $s$, indicaremos con $\Omega_s$ el conjunto de todos los
	comportamientos que se originan en $s$, con $\Omega_\M$ el conjunto de todos
	los comportamientos en $\M$ y con $\Omega^{\textit{fin}}_\M$ el conjunto de
	todos los prefijos finitos de comportamientos en $\M$ que terminan en un estado
	$s \in \St$.
\end{definition}

Veamos un ejemplo de un proceso de decisión de Markov.

\textbf{Roborto, el robot controlable}

Ahora supongamos que tenemos un robot en una grilla 2x2, \emph{Roborto}, al que
podemos manejar a través de un control remoto. Con este control, podemos
decidir si se mueve a la izquierda, derecha, arriba o abajo (dependiendo de lo
que permita su posición en la grilla). Sin embargo, por irregularidades que
puede haber en la grilla cuando seleccionamos una acción habrá una probabilidad
de que la acción no tenga el resultado deseado. Es decir, si estando en la
esquina superior izquierda, elijo que \emph{Roborto} se mueva a la derecha,
esto sucederá con una probabilidad $p$, pero por las irregularidades del
terreno, también puede suceder que se mueva hacia abajo con una probabilidad $1
	- p$.

Situaremos a \emph{Roborto} en una grilla particular con las siguientes
características:

\begin{itemize}
	\item desde la esquina superior izquierda, si se elije ir a la derecha, habrá una
	      probabilidad de $0.9$ de efectivamente ir a la esquina superior derecha y habrá
	      una probabilidad de $0.1$ de ir a la esquina inferior izquierda; mientras que
	      si se elije ir hacia abajo, con una probabilidad
\end{itemize}

\begin{figure}[h]
	\centering
	\begin{tikzpicture}[
			>=Stealth,
			node distance=3cm,
			state/.style={draw, circle, minimum size=1.2cm, font=\normalsize},
			every loop/.style={looseness=10},
			bend angle=20
		]

		% Definición de los cuatro nodos en forma de cuadrado
		\node[state] (SI) at (0,0)  {SI};
		\node[state] (SD) at (4,0)  {SD};
		\node[state] (II) at (0,-4) {II};
		\node[state] (ID) at (4,-4) {ID};

		% Lazos en cada nodo (loop)
		%\draw[->] (SI) edge[loop above]   node[above]   {0.5} ();
		%\draw[->] (SD) edge[loop above]   node[above]   {0.5} ();
		%\draw[->] (II) edge[loop below]   node[below]   {0.3} ();
		%\draw[->] (ID) edge[loop below]   node[below]   {0.5} ();

		% Flechas entre todos los pares de nodos
		% SI <-> SD
		\draw[->] (SI) to  node[midway, below] {0.25} (SD);
		\draw[->] (SD) to  node[midway, below] {1} (SI);

		% SI <-> II
		\draw[->] (SI) to  node[midway, right]  {0.25} (II);
		\draw[->] (II) to  node[midway, right] {0.5} (SI);

		% SD <-> ID
		\draw[->] (SD) to  node[midway, right] {0.5} (ID);
		\draw[->] (ID) to  node[midway, right]  {0.5} (SD);

		% II <-> ID
		\draw[->] (II) to  node[midway, below] {0.2} (ID);
		\draw[->] (ID) to  node[midway, below] {1} (II);

		% SI <-> ID
		% \draw[->, draw=blue] (SI) to node[midway, above] {0.15} (ID);
		% \draw[->, draw=green] (SI) to node[midway, above] {0.2} (ID);
		% \draw[->, draw=blue] (ID) to node[midway, above] {0.15} (SI);
		% \draw[->, draw=green] (ID) to node[midway, above] {0.35} (SI);

		\draw[->, draw=blue]  (SI) to[bend left=40]  node[above] {0.15} (ID);
		\draw[->, draw=green] (SI) to[bend left=20]  node[above] {0.2}  (ID);
		\draw[->, draw=blue]  (ID) to[bend right=40] node[below] {0.15} (SI);
		\draw[->, draw=green] (ID) to[bend right=20] node[below] {0.35} (SI);

		% SD <-> II
		\draw[->, draw=blue] (SD) to node[pos=0.1, above] {0.15} (II);
		\draw[->, draw=green] (SD) to node[pos=0.1, above] {0.2} (II);
		\draw[->, draw=blue] (II) to node[midway, above] {0.07} (SD);
		\draw[->, draw=green] (II) to node[midway, above] {0.05} (SD);

	\end{tikzpicture}
	\caption{Cadena de Markov representando el comportamiento probabilístico de \emph{Randomito}.}
	\label{fig:mdp-roborto}
\end{figure}

Para cadenas de Markov, el conjunto de caminos está equipado con una
$\sigma$-álgebra y una medida de probabilidad que refleja la noción intuitiva
de probabilidad para conjuntos (medibles) de caminos. Para los MDPs, esto es
levemente distinto. Como no hay restricciones en la resolución de las
elecciones no determinista, no hay una única medida de probabilidad asociada.
(\hl{capaz acá podría ir algo tipo un ejemplo como el de la página 841})

Para poder razonar sobre probabilidades de conjuntos de comportamientos en un
MDP necesitamos resolver de alguna manera el no determinismo, y para ello
introduciremos el concepto de estrategia.

\begin{definition}[Estrategia en un MDP]
	Sea $\M = (\St, \Act, \theta)$ un MDP. Una estrategia para $\M$ es una función $\pi: \Omega^{\textit{fin}} \rightarrow \dist (\Act)$ que asigna una distribución de probabilidad a cada prefijo finito de comportamiento tal que $\pi(\hat \omega) (a) > 0$ solo si $a \in \Act(s)$.
\end{definition}

\hl{Comentar algo de que en la literatura se suelen bajar las acciones de la definición de estrategia? O capaz la idea sería dropear las acciones de la definición de estrategia acá?}

\hl{Lo que está a continuación me hace un poco de ruido por la parte de la definición de la cadena de Markov. Es como lo presentan en el libro de Baier y Katoen y tiene esta idea de ir armando una continuidad entre MCs y MDPs en la explicación. La otra opción es ir poco más con el enfoque de Luca de Alfaro y presentar la $\sigma$-álgebra sin mencionar nada de MDPs}.

Como una estrategia resuelve todas las elecciones no deterministas en un MDP,
induce una cadena de Markov. Esto es, el funcionamiento de un MDP $\M$
siguiendo las decisiones de una estrategia $\pi$ puede ser formalizado por una
cadena de Markov $\M_\pi$, donde los estados son los prefijos finitos de
comportamientos en $\M$.

\begin{definition}[Cadena de Markov de un MDP inducida por una estrategia]
	Sea $\M = (\St, \Act, \theta)$ un MDP y $\pi$ una estrategia en $\M$. La cadena de Markov $\M_\pi$ está dada por
	$$\M_\pi = (\Omega^{\textit{fin}}_\M, P_\pi)$$
	donde para $\hat \omega = (s_0, a_0, s_1, a_1, \dots, s_n)$:
	$$P_\pi(\hat \omega, \hat \omega a s_{n+1}) = \pi(\hat \omega)(a) \cdot \theta(s_n, a, s_{n+1})$$
\end{definition}

Nótese que $\M_\pi$ cuenta con un espacio de estados infinito aun, aun cuando
el MDP $\M$ es finito. \hl{Entonces cuenta como una MC?}.

Como $\M_\pi$ es una cadena de Markov, uno ahora puede razonar sobre las
probabilidades de los conjuntos medibles de caminos que siguen la estrategia
$\pi$, simplemente usando las distintas medidas de probabilidad
$\Prob^{\M_\pi}_s$ asociadas a la cadena de Markov $\M_\pi$ (véase
\ref{probMC}).

\hl{Responder preguntas de probabilidad de cosas en MDP robot}

Intuitivamente, el estado $(s_0, a_0, \dots, s_n)$ de $\M_\pi$ representa la
configuración donde el MDP $\M$ está en el estado $s_n$ y cuenta con la
historia $(s_0, a_0, \cdots, s_{n-1}, a_{n-1})$. Según la definición que vimos
las estrategias pueden depender de la historia en su totalidad, produciendo
resultados distintos si al menos una acción o estado en su historia cambia,
pero es cierto que este caso no es lo usual. Veamos algunos tipos particulares
de estrategias donde esto no sucede.

\begin{definition}[Estrategias sin memoria]
	Sea $\M$ un MDP con espacio de estados $\St$. Una estrategia $\pi$ en $\M$ es sin memoria sii para cada par de comportamientos $(s_0, a_0, \dots, s_n)$ y $(t_0, a'_0, \dots, t_m)$ con $s_n = t_m$ vale que:
	$$\pi(s_0, a_0, \dots, s_n) = \pi(t_0, a'_0, \dots, t_m)$$
	En este caso, $\pi$ puede ser vista como una función $\pi : \St \rightarrow \dist(\Act)$.
\end{definition}

Coloquialmente, una estrategia es sin memoria si no recuerda nada de la
historia y solo elige probabilidades para las acciones basándose en el estado
actual. Esto puede ser bastante extremo en ciertos casos, por eso existe una
variante que busca reflejar la idea de finitud sin ser tan restrictiva: las
estrategias de memoria finita. Una estrategia de memoria finita puede ser
pensada intuitivamente como que solo puede guardar hasta una cantidad finita
fija de información de la historia, por lo que no podrá ser distinta para
\textbf{todo} prefijo finito de comportamiento. Formalmente, la definiremos a
través de una autómata determinista finito (DFA). La distribución de
probabilidad de las acciones será seleecionada a partir del estado actual en
$\M$ y el estado actual del autómata (al que llamaremos modo). Veamos su
definición:

\begin{definition}[Estrategias con memoria finita]
	Sea $\M$ un MDP con espacio de estados $\St$ y conjunto de acciones $\Act$. Una estrategia de memoria finita para $\M$ es una tupla $\pi = (Q, f_\pi, \Gamma, start)$ donde
	\begin{itemize}
		\item $Q$ es un conjunto finito de modos,
		\item $\Delta : Q \times \Act \times \St \rightarrow Q$ es la función de transición del autómata,
		\item $start: \St \rightarrow Q$ es la función que determina el modo en el que empieza el automáta para un estado inicial $s$,
		\item $f_\pi : Q \times \St \rightarrow \dist(\Act)$ es la función que asigna la distribución de probabilidad en las acciones desde un estado $s$, es decir, lo que veníamos entendiendo como estrategia en sí.
	\end{itemize}
	El funcionamiento del MDP bajo la estrategia de memoria finita sería como sigue. En principio, se inicializa el modo del DFA a $q_0 = start(s_0)$. Luego, desde cada estado $s_i$ posterior el proceso será iterativo. Primero, se seleccionará la distribución de probabilidad en las acciones a partir del modo actual $q_i$ del autómata con $f_\pi((q_i,s_i))$. Una vez tomada la decisión, se determina probabilísticamente la siguiente acción $a_{i+1}$, y, a partir de ella, se determina también probabilísticamente el siguiente estado $s_{i+1}$. Con la nueva acción y estado se selecionará el próximo modo del DFA $q_{i+1} = \Delta(q_i, a_{i+1}, s_{i+1})$ y se repetirá el proceso.

	Para $\hat \omega \in \Omega^{\textit{fin}}_\M$, notaremos con $\pi(\hat
		\omega)$ a la distribución obtenida al realizar el proceso explicado
	anteriormente con $\hat \omega$ \hl{pero qué pasa con la elección
		probabilística de las acciones? No habría que tenerlo? Y que hay de sin
		memoria? Ahi en realidad entonces no tendria que ser ultimo estado y penultima
		accion?}.
\end{definition}

Además de su categorización en base a qué tanto dependen de su historia, existe
otro tipo notable de estrategias: las estrategias deterministas. Una estrategia
es determinista cuando para cada $\hat \omega \in \Omega^{\textit{fin}}_\M$ la
distribución $\pi(\hat \omega)$ es una distribución de Dirac (es decir, una
distribución $\delta_a$ tal que $\delta_a(a) = 1$ y $\delta_a(b) = 0$ para todo
$b \neq a$). En la literatura (véase \cite{BaierKatoen, AlfaroThesis}) es usual
encontrarse con el estudio de estrategias solo en su variante determinista y,
en ese caso, se puede pensar a las estrategias como una función $\pi:
	\Omega^{\textit{fin}}_\M \rightarrow \Act$.

\hl{Mencionar de que tipos son las estrategias vistas en el ejemplo de robot}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{definition}[Conjunto cilindro]
% 	El conjunto cilindro de $\hat \omega = (s_0, a_0, \dots, s_n) \in \Omega^{\textit{fin}}_\M$ está definido como
% 	$$Cyl(\hat \omega) = \{\omega \in \Omega_\M \mid \hat \omega \in pref(\omega)\}$$
% \end{definition}

% \begin{definition}[$\sigma$-álegra de un MDP]
% 	La $\sigma$-álgebra $\eventE^\M$ asociada al proceso de decisión de Markov $\M$ es la $\sigma$-álgebra más pequeña que contiene todos los conjuntos cilindro $Cyl(\hat \omega)$ donde $\hat \omega \in \Omega^{\textit{fin}}_\M$.

% 	Para cada estado $s \in S$, definiremos como $\B_s \subseteq 2^{\Omega_s}$ a la
% 	$\sigma$-álgebra más pequeña que contiene a todos los conjuntos cilindros
% 	$Cyl(\hat \omega)$ donde $\hat \omega \in \Omega^{\textit{fin}}_{s}$.
% \end{definition}

% \hl{Todavía no sé muy bien cómo manejar lo de estados iniciales}

% Para poder hablar de probabilidad de conjuntos de comportamientos querríamos
% asociar a cada $\gamma \in \B_s$ su medida de probabilidad

\section{Juegos estocásticos}

¿textito de conexión con MDPs?

\hl{Definir cómo queda con lo de acciones habilitadas esto o MDP}
\begin{definition}[Juego estocástico]
	Un juego estocástico (SG) es una tupla $\G = (\St, (\St_\cuad, \St_\diam), \Act, \theta)$ donde:
	\begin{itemize}
		\item $\St$, un conjunto finito de estados con $\St_\cuad, \St_\diam \subseteq \St$ siendo una partición de él,
		\item $\Act$ es un conjuno de acciones, y
		\item $\theta : \St \times \Act \times \St \rightarrow [0,1]$ es una función de transición probabilística tal que para cada $s \in \St$, $\theta(s, a, \cdot) \in \dist(\St)$ o  $\theta(s, a, \St) = 0$.
	\end{itemize}
	Notaremos con $\Act(s) = \{a \in \Act \mid \theta(s,a,\St) = 1\}$ al conjunto de acciones habilitadas en $s$, y
\end{definition}

Se puede notar que si $\St_\cuad = \emptyset$ o $\St_\diam = \emptyset$,
entonces $\G$ es un proceso de decisión de Markov, y, como vimos antes, si a la
vez $\abs{\Act(s)} = 1$ entonces $\G$ es una cadena de Markov.

Otra manera de verlo es que podemos pensar a los juegos estocásticos como un
procesos de decisión de Markov donde el conjunto de estados está particionado
en dos: los estados correspondientes al jugador $\cuad$ y los estados
correspondientes al jugador $\diam$. La idea de esta anotación de a quién
pertence los estados es lo que introduce esta idea adversarial: no solo habrá
un ente que tome las decisiones no deterministas, sino dos. Esto quiere decir
que tendremos dos tipos de estrategias, una por cada tipo de jugador.

Al igual que con los MDPs, antes de definir las estrategias, presentamos el
concepto de comportamiento en un juego estocástico.

\begin{definition}[Comportamiento en un SG]
	Un comportamiento en un SG $\G$ es una secuencia infinita $\omega = (s_0, a_0, s_1, a_1, \dots)$ tal que $s_i \in \St$, $a_i \in \Act(s_i)$ y $a_i(s_{i+1}) > 0$ para todo $i \geq 0$.

	Dado un estado $s$, indicaremos con $\Omega_s$ el conjunto de todos los
	comportamientos que se originan en $s$, con $\Omega_\G$ el conjunto de todos
	los comportamientos en $\G$ y con $\Omega^{\textit{fin}}_{\G, i}$ el conjunto
	de todos los prefijos finitos de comportamientos en $\G$ que terminan en un
	estado $s \in \St_i$, con $i \in \{\cuad, \diam\}$.
\end{definition}

\begin{definition}[Estrategia en un juego estocástico]
	Sea $\G = (\St, (\St_\cuad, \St_\diam), \Act, \theta)$ un SG. Una estrategia $\pi_i$ para el jugador $i$ en $\G$ es una función $\pi: \Omega^{\textit{fin}}_i \rightarrow \dist (\Act)$ que asigna una distribución de probabilidad a cada prefijo finito de comportamiento que termina en un estado del jugador $i$ tal que $\pi(\hat \omega) (a) > 0$ solo si $a \in \Act(s)$.

	Llamaremos $\Pi_\cuad$ al conjunto de todas las estrategias del jugador $\cuad$
	y $\Pi_\diam$ al conjunto de todas las estrategias del jugador $\diam$.
\end{definition}

Podemos ver que las estrategias en un juego estocástico se definen igual a cómo
se las definen para procesos de decisión de Markov con la salvedad de que
pertenecerán a un jugador específico. Los distintos tipos de estrategias
presentadas en la sección anterior se extienden naturalemente a SG. \hl{Ver
	redacción}. Llamaremos $\Pi^{M}_i$ al conjunto de las estrategias sin memoria
del jugador $i$, $\Pi^{F}_i$ al conjunto de las estrategias de memoria finita
del jugador $i$, $\Pi^{D}_i$ al conjunto de las estrategias deterministas del
jugador $i$, $\Pi^{MD}_i$ al conjunto de las estrategias deterministas sin
memoria del jugador $i$ y $\Pi^{FD}_i$ al conjunto de las estrategias
deterministas con memoria finita del jugador $i$.

De manera similar a como lo razonamos para procesos de decisión de Markov, si
fijamos dos estrategias $\pi_\cuad \in \Pi_\cuad$ y $\pi_\diam \in \Pi_\diam$
en un juego estocástico $\G$ obtenemos una cadena de Markov a la que
denotaremos $\G^{\picuad, \pidiam}$. Esta cadena de Markov, para cada $s \in
	\St$ definirá una medida de probabilidad $\ProbG$ en la $\sigma$-álgebra de
Borel del conjunto de comportamientos en $\G$. \hl{¿Es comportamientos?}. Si
$\varepsilon$ es un conjunto medible en la $\sigma$-álgebra de Borel,
$\ProbG(\varepsilon)$ será la probabilidad de que las estrategias $\picuad$ y
$\pidiam$ sigan un comportamiento en $\varepsilon$ empezando desde el estado
$s$.

Tal como fue aclarado en las dos secciones anteriores, usaremos notación LTL
para representar conjunto específicos de comportamientos. Sin embargo, estando
en el análisis de juegos nos interesan hacernos preguntas sobre probabilidades
de ganar. Para poder hablar de ellas necesitamos formalizar ciertos conceptos
que presentamos a continuación.

\subsubsection{Valor en un juego y maneras de ganar}

\subsubsection{Determinismo}

\subsubsection{Objetivos en juegos estocásticos}

Lo primero que será importante notar es que vamos a trabajar con juegos de suma
cero.

\subsubsection{Recompensas cualitativas}

Sea \(C = \{c_1, \ldots, c_n\}\) un conjunto finito de colores, y \(\nu : V
\rightarrow 2^C\) una valuación. Una clase importante y bien estudiada de
recompensas cualitativas son las \textbf{funciones características de
	subconjuntos \(\omega\)-regulares de \textit{Run}(G)}.

La pertenencia a un conjunto \(\omega\)-regular de ejecuciones se determina por
una de las \textbf{condiciones de aceptación} listadas a continuación. Estas
condiciones corresponden a criterios de aceptación de autómatas de estado
finito sobre palabras infinitas (ver Sección 5.2.2 para más detalles).

\begin{itemize}
	\item \textbf{Alcanzabilidad y seguridad (Reachability and safety).} Una ejecución \(w \in \textit{Run}(G)\) satisface la condición de \textbf{alcanzabilidad} determinada por un color \(c \in C\) si \(c \in \nu(w(i))\) para algún \(i \geq 0\). La condición de \textbf{seguridad} determinada por \(c\) es dual, es decir, \(c \notin \nu(w(i))\) para todo \(i \geq 0\).

	\item \textbf{Büchi y co-Büchi.} Una ejecución \(w \in \textit{Run}(G)\) satisface la condición de \textbf{Büchi} determinada por un color \(c \in C\) si \(c \in \nu(w(i))\) para infinitos \(i \geq 0\). La condición de \textbf{co-Büchi} es dual, es decir, solo hay un número finito de \(i \geq 0\) tal que \(c \in \nu(w(i))\).

	\item \textbf{Rabin, Rabin-chain y Street.} Sea \textit{Pairs} un conjunto finito de pares de colores \newline \(\{(c_1, d_1), \ldots, (c_m, d_m)\}\). Una ejecución \(w \in \textit{Run}(G)\) satisface la condición de \textbf{Rabin} determinada por \textit{Pairs} si existe \((c, d) \in \textit{Pairs}\) tal que \(w\) satisface la condición de Büchi determinada por \(d\) y la condición de co-Büchi determinada por \(c\).

	      La condición de \textbf{Street} determinada por \textit{Pairs} es dual a Rabin,
	      es decir, para cada \((c, d) \in \textit{Pairs}\) tenemos que \(w\) satisface
	      la condición de co-Büchi determinada por \(d\) ó la condición de Büchi
	      determinada por \(c\).

	      Para un color dado \(c\), sea \(V(c)\) el conjunto de todos los \(v \in V\)
	      tales que \(c \in \nu(v)\). La condición de \textbf{Rabin-chain} (o paridad) es
	      un caso especial de la condición de Rabin donde \textit{Pairs} y \(\nu\)
	      satisfacen \(V(c_1) \subset V(d_1) \subset \cdots \subset V(c_m) \subset
	      V(d_m)\).

	\item \textbf{Muller.} Sea \(M \subseteq 2^C\) un conjunto de subconjuntos de colores. Una ejecución \(w \in \textit{Run}(G)\) satisface la condición de \textbf{Muller} determinada por \(M\) si el conjunto de todos los \(c \in C\) tales que \(w\) satisface la condición de Büchi determinada por \(c\) es un elemento de \(M\). (En criollo, ``los conjuntos de colores que w visita infinitas veces están en M")
\end{itemize}

Nótese que los conjuntos \(\omega\)-regulares de ejecuciones son relativamente
simples en el sentido de que están contenidos en los dos primeros niveles de la
jerarquía de Borel (los conjuntos de ejecuciones que satisfacen las condiciones
de alcanzabilidad y seguridad están en el primer nivel).

distintos tipos y organizacion / ver de que manera presentarlos

Kucera, omega reg, Chatterjee

\subsection{Una breve historia sobre los juegos estocásticos}

Enfoque mc -> mdp -> sg -> psg

Enfoque juegos deterministas -> juegos estocasticos (Shapley etc)

\section{Juegos deterministas}

Agg transiciones de texto escrito entre definiciones

\begin{definition}[juego de grafo de 2 jugadores]
	Definimos a un juego de grafo de dos jugadores (2G) como una tupla $G = (V, V_\cuad,V_\diam, E)$ donde $V = V_\cuad \biguplus V_\diam$ es un conjunto de vértices (o estados) particionado en $V_\cuad$ y $V_\diam$, y $E \subseteq (V\times V)$ es una relación que denota el conjunto de aristas (dirigidas) que representan transiciones de un estado a otro del juego.

	Los 2 jugadores son llamados $\cuad$ y $\diam$ y controlan los vértices
	$V_\cuad$ y $V_\diam$, respectivamente.
\end{definition}

\begin{definition}[estrategia sobre un 2G]
	Una estrategia para un jugador $i$ con $i \in \{\cuad, \diam\}$ es una función $\sigma_i: V^*V_i \rightarrow V$ con la restricción de que $\sigma_i(wv) \in E(v)$ para todo $wv \in V^*V_i$.
\end{definition}

\begin{definition}[jugada sobre un 2G]
	Una jugada en un juego de grafo de dos jugadores es una secuencia infinita de vértices $\rho = v_0v_1v_2 \dots \in V^\omega$, donde para todo $i \in \N_0$ tenemos que $v^i \in V$ y $(v^i, v^{i+1}) \in E$.

	Sean $\sigma_\cuad$ y $\sigma_\diam$ un par de estrategias y sea $v_0$ un
	vértice inicial, la jugada que cumple con $\sigma_\cuad$ y $\sigma_\diam$ es la
	única jugada $\rho = v_0 v_1 v_2 \dots$ para la cual cada $i \in \N_0$, si $v_i
		\in V_\cuad$ entonces $v_{i+1} = \sigma_\cuad(v_0 \dots v_i)$, y si $v_i \in
		V_\diam$ entonces $v_{i+1} = \sigma_\diam(v_0 \dots v_i)$.
\end{definition}

\begin{definition}[condición ganadora]
	Una condición ganadora $\varphi$ en un juego de grafo de dos jugadores es un conjunto de jugadas sobre el juego, i.e., $\varphi \subseteq V^\omega$. Usaremos notación LTL para describir conjuntos de jugadas específicos.
\end{definition}

\begin{definition}[regiones ganadoras]
	El jugador $\cuad$ gana el juego de grafo de dos jugadores $G$ para una condición ganadora $\varphi$ desde un vértice $v_0$ so existe una estrategia $\picuad$ tal que para cada $\pidiam$, la jugada $\rho$ que sigue $\picuad$ y $\pidiam$ satisface $\varphi$, i.e., $\rho \in \varphi$.
	La región ganadora $\W \subseteq V$ para el jugador $\cuad$ es el conjunto de vértices desde donde el jugador $\cuad$ gana el juego.
\end{definition}

Agg ejemplo

\section{Comparación de los distintos juegos}

Incluir cuadros comparativos entre juegos // ejemplos para todas las nociones
nuevas explicadas

(Capaz agregar un apéndice con cuadritos de esto ?)

(estaría bueno ver problemas y agregar un apéndice con ejemplos?)